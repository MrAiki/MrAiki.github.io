var tipuesearch = {"pages":[{"title":"2020-04-21","text":"いろいろなTODO 優先度が大きい順に。 残差勾配 \\(\\mathrm{E}[\\varepsilon(n) x(n - m)]\\) の挙動観察をすべし。 \\(m\\) が大きいときは無視できるのでは？ なお、長時間平均値は0に収束していることを見た。 → 確かめるべし。 \\(m\\) をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。 ガチャガチャ弄ってるってるけど示唆があんまりない。 低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。 \\(\\mathrm{E}[\\varepsilon(n) x(n - m)]\\) は \\(m\\) を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。 同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一のsin波を等価させたときはわかりやすい。 440.0Hzのsin波に対する各タップの平均勾配変化 ボイス対する各タップの平均勾配変化 ピアノ演奏に対する各タップの平均勾配変化 メッセージパッシング使えない？ 何らかの確率モデル化をせよ、というふうに受け取った。 AMP, Survay-Propagation（三村さん、樺島さん）がありえる。 → AMP, Survay-Propagationについて調査すべし。 周波数領域に一旦飛ばすのはあり？ ありだけど計算量が高い。圧縮率が上がるのであれば大アリ。 確率的PCAとか使えない？辞書は小さくて済む。 線形ダイナミクスにより上手く定式化できない？ 出す学会については HND 先生に聞くこと。 相談する機会はどこかで絶対に必要。 優先度低 著作権処理済み音源データベースについて相談 → 自分で情報をまとめて、申し込んでいいかというところまで進めるべし。 RWC 研究用音楽データベース: 音楽ジャンルデータベースと楽器音データベース RWC研究用音楽データベース → 進めた。動けるようになったら書類をまとめていく。 Donohoさんなどが圧縮センシングの文脈で既にやりきってない？ ありえる。調査すべし。 → ライス大学では成果をすべて公開しているから見るだけ見たほうが良い。 → http://dsp.rice.edu/cs/ を見よ。 Compressed sensing block MAP-LMS adaptive filter for sparse channel estimation and a bayesian Cramer-Rao bound 残差はガウス分布としてるけどクラメル-ラオ下限との絡みを述べている。何か重要そう。 Bayesian Compressive Sensing Using Laplace Priors これもパラメータの事前分布にラプラス分布を導入してベイズ推定するもの。残差ではないはず。 「L1」, 「Laplace」, 「residual」, 「lossless」で検索したけどスパース解を求めるものばかり。今のところはセーフ？ → 継続して調査はする。 分散行列、ヘッセ行列、フィッシャー情報行列、自然勾配 ガウス分布の最尤推定で、分散行列の逆行列はフィッシャー情報行列に一致する お？じゃあラプラス分布は？ 絶対値関数入ってるけど、NNのデルタ則を導くときみたいに連続関数で近似して微分して後で極限取るみたいなのはできる。式変形チャレンジしてみるべし。符号関数はシグモイド関数で近似できる。後で温度パラメータの極限を取る。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"雑記","url":"/2020-04-21.html","loc":"/2020-04-21.html"},{"title":"2020-04-20","text":"IRLSの更新式について MathJaxの環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 newcommand を行えばずっと使えるみたい。便利。 \\begin{equation*} \\newcommand\\innerp[2]{\\langle #1, #2 \\rangle} \\newcommand\\ve[1]{\\boldsymbol{#1}} \\newcommand\\parfrac[2]{\\frac{\\partial #1}{\\partial #2}} \\end{equation*} 逐次的更新の件について。IRLSでは以下の評価関数 \\(J(\\ve{\\beta})\\) の最小化を考える。 \\begin{equation*} J(\\ve{\\beta}) = \\sum&#94;{M}_{i = 1} w_{i} (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}})&#94;{2} \\end{equation*} ここで \\(M\\) は観測数。これは二次式だから評価関数は凸関数になる。早速 \\(\\ve{\\beta}\\) で偏微分してみると、 \\begin{align*} \\parfrac{}{\\ve{\\beta}} J(\\ve{\\beta}) &= \\sum&#94;{M}_{i = 1} w_{i} 2 \\left(- \\frac{\\partial}{\\partial \\ve{\\beta}} \\innerp{\\ve{\\beta}}{\\ve{x}_{i}} \\right) (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}) \\\\ &= -2 \\sum&#94;{M}_{i = 1} w_{i} (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}) \\ve{x}_{i} \\end{align*} \\(\\parfrac{}{\\ve{\\beta}} J(\\ve{\\beta}) = 0\\) とおくと、 \\begin{align*} \\sum_{i = 1}&#94;{M} w_{i} \\innerp{\\ve{\\beta}}{\\ve{x}_{i}} \\ve{x}_{i} &= \\sum_{i = 1}&#94;{M} w_{i} y_{i} \\ve{x}_{i} \\\\ \\iff \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{c} w_{1} \\innerp{\\ve{\\beta}}{\\ve{x}_{1}} \\\\ \\vdots \\\\ w_{M} \\innerp{\\ve{\\beta}}{\\ve{x}_{M}} \\end{array} \\right] &= \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{c} w_{1}y_{1} \\\\ \\vdots \\\\ w_{M}y_{M} \\end{array} \\right] \\\\ \\iff \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{ccc} w_{1} & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & w_{M} \\end{array} \\right] \\left[ \\begin{array}{c} \\innerp{\\ve{\\beta}}{\\ve{x}_{1}} \\\\ \\vdots \\\\ \\innerp{\\ve{\\beta}}{\\ve{x}_{M}} \\end{array} \\right] &= \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{ccc} w_{1} & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & w_{M} \\end{array} \\right] \\left[ \\begin{array}{c} y_{1} \\\\ \\vdots \\\\ y_{M} \\end{array} \\right] \\\\ \\iff \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{ccc} w_{1} & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & w_{M} \\end{array} \\right] \\left[ \\begin{array}{c} \\ve{x}_{1}&#94;{\\mathsf{T}} \\\\ \\vdots \\\\ \\ve{x}_{M}&#94;{\\mathsf{T}} \\end{array} \\right] \\ve{\\beta} &= \\left[ \\ve{x}_{1} ... \\ve{x}_{M} \\right] \\left[ \\begin{array}{ccc} w_{1} & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & w_{M} \\end{array} \\right] \\ve{y} \\\\ \\iff \\ve{X} \\ve{W} \\ve{X}&#94;{\\mathsf{T}} \\ve{\\beta} &= \\ve{X} \\ve{W} \\ve{y} \\end{align*} \\(\\ve{X}\\ve{W}\\ve{X}&#94;{\\mathsf{T}}\\) が正則（TODO: \\(\\ve{X}\\) が行フルランク、かつ \\(\\ve{W}\\) が正則なら行けそうに見えるけど本当か？）の場合は閉形式で係数が求まる: \\begin{equation*} \\ve{\\beta} = (\\ve{X} \\ve{W} \\ve{X}&#94;{\\mathsf{T}})&#94;{-1} \\ve{X} \\ve{W} \\ve{y} \\end{equation*} ここまでは一般論。さて、更新式に注目する。 \\(\\beta_{j}\\) だけで偏微分してみると、 \\begin{align*} \\parfrac{J(\\ve{\\beta})}{\\beta_{j}} &= \\sum_{i = 1}&#94;{M} \\parfrac{}{\\beta_{j}} w_{i} (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}})&#94;{2} \\\\ &= -2 \\sum_{i = 1}&#94;{M} w_{i} (\\ve{x}_{i})_{j} (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}) \\end{align*} 残差のL1ノルム最小化を考えるときは \\(w_{i} = \\frac{1}{|y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}|}\\) とおくので代入すると、 \\begin{equation*} \\parfrac{J(\\ve{\\beta})}{\\beta_{j}} = -2 \\sum_{i = 1}&#94;{M} (\\ve{x}_{i})_{j} \\mathrm{sign}(y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}) \\end{equation*} 瞬間値（ \\(M=1\\) とする）を考えるとSigned-LMSの更新式そのものになっている。 和を取ると平均操作に近いから、LMSアルゴリズムと考えていることは同じ。 \\(\\parfrac{J(\\ve{\\beta})}{\\beta_{j}}\\) を更に \\(\\beta_{k}\\) で偏微分してみると、 \\begin{align*} \\frac{\\partial&#94;{2} J(\\ve{\\beta})}{\\partial \\beta_{j} \\partial \\beta_{k}} &= -2 \\sum_{i = 1}&#94;{M} w_{i} (\\ve{x}_{i})_{j} \\parfrac{}{\\beta_{k}} (y_{i} - \\innerp{\\ve{\\beta}}{\\ve{x}_{i}}) \\\\ &= 2 \\sum_{i = 1}&#94;{M} w_{i} (\\ve{x}_{i})_{j} (\\ve{x}_{i})_{k} \\\\ &= 2 \\left[ (\\ve{x}_{1})_{j} \\dots (\\ve{x}_{M})_{j} \\right] \\left[ \\begin{array}{c} w_{1} (\\ve{x}_{1})_{k} \\\\ \\vdots \\\\ w_{M} (\\ve{x}_{M})_{k} \\end{array} \\right] = 2 \\left[ (\\ve{x}_{1})_{j} \\dots (\\ve{x}_{M})_{j} \\right] \\ve{W} \\left[ \\begin{array}{c} (\\ve{x}_{1})_{k} \\\\ \\vdots \\\\ (\\ve{x}_{M})_{k} \\end{array} \\right] \\end{align*} 2次式が出てくるのがわかる（ \\(\\ve{W}\\) は計量だ）。そして \\((\\ve{H})_{jk} = \\frac{\\partial&#94;{2} J(\\ve{\\beta})}{\\partial \\beta_{j} \\partial \\beta_{k}}\\) なるヘッセ行列 \\(\\ve{H}\\) は以下: \\begin{equation*} \\ve{H} = 2 \\ve{X} \\ve{W} \\ve{X}&#94;{\\mathsf{T}} \\end{equation*} ヘッセ行列の性質により関数の最小値・最大値の存在がわかる。対称行列なのは間違いない（ \\((\\ve{X})_{ij} = (\\ve{X})_{ji}\\) は自明）。（固有値分解とは見れない。 \\(\\ve{H}\\) は \\(N \\times N\\) の行列であるのに対して、 \\(\\ve{X}\\) は \\(N \\times M\\) の行列。 \\(\\ve{X} \\ve{X}&#94;{\\mathsf{T}}\\) は平均化、除算を抜いた分散共分散行列になり半正定値行列。）また、任意のベクトル \\(\\ve{v}\\) に対して、 \\begin{align*} \\ve{v}&#94;{\\mathsf{T}} \\ve{X} \\ve{W} \\ve{X}&#94;{\\mathsf{T}} \\ve{v} &= \\ve{v}&#94;{\\mathsf{T}} \\ve{X} \\ve{W}&#94;{1/2} \\ve{W}&#94;{1/2} \\ve{X}&#94;{\\mathsf{T}} \\ve{v} \\\\ &= (\\ve{W}&#94;{1/2} \\ve{X}&#94;{\\mathsf{T}} \\ve{v})&#94;{\\mathsf{T}} \\ve{W}&#94;{1/2} \\ve{X}&#94;{\\mathsf{T}} \\ve{v} \\\\ &= || \\ve{W}&#94;{1/2} \\ve{X}&#94;{\\mathsf{T}} \\ve{v} ||_{2}&#94;{2} \\geq 0 \\end{align*} だから、 \\(\\ve{W}\\) が半正定値（ \\(\\iff\\) すべての重みが非負）ならばヘッセ行列は半正定値行列で、極小値が最小値になる。また、 \\(J(\\ve{\\beta})\\) は凸関数（半正定値だから狭義の凸関数ではない）。 もう少しヘッセ行列を見る。ヘッセ行列を上手く使えたらニュートン法で解けそうな気がして。 \\begin{equation*} (\\ve{H})_{jk} = 2 \\sum_{i = 1}&#94;{M} w_{i} (\\ve{x}_{i})_{j} (\\ve{x}_{i})_{k} \\end{equation*} より、スペクトル分解的に見ると、 \\begin{align*} \\frac{1}{2} \\ve{H} &= w_{1} \\left[ \\begin{array}{ccc} (\\ve{x}_{1})_{1}&#94;{2} & \\dots & (\\ve{x}_{1})_{1} (\\ve{x}_{1})_{N} \\\\ \\vdots & \\ddots & \\vdots \\\\ (\\ve{x}_{1})_{N} (\\ve{x}_{1})_{1} & \\dots & (\\ve{x}_{1})_{N}&#94;{2} \\\\ \\end{array} \\right] + \\dots + w_{M} \\left[ \\begin{array}{ccc} (\\ve{x}_{M})_{1}&#94;{2} & \\dots & (\\ve{x}_{M})_{1} (\\ve{x}_{M})_{N} \\\\ \\vdots & \\ddots & \\vdots \\\\ (\\ve{x}_{M})_{N} (\\ve{x}_{M})_{1} & \\dots & (\\ve{x}_{M})_{N}&#94;{2} \\\\ \\end{array} \\right] \\\\ &= w_{1} \\ve{x}_{1} \\ve{x}_{1}&#94;{\\mathsf{T}} + \\dots + w_{M} \\ve{x}_{M} \\ve{x}_{M}&#94;{\\mathsf{T}} \\\\ &= \\sum_{i = 1}&#94;{M} w_{i} \\ve{x}_{i} \\ve{x}_{i}&#94;{\\mathsf{T}} \\end{align*} 信号処理的には \\(\\ve{x}_{1}, \\ve{x}_{2}, \\dots \\ve{x}_{M}\\) は系列で現れる。 LMSフィルタでは \\(i = 1\\) の時だけを考えていたと考えられれる。 \\(i = 2,\\dots,M\\) のときの影響は少ないのではないかと思う。 FIRフィルタを考えるのならば、各 \\(\\ve{x}_{1}\\) は入ってきた1次元信号データを時系列順に並べたものだから、直前のベクトル \\(\\ve{x}_{2}\\) を使えそうな構造に見える。 上の仮定を使ってヘッセ行列の逆行列 \\(\\ve{H}&#94;{-1}\\) を逐次近似計算できない？ 分散共分散行列がほぼヘッセ行列になってるけどこれは何？ 金谷さんの解説 にそれとなく解説がある。フィッシャー情報行列との関連もある。。。クラメル・ラオの下限についてわかりやすい説明あり。 最尤法 にもそれとなく解説あり。 奥村さん もあり。観測からヘッセ行列を構成できる？ そして自然勾配のアイディアが出てくる。自然勾配を使ったLMSアルゴリズムは…あった… Normalized Natural Gradient Adaptive Filtering for Sparse and Nonsparse Systems 甘利先生による解説 で、LMSアルゴリズム含めて大まかなところはだいたい言ってる。 高知工科大学の博論 ワンチャンスL1残差最小化はやってないかも。 TODO: 前のMTGで言われたことの整理 分散行列、ヘッセ行列、フィッシャー情報行列、自然勾配の整理 Fisher Information Matrix OMPが気になる。試してみたい。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"雑記","url":"/2020-04-20.html","loc":"/2020-04-20.html"},{"title":"2020-04-19","text":"IRLS(Iteratively Reweighted Least Squares) その2 理論ばっかり追っていて悶々してきたので、IRLSでL1残差最小化が解けないか実験してみる。 第5章 厳密解から近似解へ に『スパースモデリング』5章のPython実装あり。 スパースモデリング：第3章 追跡アルゴリズム は『スパースモデリング』3章のPython実装。 IRLSの実装は カレル大学卒論 を参考に。Pythonで簡単にできた。 import numpy # IRLS法によりPhi @ x = yのスパース解を求める def irls_update ( Phi , x , y , order ): EPSILON = 10 ** ( - 8.0 ) # 重みの計算 weight = numpy . abs ( y - Phi @ x ) . flatten () # 小さくなりすぎた重みは打ち切る weight [ weight < EPSILON ] = EPSILON # 対角行列に展開 W = numpy . diag ( weight ** ( order - 2 )) # 更新後の係数: Phi.T @ W @ Phi @ x = Phi.T @ W @ y の解 return numpy . linalg . solve ( Phi . T @ W @ Phi , Phi . T @ W @ y ) if __name__ == \"__main__\" : DIMENSION = 2 NUM_SAMPLES = 100 NUM_ITERATION = 50 # 解ベクトル X_ANSWER = numpy . array ([ 0.5 , 0.5 ]) . reshape (( DIMENSION , 1 )) x = numpy . zeros (( DIMENSION , 1 )) xhistory = numpy . zeros (( DIMENSION , NUM_ITERATION )) # 観測を生成 Phi = numpy . random . rand ( NUM_SAMPLES , DIMENSION ) y = Phi @ X_ANSWER # 加法的雑音を重畳 # yrand = y + numpy.random.normal(0, 0.3, (NUM_SAMPLES, 1)) yrand = y + numpy . random . laplace ( 0 , 0.3 , ( NUM_SAMPLES , 1 )) error = numpy . zeros ( NUM_ITERATION ) emp_error = numpy . zeros ( NUM_ITERATION ) # IRLSを繰り返し適用 for count in range ( NUM_ITERATION ): x = irls_update ( Phi , x , yrand , 1 ) xhistory [:, count ] = x . reshape ( 2 ) error [ count ] = numpy . linalg . norm ( y - Phi @ x , ord = 1 ) / NUM_SAMPLES emp_error [ count ] = numpy . linalg . norm ( yrand - Phi @ x , ord = 1 ) / NUM_SAMPLES 実装は楽だったけど、誤差解析が沼。 誤差を重畳してみると、真の誤差と経験誤差が当然一致しない。 経験誤差的には局所解に入っている印象。 サンプル数が少ないと大域最小解に入らないケースあり（経験誤差曲面の最小値が真の誤差の曲面の最小値に不一致） 経験誤差の曲面は二次曲線に見える。（2次式の最小化を考えているから当然のはず。） 最小二乗解よりも誤差が悪い時がある。最小二乗解はorder=2とすれば良くて、その時重み行列Wは単位行列になり、普通の最小二乗法と一致。 思いつき: IRLSは評価関数の最小化を考える時閉形式で求まるので何も考えない。パラメータに関してもう一度微分できるのでニュートン法使えそう。 フィルタのときのように逐次的に求められない？ パラメータ全てではなく1こずつ。サンプルについても1こずつ。更新していく。評価関数の最小化は平均値の最小化に見受けられるので、逐次的に更新しても良いように見える。 今日は遅いのでもう寝る","tags":"雑記","url":"/2020-04-19.html","loc":"/2020-04-19.html"},{"title":"2020-04-18","text":"IRLS(Iteratively Reweighted Least Squares) LAD(Least Absolute Deviation)を近似的・逐次的に解く方法としてのIRLSについて調査。そういえば基本的な原理を抑えていなかった。 Iteratively Reweighted Least Squares についてサクッと。 文字通りサクッとしたまとめ。OMPを使って解いているというのがとても気になる Iterative Reweighted Least Squares 導入から解法まで。しかしなぜ解が求まるのかは不明。 Iterative Reweighted Least Squares バッファロー大の講義資料？これも何故解けるのかはちゃんと書いてない。 Iterative Reweighted Least Squares これが一番いいかも。なぜ解けるかもざっくり証明がある。 そこで出てきたsupergradient（優勾配？劣勾配に対応している？）がよくわからん。資料のすぐ下に解説があったけど。 Supergradients に定義はあったけど幾何学的イメージが欲しい。 Weiszfeld Algorithmsという幾何中央値を求めるアルゴリズムは Generalized Weiszfeld Algorithms for Lq Optimization に解説あり。しかしこの論文いいこと言ってる。「Generalized Weiszfeld Algorithms」は圧縮センシングとは異なりスパース表現を求めるわけではない。スパース性は担保されなくても、よりL1ノルムの意味で小さい解を求める。 なぜ、IRLSとLMSアルゴリズムを結びつける研究がないのか。IRLSの逐次適用によってもフィルタ係数を更新していけそうだけど。試してみるし、類似研究が無いか引き続き調べる。 『スパースモデリング』の5章にも記述はある。しかし残差のL1最小化ではない。","tags":"雑記","url":"/2020-04-18.html","loc":"/2020-04-18.html"},{"title":"2020-04-17","text":"LAD(Least Absolute Deviation) LAD(Least Absolute Deviation)を見ている。これは、残差をL1ノルムにした回帰問題一般のこと。 カレル大学卒論 が結構まとまっている。 最尤推定による近似的手法 は軽く読んだ。各傾きと切片を固定して逐次更新していく。更新時は中央値を拾ってくる。うーん中央値だと高速推定が厳しい。。。 ラプラス分布の最尤推定しようとしてもがく。対数尤度とって見てみても、単純な絶対値和が出て止まるし、反復スケーリング法を参考に、パラメータの増分を加えた時の対数尤度の下限を求めようとしたが上手く行かず。4時間飛ばす。 最尤推定の計算のあがき あがいて「A maximum likelihood approach to least absolute deviation regression」を引用している文献を漁ったら辞書学習をL1にしているやつが、やっぱりいた。 Online Robust Non-negative Dictionary Learning for Visual Tracking パーティクルフィルターを使っておる。 上の文献で使ってるHuber Loss結構すごくね？この誤差に基づくLMSアルゴリズムねえの？→「Robust Huber adaptive filter」だけど中身を読めず… また、 Convex Optimization and Modeling を読んでたらHuber損失はL1とL2の中間的な性質を示すようで、0に集中しなくなりそうな印象を受けた。","tags":"雑記","url":"/2020-04-17.html","loc":"/2020-04-17.html"},{"title":"2020-04-16","text":"LMSフィルターの挙動観察 \\(\\mathrm{E}[\\mathrm{sign}[e(n)]x(n-m)]\\) の挙動を追いたい。色々な信号に対して、 \\(m\\) が十分大きいとき、0に近づくかどうか を知りたい。もし0に近づくならば有効な過程として解法に使える。 しかしその前に、LMSフィルター自体の挙動を追いたい。 残差はどの様に減る？残差の時系列は？ ステップサイズにより収束の度合い（残差の分布）が違う... 当然、フィルタ次数でも収束の度合い（残差の分布）が違う 残差分布はどうなってる？Signed-LMSでラプラス分布に近づいてる？ これは本当のようで、Signed-LMSの方が裾が細い残差分布が得られている。 単純な正弦波に対してはLMSのほうが残差が小さくなるが、ボイスやピアノ音源に対しては圧倒的にSignLMSの方が性能が良い（残差のヒストグラムを見ると、裾が狭い） \\(\\mathrm{E}[\\mathrm{sign}[e(n)]x(n-m)]\\) , \\(\\mathrm{E}[e(n)x(n-m)]\\) は両方とも0。 逐次計算していったら、音源非依存で0に近づいていく 当然だよな…そもそもの過程として入力と雑音は無相関と仮定しているのだから。 仮定しているのだからは正しくなくて、無相関にするようにフィルタ係数を更新しているが正しい。 無相関になったときに勾配が0で最急勾配法が止まる。 なんか絶対値誤差最小化ってどっかで見たよな…と思っていたら、 https://en.wikipedia.org/wiki/Least_absolute_deviations 修士のときに一回戦っていた。 カレル大学卒論 が結構まとまっている。 \\(L_{1}\\) ノルム最小化を近接オペレータの繰り返し適用で解けんじゃね？と思っている 近接勾配法とproximal operator を読んだが、パラメータ正則化だけだな パラメータ正則化はあるけど、残差をスパースにするのがない。なんで？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"雑記","url":"/2020-04-16.html","loc":"/2020-04-16.html"},{"title":"2020-04-10","text":"続・古いロスレス音声コーデックの調査 古いロスレス音声コーデックと理論の概要を取りまとめた雑誌の特集があった: Lossless Compression of Digital Audio 理論としてもその通りだし、雑誌発行時点(1998)からさしたるブレークスルーが無いように見える。 AudioPak, OggSquish, Philips, Sonarc, WAという謎のコーデック現る…。いったい何個あるんだ。","tags":"雑記","url":"/2020-04-10.html","loc":"/2020-04-10.html"},{"title":"2020-04-08","text":"古いロスレス音声コーデックの調査 ロスレス音声の歴史を探るために古いロスレス音声コーデックの情報を探っている。以下のサイトが Hydrogenaudioでの比較 よりも古い内容を扱っている。 Lossless Compression of Audio 見つけたロスレス音声コーデックを一覧する。というかほぼ Really Rare Wares 様へのリンク。 古めのロスレス音声コーデック RKAU(RK Audio) 古い比較において優秀な圧縮率を誇っていた。当時のMonkey's Audioよりも上。サイトを覗いたら exe と dll のみの配布だった。 RKAUのホームページ（魚拓） を見ても特に情報なし。 AudioZip これも圧縮率が比較的優秀。 AudioZipのホームページ（魚拓） を見てもこちらも特に情報なし。 WavArc こちらも最大圧縮率(-c5)を選択するとそれなりに優秀な結果を出していた。このページにexeとドキュメントをまとめたzipもあり。 WaveZip 圧縮率よりは速度重視のコーデックのようだ。MUSICompress というアルゴリズムの実装。 WaveZipのデータシート によると符号化にはLZ(Lampel-Ziv)を使用しているようだ。 WaveZipの概要 が比較サイトに掲載されていた。どうやら、入力波形を近似波形と誤差波形に分けて符号化するようだ。WaveZipではHu LPAC/LTAC LPACはMPEG4-ALSの前身。LPACの前身がLTAC。LPACの平均的な圧縮率は優秀なようだ。 LPAC（魚拓） に以前公開していたサイトあり。 LTAC(Lossless Transform Audio Compression)は名前の通り変換符号化に基づくロスレス音声圧縮コーデック、LPAC(Lossless Predictive Audio Compression)は予測に基づくロスレス音声圧縮コーデック。 LPACに ベルリン工科大学、Real Networks、NTT の改良が加わってMPEG4-ALSが出来上がり、それ以降LPACの開発は停止されている。この経緯については MPEG4-ALS（魚拓） に記述あり。 Shorten（魚拓） おそらくロスレス音声の最古参にして基礎。なんと執筆時点（2020-04-08）でも brew でインストールできた（ Shortenのmanページ もあるから各Linuxディストリビューションで使えるものと想像する）。エンコード速度はピカイチ。 Shortenの論文 （テクニカルレポート）もある。この論文で、今のロスレス音声につながる重要な事実に幾つか触れている。 音声信号は準定常（短い区間では定常とみなせる）だからブロックに分けてエンコード/デコードすべき。 音声のモデル化には線形予測(LPC, Linear Predictive Coding)が使える。 残差信号はガウス分布よりもラプラス分布に従っていると見える。その符号化にはライス符号を使うのが良い。 この時点で既にラプラス分布を仮定したパラメータ設定を行っているからかなりの慧眼。他のロスレス音声コーデックはShortenを発展させたものに過ぎないと見える。 所感 どうも2000年代前半までは各自でロスレス音声コーデックを作り、各自で最強を謳っていたらしい。 歴史を雑にまとめると、1994年にShortenの論文が出てから、それよりも圧縮率の良いもの、圧縮速度（展開速度）が早いものが開発されて混沌に突入し上記のコーデックが現れた。その後、Monkey's Audio, WavPack, FLAC, LPAC（MPEG4-ALS）が生き残り、2000年以降はLa（更新停止）, TAK, TTA, ALAC（更新停止）, WMAL(Windows Media Audio Lossless), 2010年以降はOptimFROGが出現しているようだ。 気になるのは比較サイトの Rice Coding, AKA Rice Packing, Elias Gamma codes and other approaches である。Rice符号よりも効率の良いとされるPod符号の紹介がある。要観察。 スパース適応フィルタ LPCの定式化をスパースにする試みは多くなされている。 Sparse Modeling for Lossless Audio Compression : Ghidoさん（OptimFROGの人）の試み 貪欲法によりスパース解を求めている。 スパース表現に基づく音声音響符号化 : NTTの試み 最小二乗解を求めるのではなくL1最小化に置き換えた定式化を行う。 でも、TTAがやっているような適応フィルタをスパース解に近づける手法はまだロスレス音声に対してやっていないように見える。 スパースな解を目指してフィルタ係数を更新する適応フィルタはスパース適応フィルタ(Sparse Adaptive Filters)というようで、2000年代以降に研究が進んでいるようだ。 最も基本的な適応フィルタであるLMS(Least Mean Square)フィルタは名前の通り二乗誤差最小化に立脚している。 スパース適応フィルタの主な用途はエコーキャンセル、ブラインド話者分離、複数話者特定ではあるが、やはり変換後の分布がスパースになるというのは大きい。 スパース適応フィルタの最近のサーベイ論文 を流し読みした。スパース適応フィルタは、変数更新のときに1部の変数だけ更新する方法と、スパース最適化に従って更新するやり方の2つがあった。PNLMS(Proportionate NLMS), IPNLMS(Improved PNLMS)が後者の定式化で興味あり。引き続き見ていく。 Regularized Least-Mean-Square Algorithms には正則化を入れたLMSアルゴリズムの解説あり。LASSOにモチベーションを受けた最適化アルゴリズムが ZA-LMS や APWL1 として提案されている。","tags":"雑記","url":"/2020-04-08.html","loc":"/2020-04-08.html"},{"title":"2020-04-02","text":"GitHub io + Pelican を使ってみた。しばらくこちらで日報を書きたい。 GitHub io + Pelicanは以下の記事を参考にしている。まだあんまり分かってない。 Python製静的HTMLジェネレータのPelicanでGitHub Pagesを公開する方法 GitHub Pagesで静的サイトを簡単に作る Python製 Pelican を使ってサクッとブログを公開する Pelicanのテーマ集 テーマ導入時にハマったので参考にしたissue comment 今日は（というか3月末）からSLAの高速化作業とまとめをしていた。 格子型フィルタ演算はどうしても1乗算型にできず。次数演算を4次数にしてSSE演算するのがやっと。 SSE化するときに、スカラー演算とベクトル演算が混じったときに処理負荷が大きく上がってハマった。 StackOverFlowの記事 では _mm_set_epi32 のコストが高い旨記述あり。 _mm_loadu_si128 の使用に置き換えた。 他の記事 で言及があってようやく分かった。全てをベクトル演算化したところ、処理負荷は4/5倍になった。あんまり早くなっていない。遺憾。 gccとVC にはgccとVisual Studioの挙動の差異について色々と書いてあった。","tags":"雑記","url":"/2020-04-02.html","loc":"/2020-04-02.html"}]};