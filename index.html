<!DOCTYPE html>
<html lang="ja" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Aiki's Blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="">
        <meta name="author" content="aiki" />

    <!-- Open Graph tags -->
        <meta property="og:site_name" content="Aiki's Blog" />
        <meta property="og:type" content="website"/>
        <meta property="og:title" content="Aiki's Blog"/>
        <meta property="og:url" content=""/>
        <meta property="og:description" content="Aiki's Blog"/>



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>

        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Aiki's Blog ATOM Feed"/>


</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
Aiki's Blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                        <li >
                            <a href="/category/ji-shi.html">記事</a>
                        </li>
                        <li >
                            <a href="/category/za-ji.html">雑記</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
            <article>
                <h2><a href="/paseputoronxi-hua.html">パーセプトロン昔話</a></h2>
                <div class="summary"><p><strong>ニューラルネットワーク（Neural Network, 以下NN）</strong>は機械学習の歴史と共に歩んできたと言っても過言ではない.
戦後間もないウィーナーの時代<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup>からモデルが構築され始め,
幾つかの冬の時代（挫折）を超えて,
そして現在流行りのディープラーニング（深層学習）は多層構造のNNによって構成されている.
ここでは, NNの歴史に少しずつ触れながら,
多層パーセプトロンの学習則（逆誤差伝搬）までを解説していく.
本稿は主に<sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup><sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup>を参照している.</p>
<div class="math">$$
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\tvec}[1]{\bvec{#1}^{\mathsf{T}}}
\newcommand{\ve}[1]{\bvec{#1}}
\newcommand{\inpro}[2]{{\left\langle #1 , #2 \right\rangle}}
\newcommand …</div>
                    <a class="btn btn-default btn-xs" href="/paseputoronxi-hua.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/zui-da-entoropimoderu.html">最大エントロピーモデル</a></h2>
                <div class="summary"><p>最大エントロピーモデルの導出過程、学習の更新則、素性選択についての理論的側面を述べる。記述の大部分は<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>を参照し、一部<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup><sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>も参照している。</p>
<p>最大エントロピーモデルは、データの特徴を <strong>素性関数(feature function)</strong> によって記述し、素性関数がある <strong>制約(constraint)</strong> を満たし、かつ、モデルを表現する確率分布のエントロピーが最大となる（最大エントロピー原理を満たす）モデルである。</p>
<p>エントロピーを最大にする事により、制約を満たしながら最大エントロピーモデルの確率分布が最も一様に分布する様になり、未知データに対する確率を無下に<span class="math">\(0\)</span>にすることが無くなるため、高い汎用性（汎化性能）が期待できる。</p>
<h2>モデルを表現する確率分布の導出</h2>
<p>まず、サンプル（事例）データのドメイン（定義域）を<span class="math">\(X\)</span>、データに付与されたラベルのドメインを<span class="math">\(Y\)</span>と書く。例えば、次に来る単語を予測させたい場合には、サンプル<span class="math">\(X\)</span>は1つ前までの単語の並び、ラベル<span class="math">\(Y …</span></p>
                    <a class="btn btn-default btn-xs" href="/zui-da-entoropimoderu.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/svmsapotobekutorumashin.html">SVM（サポートベクトルマシン）</a></h2>
                <div class="summary"><p><strong>SVM(Support Vector Machine, サポートベクトルマシン)</strong>は、深層学習の影に隠れがちではあるものの、現在使われている識別学習モデルの中でも比較的認識性能が優れ、実用に供される事はもちろん、様々な研究でも比較対象となる手法の一つである。</p>
<p>SVMの大雑把な理論的概要を述べると、SVMは与えられた学習サンプルを最も適切に分離（識別）する境界面（<strong>識別面</strong>）を発見する手法である。その識別面は凸計画問題に帰着して求める事ができるので、どの様なサンプルにおいても（存在するならば）最適な識別面を構成できる。 </p>
<p>本稿では、最初に基本となる線形SVMの定式化を行い、次に汎用性をより高めた非線形SVMとソフトマージンSVMを説明し、最後にSVMを回帰問題に適用したSVR(Support Vector Regression, サポートベクトル回帰)を説明する。最後にC言語による実装例を挙げる。</p>
<p>SVMも知り尽くされており、文献・資料は大量に存在する。ここでは、参考書籍<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>を挙げる。</p>
<h2>線形SVM</h2>
<h3>マージンの定式化</h3>
<p>識別の例として、まずは図にあるような、2次元空間<span class="math">\(X\times Z\)</span>に存在する2クラスのサンプルデータ（以下サンプル）を仮定する。各クラスは二値のラベル付け …</p>
                    <a class="btn btn-default btn-xs" href="/svmsapotobekutorumashin.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/mcmcmarukohulian-suo-montekarurofa.html">MCMC（マルコフ連鎖モンテカルロ）法</a></h2>
                <div class="summary"><p>本稿ではMCMC法の解説のため、MC法による積分の計算方法（モンテカルロ積分）から、MCMCによる手法の概要を見ていく。MCMC法は有名かつ知り尽くされた手法で、多くの良質な説明資料 <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup><sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup><sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup><sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> が存在している。従ってここの説明は読まずに、資料を見てもらった方が理解が早いかもしれない。</p>
<p>一般に <strong>MC（Monte-Calro, モンテカルロ）法</strong> は、サンプリング（サンプルを乱数から生成すること）によってシミュレーションや数値計算を行う手法である。特に確率分布が関わる積分値<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup> を近似的に求めるMC法はモンテカルロ積分と呼ばれる。モンテカルロ積分は確率的な推論の一種であり、大数の法則<sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup> によって、十分なサンプル数をとれば近似精度をいくらでも良くする事ができる。サンプリングの手間がある為、近似分布をあらかじめ仮定する様な決定論的な推論よりも遥かに推論が遅い。しかし、MCは近似分布が求められないような場合にも適用可能であり、汎用性が高いと言える。</p>
<p>MC法によって原理的には任意の解を求められるが、十分なサンプル数の要求というのが大きな問題を孕んでいる。サンプリングの自由度（範囲及び次元）が大きくなると、解の計算にあまり寄与しない（無駄な）サンプルが増えてしまう。計算を現実的かつ効率的に行うためには、サンプルの選択が重要になる。</p>
<p>そして <strong>MCMC（Markov …</strong></p>
                    <a class="btn btn-default btn-xs" href="/mcmcmarukohulian-suo-montekarurofa.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/lpclinear-predictive-coding-xian-xing-yu-ce-fu-hao-hua.html">LPC（Linear Predictive Coding, 線形予測符号化）</a></h2>
                <div class="summary"><p>線形予測分析等とも言及される。</p>
<p>英語版で決定的に簡単な資料は<a href="http://www.emptyloop.com/technotes/A%20tutorial%20on%20linear%20prediction%20and%20Levinson-Durbin.pdf">ここ</a>で見れます。ここの解説はその日本語訳以下の何かです。英語が読める人はそっちを見たほうが絶対早いです。</p>
<p>ここよりも良い資料が有ります：（<a href="http://aidiary.hatenablog.com/entry/20120415/1334458954" title="人工知能に関する断創録 - 線形予測分析（LPC）">人工知能に関する断創録</a>）</p>
<h2>アルゴリズムの導出</h2>
<h3>問題設定</h3>
<p>時間について離散化した信号が<span class="math">\(y_{0}, y_{1}, ..., y_{n}\)</span>として得られたとする。ここで、<span class="math">\(y_{n}\)</span>を直前の<span class="math">\(y_{i}\ (i=0,...,n-1)\)</span>によって予測する事を考える。</p>
<p>予測にあたって、線形予測では<span class="math">\(k\)</span>個の係数<span class="math">\(a_{1},...,a_{k}\)</span>を用いた単純な線形結合
</p>
<div class="math">$$
-a_{1}y_{n-1} - a_{2}y_{n-2} - ... - a_{k}y_{n-k} = - \sum_ …</div>
                    <a class="btn btn-default btn-xs" href="/lpclinear-predictive-coding-xian-xing-yu-ce-fu-hao-hua.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/li-san-huriebian-huan-dft.html">離散フーリエ変換（DFT）</a></h2>
                <div class="summary"><p>離散時間かつ離散周波数でのフーリエ変換を離散フーリエ変換という。</p>
<h2>準備：時間領域で離散化すると、周波数領域では周期的になる</h2>
<p><span class="math">\(f(t)\)</span>を離散化した信号を<span class="math">\(g(t)\)</span>とおく。離散化には、サンプリング周期<span class="math">\(t_{s}\)</span>の周期的デルタ関数<span class="math">\(\delta_{t_{s}}(t)\)</span>を用いて</p>
<div class="math">$$ g(t) = f(t) \delta_{t_s}(t) = \sum_{n=-\infty}^{\infty} f(t) \delta(t - nt_{s}) $$</div>
<p>とする。デルタ関数<span class="math">\(\delta(t)\)</span>は関数<span class="math">\(f(t)\)</span>に対して次が成り立つ（超）関数である：</p>
<div class="math">$$
\int^{\infty}_ …</div>
                    <a class="btn btn-default btn-xs" href="/li-san-huriebian-huan-dft.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/2020-04-22.html">2020-04-22</a></h2>
                <div class="summary"><div class="math">
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
</div>
<div class="section" id="signed-lms2-2">
<h2>Signed-LMSの2階微分 その2</h2>
<p>早速既存研究が無いか見ている。二乗誤差最小化のLMSでもヘッセ行列の逆行列の計算負荷が高いから使わん、という論調がほとんど。Signed-LMSについては今の所、微分してるところも見てない。</p>
<ul class="simple">
<li><a class="reference external" href="https://pt.slideshare.net/mentelibre/neural-network-widrowhoff-learning-adaline-hagan-lms">NEURAL NETWORK Widrow-Hoff Learning Adaline Hagan LMS</a> 観測分散行列がヘッセ行列に一致することが書いてあった。</li>
<li><a class="reference external" href="http://www1.coe.neu.edu/~erdogmus/publications/J013_NEUNET_SpIssueIJCNN03_EWCLMS_Yadu.pdf">Stochastic error whitening algorithm for linear filter estimation with noisy data …</a></li></ul></div>
                    <a class="btn btn-default btn-xs" href="/2020-04-22.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/2020-04-21.html">2020-04-21</a></h2>
                <div class="summary"><div class="section" id="mathrm-e-varepsilon-n-x-n-m">
<h2>残差勾配 <span class="math">\(\mathrm{E}[\varepsilon(n) x(n - m)]\)</span> の挙動観察</h2>
<p><span class="math">\(m\)</span> が大きいときは無視できるのでは？ なお、長時間平均値は0に収束していることを見た。
<span class="math">\(m\)</span> をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。</p>
<ul class="simple">
<li>ガチャガチャ弄ってるってるけど示唆があんまりない。</li>
<li>低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。</li>
<li><span class="math">\(\mathrm{E}[\varepsilon(n) x(n - m)]\)</span> は <span class="math">\(m\)</span> を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。</li>
<li>同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一のsin波を等価させたときはわかりやすい。</li>
</ul>
<div class="figure align-center">
<img alt="440.0Hzのsin波に対する各タップの平均勾配変化グラフ" src="./images/sin_mean_gradient.png" style="width: 40%;" />
<p class="caption">440.0Hzのsin波に対する各タップの平均勾配変化</p>
</div>
<div class="figure align-center">
<img alt="ボイス対する各タップの平均勾配変化グラフ" src="./images/voice_mean_gradient.png" style="width: 40%;" />
<p class="caption">ボイス対する各タップの平均勾配変化</p>
</div>
<div class="figure align-center">
<img alt="ピアノ演奏に対する各タップの平均勾配変化グラフ" src="./images/ruriko_mean_gradient.png" style="width: 40%;" />
<p class="caption">ピアノ演奏に対する各タップの平均勾配変化</p>
</div>
</div>
<div class="section" id="signed-lms2">
<h2>Signed-LMSの目的関数の2階微分</h2>
<p>勇気を出してやってみる。</p>
<div class="math">
\begin{equation*}
\newcommand\innerp[2 …</div></div>
                    <a class="btn btn-default btn-xs" href="/2020-04-21.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/2020-04-20.html">2020-04-20</a></h2>
                <div class="summary"><div class="section" id="irls">
<h2>IRLSの更新式について</h2>
<p>MathJaxの環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 <tt class="docutils literal">newcommand</tt> を行えばずっと使えるみたい。便利。</p>
<div class="math">
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
</div>
<p>逐次的更新の件について。IRLSでは以下の評価関数 <span class="math">\(J(\ve{\beta})\)</span> の最小化を考える。</p>
<div class="math">
\begin{equation*}
J(\ve{\beta}) = \sum^{M}_{i = 1} w_{i} (y_{i …</div></div>
                    <a class="btn btn-default btn-xs" href="/2020-04-20.html">more ...</a>
                </div>
            </article>
            <hr/>
            <article>
                <h2><a href="/2020-04-19.html">2020-04-19</a></h2>
                <div class="summary"><div class="section" id="irls-iteratively-reweighted-least-squares-2">
<h2>IRLS(Iteratively Reweighted Least Squares) その2</h2>
<p>理論ばっかり追っていて悶々してきたので、IRLSでL1残差最小化が解けないか実験してみる。</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/kibo35/sparse-modeling/blob/master/ch05.ipynb">第5章 厳密解から近似解へ</a> に『スパースモデリング』5章のPython実装あり。</li>
<li><a class="reference external" href="https://qiita.com/kibo35/items/66ec4479b0899ea4987d#irlsの概要">スパースモデリング：第3章 追跡アルゴリズム</a> は『スパースモデリング』3章のPython実装。</li>
</ul>
<p>IRLSの実装は <a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f">カレル大学卒論</a> を参考に。Pythonで簡単にできた。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># IRLS法によりPhi @ x = yのスパース解を求める</span>
<span class="k">def</span> <span class="nf">irls_update</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
    <span class="n">EPSILON</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">)</span>
    <span class="c1"># 重みの計算</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">Phi</span> <span class="err">@</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="c1"># 小さくなりすぎた重みは打ち切る</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">weight</span> <span class="o">&lt;</span> <span class="n">EPSILON …</span></pre></div></div>
                    <a class="btn btn-default btn-xs" href="/2020-04-19.html">more ...</a>
                </div>
            </article>
            <hr/>

        <ul class="pagination">
                <li class="prev disabled"><a href="#">&laquo;</a></li>
                    <li class="active"><a
                            href="/index.html">1</a></li>
                    <li class=""><a
                            href="/index2.html">2</a></li>
                <li class="next"><a
                        href="/index2.html">&raquo;</a></li>
        </ul>
        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://github.com/aikiriao/"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://getpelican.com/" target="_blank">Pelican</a>
    </li>
    <li class="list-group-item">
      <a href="http://python.org/" target="_blank">Python.org</a>
    </li>
    <li class="list-group-item">
      <a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2020 aiki
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>              <p><small>Unless otherwise stated, all articles are published under the <a href="http://www.wtfpl.net/about/">WTFPL</a> license.</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>




</body>
</html>