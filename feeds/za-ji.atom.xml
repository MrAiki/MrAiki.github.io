<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aiki's Blog - 雑記</title><link href="/" rel="alternate"></link><link href="/feeds/za-ji.atom.xml" rel="self"></link><id>/</id><updated>2020-05-24T11:00:00+09:00</updated><entry><title>正則化(1)</title><link href="/zheng-ze-hua-1.html" rel="alternate"></link><published>2020-05-24T11:00:00+09:00</published><updated>2020-05-24T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-24:/zheng-ze-hua-1.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\newcommand\mean[2]{\mathrm{E}_{#1} \left[ #2 \right]}
\newcommand\KL[2]{\mathrm{KL} \left[ #1 \ \middle| \middle| \ #2 \right]}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逆行列補題を使えば、どんな自然勾配法でも上手く動きそうな気がしてきた …。
勾配の分散行列を逐次的に求められるから相当強い。
自己相関行列であることはそんなに重要でもないかも。。。。でも評価待ちましょう …&lt;/p&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\newcommand\mean[2]{\mathrm{E}_{#1} \left[ #2 \right]}
\newcommand\KL[2]{\mathrm{KL} \left[ #1 \ \middle| \middle| \ #2 \right]}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逆行列補題を使えば、どんな自然勾配法でも上手く動きそうな気がしてきた …。
勾配の分散行列を逐次的に求められるから相当強い。
自己相関行列であることはそんなに重要でもないかも。。。。でも評価待ちましょう。。。&lt;/p&gt;
&lt;p&gt;一方で今日から正則化をどうすればいいか考えている。答えはフィッシャー情報行列に &lt;span class="math"&gt;\(\lambda \ve{I}\)&lt;/span&gt; を足すだけなんだが、意味づけというか解釈が上手くできない。どういう損失関数ならばフィッシャー情報行列に単位行列を足す形になるのか。。。&lt;/p&gt;
&lt;p&gt;SignedLMS で試したけど難航中。どうしても &lt;span class="math"&gt;\(\mathrm{sign}[\varepsilon(n)]\ve{x}(n)\)&lt;/span&gt; との積をとる項が出てきて、その平均がどうなるかわからない。。。&lt;/p&gt;
&lt;p&gt;実験的に勾配に係数ベクトルを足すなり係数の符号ベクトルを足すなりしてるけど、
正則化パラメータをめちゃくちゃ小さく取らないと結果が発散する …。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category><category term="Regularization"></category></entry><entry><title>成果まとめ中(5) / 自然勾配法の概観</title><link href="/cheng-guo-matomezhong-5-zi-ran-gou-pei-fa-nogai-guan.html" rel="alternate"></link><published>2020-05-23T11:00:00+09:00</published><updated>2020-05-23T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-23:/cheng-guo-matomezhong-5-zi-ran-gou-pei-fa-nogai-guan.html</id><content type="html">&lt;p&gt;昨日まで苦悩しつつ収束条件をまとめた（運良く見つけることができた。同時に NLMS の収束条件も掴んだ。）
でも、同時に新規性が無い気がしてきた。発見となるのは、SignedLMS のフィッシャー情報行列が自己相関行列になっているくらいか？
適応ステップサイズ導出後は、普通のフィッシャー情報行列込みの NLMS と全く同じだし。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>成果まとめ中(4)</title><link href="/cheng-guo-matomezhong-4.html" rel="alternate"></link><published>2020-05-21T23:00:00+09:00</published><updated>2020-05-21T23:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-21:/cheng-guo-matomezhong-4.html</id><content type="html">&lt;p&gt;AdaptiveFilter の本見て NLMS の議論を色々見てる。が、いい結果が出てこない。
本の内容も掴みかねてる。NLMS は係数誤差ベクトルの L2 ノルムが指数的に減少するようだが本当か …？&lt;/p&gt;
&lt;p&gt;LMS の収束条件が分かっていないことに気づく。
「Adaptive Filter Theory」では式 4.22 に、「Adaptive Signal Processing」では式 4.45 で示されているので確認中。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>成果まとめ中(3)</title><link href="/cheng-guo-matomezhong-3.html" rel="alternate"></link><published>2020-05-20T23:00:00+09:00</published><updated>2020-05-20T23:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-20:/cheng-guo-matomezhong-3.html</id><summary type="html">&lt;p&gt;SGD（確率的最急勾配法）の収束レートが少し気になったのちょっと観察。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.jstage.jst.go.jp/article/bjsiam/28/3/28_27/_pdf"&gt;機械学習における確率的最適化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;もっと初等的な説明があると良いなあ。確率 1 で極値に収束したような気がしている …。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;適応ステップサイズの分母の &lt;span class="math"&gt;\(\ve{x}(n)^{\mathsf{T}}\ve{R}^{-1}\ve{x}(n)\)&lt;/span&gt; がめちゃくちゃ気になって止まる。学習理論に「例えばパラメタ空間上のベクトル &lt;span class="math"&gt;\(x(\theta)\)&lt;/span&gt; の内積 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;SGD（確率的最急勾配法）の収束レートが少し気になったのちょっと観察。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.jstage.jst.go.jp/article/bjsiam/28/3/28_27/_pdf"&gt;機械学習における確率的最適化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;もっと初等的な説明があると良いなあ。確率 1 で極値に収束したような気がしている …。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;適応ステップサイズの分母の &lt;span class="math"&gt;\(\ve{x}(n)^{\mathsf{T}}\ve{R}^{-1}\ve{x}(n)\)&lt;/span&gt; がめちゃくちゃ気になって止まる。学習理論に「例えばパラメタ空間上のベクトル &lt;span class="math"&gt;\(x(\theta)\)&lt;/span&gt; の内積 &lt;span class="math"&gt;\(\innerp{x}{x}\)&lt;/span&gt; は、座標変換により不変な量として定義するならば &lt;span class="math"&gt;\(x^{\prime}(g_{ij}(\theta))^{-1}x\)&lt;/span&gt; となる。」（なるべく原文ママ）と言ってて、まさにこの不変な量を指していると思っている。&lt;/p&gt;
&lt;p&gt;これどういうこと？と思って探し始めたら沼。相対性理論にぶつかる。わかりやすかったのは下くらいか？&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://yuru2physics.blog.fc2.com/blog-entry-99.html"&gt;内積が不変という意味&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://fnorio.com/0180covariant_contravariant/covariant_contravariant.html#3-6"&gt;(6) ベクトル内積の座標変換不変性の確認&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;情報幾何の観点からすると、幾何的に微小線素は座標変換によって値を全く変えないことが重要らしい。&lt;span class="math"&gt;\(\ve{x}(n)\)&lt;/span&gt; をパラメタ空間上のベクトルと捉えると、&lt;span class="math"&gt;\(\ve{R}^{-1}\)&lt;/span&gt; はパラメタ空間上の計量（元の空間の計量は :math:&lt;cite&gt;ve{R}&lt;/cite&gt; ）を定め、&lt;span class="math"&gt;\(\ve{x}(n)^{\mathsf{T}}\ve{R}^{-1}\ve{x}(n)\)&lt;/span&gt; はパラメタ空間上のノルムを計算していて、ノルムだから不変でしょっていう議論になる？まだピンとこない。&lt;/p&gt;
&lt;p&gt;あと、AdaptiveFilter の本見て NLMS の議論を色々見てる。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>成果まとめ中(2)</title><link href="/cheng-guo-matomezhong-2.html" rel="alternate"></link><published>2020-05-19T11:00:00+09:00</published><updated>2020-05-19T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-19:/cheng-guo-matomezhong-2.html</id><content type="html">&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://dsl4.eee.u-ryukyu.ac.jp/DOCS/DSP/p10.pdf"&gt;ディジタル信号処理 第 10 回 適応信号処理&lt;/a&gt; 少し詳しく書いてある。やはり適応フィルタの原典を当たりたい。&lt;/li&gt;
&lt;/ul&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>書類整理終わり、復帰 / 成果まとめ中(1)</title><link href="/shu-lei-zheng-li-zhong-wari-fu-gui-cheng-guo-matomezhong-1.html" rel="alternate"></link><published>2020-05-18T11:00:00+09:00</published><updated>2020-05-18T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-18:/shu-lei-zheng-li-zhong-wari-fu-gui-cheng-guo-matomezhong-1.html</id><content type="html">&lt;p&gt;諸々の提出書類で実験できず。1 週間空けて復帰。&lt;/p&gt;
&lt;p&gt;報告書類を書いていたら、やっぱりフィッシャー情報行列とヘッセ行列の違いがよくわからなくなってきた。
対数尤度のヘッセ行列とフィッシャー情報行列に何かしらの共通点があるはず。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.inference.vc/on-empirical-fisher-information/"&gt;Notes on the Limitations of the Empirical Fisher Approximation&lt;/a&gt; 経験フィッシャー行列の性能限界について。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ヘッセ行列のくだりから自然勾配の導出まで、紙に証明をまとめた。明日辺りに記事に起こす。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>LMS Filterの挙動観察中(10)</title><link href="/lms-filternoju-dong-guan-cha-zhong-10.html" rel="alternate"></link><published>2020-05-06T11:00:00+09:00</published><updated>2020-05-06T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-06:/lms-filternoju-dong-guan-cha-zhong-10.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Normalize するやつの意味付けを追っている。非常に RLS(Recursive Least Square) に近い、下手すると RLS そのものかも知れない。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen …&lt;/script&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Normalize するやつの意味付けを追っている。非常に RLS(Recursive Least Square) に近い、下手すると RLS そのものかも知れない。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>書類整理中</title><link href="/shu-lei-zheng-li-zhong.html" rel="alternate"></link><published>2020-05-06T11:00:00+09:00</published><updated>2020-05-06T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-06:/shu-lei-zheng-li-zhong.html</id><content type="html">&lt;p&gt;色々と書類整理しているためあんまり実験が進んでいない。でも、既存研究がありそうでヒヤヒヤする毎日。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=2780&amp;amp;context=ele_comeng_facwork"&gt;Normalized Natural Gradient Adaptive Filtering for Sparse and Nonsparse Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;かなり近い。が、LMS ベースの計量を自ら設計している。コスト関数に、2 乗誤差項に何か変換の L2 ノルムを加算しており、それに対しての自然勾配を求めている。そうか、コスト関数の自然勾配を考えればいいのか。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pdfs.semanticscholar.org/bf3b/fe757d9156cc863ffdde15b1664c337819bd.pdf"&gt;Proportionate Normalized Least-Mean-Squares Adaptation in Echo Cancelers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自然勾配の発想に近づいていた PNLMS の実装&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiki.eecs.umich.edu/global/data/hero/images/7/7b/Yilun-icassp2-09.pdf"&gt;SPARSE LMS FOR SYSTEM IDENTIFICATION&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;スパース LMS(ZA-LMS) の最初の論文。定式化が明確。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>LMS Filterの挙動観察中(9)</title><link href="/lms-filternoju-dong-guan-cha-zhong-9.html" rel="alternate"></link><published>2020-05-05T11:00:00+09:00</published><updated>2020-05-05T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-05:/lms-filternoju-dong-guan-cha-zhong-9.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;実装誤りを見直しながらもう一度。&lt;/p&gt;
&lt;p&gt;やはり、自然勾配法が何故うまくいくのか、更新式の導出までやったほうが宜しい。実装間違いするから。適応的自然勾配の更新式は微小量の近似を使っている。微小量の近似は今まで何度も避けてきたが、この際おさらいする。ようはテイラー展開して 2 次以降の項を打ち切れば良し。高校数学レベルの話。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://physnotes.jp/foundations/approximation/"&gt;近似式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://w3e.kanazawa-it.ac.jp/math/category/suuretu/maclaurin/henkan-tex.cgi?target=/math/category/suuretu/maclaurin/maclaurin_1-x.html"&gt;1/(1-x) のマクローリン展開&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実験が落ち着いたら書いていきたい。&lt;/p&gt;
&lt;p&gt;→ 実験 OK。ステップサイズの設定が難しかったけど、ナイーブなものよりは性能がよいはず。&lt;/p&gt;
&lt;p&gt;また、軽く見た感じでも自然勾配学習法は発散しやすい。以下の記事にあるように、正則化を掛けたほうが良さそう。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tfjgeorge.github.io/articles/note/2018/11/09/empirical-fisher.html"&gt;What …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;実装誤りを見直しながらもう一度。&lt;/p&gt;
&lt;p&gt;やはり、自然勾配法が何故うまくいくのか、更新式の導出までやったほうが宜しい。実装間違いするから。適応的自然勾配の更新式は微小量の近似を使っている。微小量の近似は今まで何度も避けてきたが、この際おさらいする。ようはテイラー展開して 2 次以降の項を打ち切れば良し。高校数学レベルの話。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://physnotes.jp/foundations/approximation/"&gt;近似式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://w3e.kanazawa-it.ac.jp/math/category/suuretu/maclaurin/henkan-tex.cgi?target=/math/category/suuretu/maclaurin/maclaurin_1-x.html"&gt;1/(1-x) のマクローリン展開&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実験が落ち着いたら書いていきたい。&lt;/p&gt;
&lt;p&gt;→ 実験 OK。ステップサイズの設定が難しかったけど、ナイーブなものよりは性能がよいはず。&lt;/p&gt;
&lt;p&gt;また、軽く見た感じでも自然勾配学習法は発散しやすい。以下の記事にあるように、正則化を掛けたほうが良さそう。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tfjgeorge.github.io/articles/note/2018/11/09/empirical-fisher.html"&gt;What is the empirical Fisher ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ヘッセ行列の計算は少し回り道になったが、理論的最適値との比較において議論できそう。&lt;/p&gt;
&lt;p&gt;逆行列補題を使っていて、最早カルマンフィルタや RLS に近いんでないかと思えてきた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.bode.amp.i.kyoto-u.ac.jp/~kashima/lecture/ss/slide17_8.pdf"&gt;信号とシステム 第 6 章 適応フィルタ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.cs.tut.fi/~tabus/course/ASP/LectureNew10.pdf"&gt;Recursive Least Squares Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;に LMS から RLS まで記述あり。&lt;/p&gt;
&lt;p&gt;上の「信号とシステム」を眺めていたら、NLMS における適応的ステップサイズ決定則が使えそうな印象。NLMS は事後誤差 &lt;span class="math"&gt;\(e^{+}(k)\)&lt;/span&gt; を 0 にするように適応的なステップサイズ &lt;span class="math"&gt;\(\alpha(k)\)&lt;/span&gt; を定める。普通の Signed-LMS では、リファレンス信号 &lt;span class="math"&gt;\(d(k)\)&lt;/span&gt;, フィルタ係数 &lt;span class="math"&gt;\(\ve{h}(k)\)&lt;/span&gt;, 入力データ &lt;span class="math"&gt;\(\ve{x}(k)\)&lt;/span&gt; に対し、事後誤差は次のように展開できる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
e^{+}(k) &amp;amp;= d(k) - \innerp{\ve{h}(k+1)}{\ve{x}(k)} \\
&amp;amp;= d(k) - \innerp{\ve{h}(k) + \alpha(k) \mathrm{sign}[e(k)] \ve{x}(k)}{\ve{x}(k)} \\
&amp;amp;= d(k) - \innerp{\ve{h}(k)}{\ve{x}(k)} - \alpha(k) \mathrm{sign}[e(k)] \innerp{\ve{x}(k)}{\ve{x}(k)} \\
&amp;amp;= e(k) - \alpha(k) \mathrm{sign}[e(k)] ||\ve{x}(k)||_{2}^{2}
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(e^{+}(k) = 0\)&lt;/span&gt; となるように &lt;span class="math"&gt;\(\alpha(k)\)&lt;/span&gt; を選ぶと、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\alpha(k) = \frac{e(k)}{\mathrm{sign}[e(k)] ||\ve{x}(k)||_{2}^{2}} = \frac{|e(k)|}{||\ve{x}(k)||_{2}^{2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;として、事後誤差を最小にするステップサイズが求まった。（Signed-LMS でこういう議論があんまり見られないのはなぜだ？&lt;strong&gt;この&lt;/strong&gt; &lt;span class="math"&gt;\(\alpha(k)\)&lt;/span&gt; &lt;strong&gt;を Signed-LMS の更新則に突っ込むと NLMS になる&lt;/strong&gt; ）&lt;/p&gt;
&lt;p&gt;自然勾配を使った場合が有益（ステップサイズ設定つらい）なので、求めてみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
e^{+}(k) &amp;amp;= d(k) - \innerp{\ve{h}(k+1)}{\ve{x}(k)} \\
&amp;amp;= d(k) - \innerp{\ve{h}(k) + \alpha(k) \mathrm{sign}[e(k)] \ve{F}(k)^{-1} \ve{x}(k)}{\ve{x}(k)} \\
&amp;amp;= d(k) - \innerp{\ve{h}(k)}{\ve{x}(k)} - \alpha(k) \mathrm{sign}[e(k)] \innerp{\ve{F}(k)^{-1}\ve{x}(k)}{\ve{x}(k)} \\
&amp;amp;= e(k) - \alpha(k) \mathrm{sign}[e(k)] \innerp{\ve{x}(k)}{\ve{F}(k)^{-1}\ve{x}(k)}
\end{align*}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\alpha(k) = \frac{e(k)}{\mathrm{sign}[e(k)] \innerp{\ve{x}}{\ve{F}(k)^{-1}\ve{x}(k)}} = \frac{|e(k)|}{\innerp{\ve{x}(k)}{\ve{F}(k)^{-1}\ve{x}(k)}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;が得られる。これは計量としてフィッシャー情報行列の逆行列を使った時の &lt;span class="math"&gt;\(\ve{x}(k)\)&lt;/span&gt; のノルムによる正規化に対応する。すると残差の絶対値が外れる。NLMS とかなり近いけど計量が入っているところが違う。&lt;/p&gt;
&lt;p&gt;実装してみたら実験でも音源に依存せず安定している印象（注意！ノイズのない正弦波で発散した！おそらく、情報行列の要素が全て同一で特異になっている。）。&lt;/p&gt;
&lt;p&gt;結果の意味付けが非常に大事な気がする。資料 35p あたりの議論を当てはまると、何か幾何的な解釈が出てくるはずだ。改めて、ここらへんの議論って誰かやっていないか、気になる。明日はそこを考えてみる。改めて既存研究が無いか見て、報告に移そうか。&lt;/p&gt;
&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Normalize するやつの結果の意味付け&lt;/li&gt;
&lt;li&gt;忘却係数として捉えれば式が簡単にならんか？ &lt;span class="math"&gt;\((\lambda \ve{F} + \ve{x}\ve{x}^{\mathsf{T}})^{-1}\)&lt;/span&gt; で &lt;span class="math"&gt;\(0 &amp;lt; \lambda &amp;lt; 1\)&lt;/span&gt; は 1 に近い係数。&lt;/li&gt;
&lt;li&gt;自然勾配法がなんでうまくいくのか &lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/"&gt;Natural Gradient Descent&lt;/a&gt; を訳しながら理解していく。 &lt;a class="reference external" href="https://arxiv.org/pdf/1412.1193.pdf"&gt;New insights and perspectives on the natural gradient method&lt;/a&gt; も参考になりそう。&lt;/li&gt;
&lt;li&gt;RLS(Recursive Least Square) の更新式の誤差に符号関数を被せたものが、自分が導いているものかも知れないと思い立つ。確認。&lt;/li&gt;
&lt;/ul&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="RLS"></category><category term="Natural Gradient"></category><category term="Empirical Fisher"></category></entry><entry><title>LMS Filterの挙動観察中(8)</title><link href="/lms-filternoju-dong-guan-cha-zhong-8.html" rel="alternate"></link><published>2020-05-04T11:00:00+09:00</published><updated>2020-05-04T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-04:/lms-filternoju-dong-guan-cha-zhong-8.html</id><summary type="html">&lt;p&gt;まだ粘る。GW 終わるまでには何らかのアウトプットがほしい。&lt;/p&gt;
&lt;p&gt;指数移動平均の α を増やすと性能（誤差、エントロピー）が悪化する傾向あり。特に 0.5 以上（瞬間値の重みを大きく）すると、悪化が顕著。&lt;/p&gt;
&lt;p&gt;分散行列の逆行列を見てみると、非常に大きい値を取っていることが分かる。これは特異にかなり近いのではないかと予測している。&lt;/p&gt;
&lt;p&gt;また、指数移動平均で求めた分散行列の対角要素は経験分散に漸近するはずで、対角要素は時間遅延が加わった自分自身との 2 乗和で、全てが同じ値になることを期待していたが、なっていなかった。これは、指数移動平均は入力の順序により最終結果が異なるという状態が現れていると思う。（例：1,1,1,0 という系列と 0,1,1,1 という系列では指数移動平均の結果が異なる。）&lt;/p&gt;
&lt;p&gt;学習率の設定も音源依存でだいぶ変わってしまう印象。ボイスでは 0.0001 が、ピアノでは 0.00001、50Hz サイン波では発散した（恐らくこれはほぼ定常な信号になっているからと思われる …&lt;/p&gt;</summary><content type="html">&lt;p&gt;まだ粘る。GW 終わるまでには何らかのアウトプットがほしい。&lt;/p&gt;
&lt;p&gt;指数移動平均の α を増やすと性能（誤差、エントロピー）が悪化する傾向あり。特に 0.5 以上（瞬間値の重みを大きく）すると、悪化が顕著。&lt;/p&gt;
&lt;p&gt;分散行列の逆行列を見てみると、非常に大きい値を取っていることが分かる。これは特異にかなり近いのではないかと予測している。&lt;/p&gt;
&lt;p&gt;また、指数移動平均で求めた分散行列の対角要素は経験分散に漸近するはずで、対角要素は時間遅延が加わった自分自身との 2 乗和で、全てが同じ値になることを期待していたが、なっていなかった。これは、指数移動平均は入力の順序により最終結果が異なるという状態が現れていると思う。（例：1,1,1,0 という系列と 0,1,1,1 という系列では指数移動平均の結果が異なる。）&lt;/p&gt;
&lt;p&gt;学習率の設定も音源依存でだいぶ変わってしまう印象。ボイスでは 0.0001 が、ピアノでは 0.00001、50Hz サイン波では発散した（恐らくこれはほぼ定常な信号になっているからと思われる。定常な信号では全ての分散と共分散が同じ値になって、行列が特異になる。正則化（分散行列に定数を掛けた単位行列を加算）を行ったら安定した ...）&lt;/p&gt;
&lt;p&gt;行き詰まりを感じ、適応的自然勾配の更新式を逆行列補題（Woodbury の恒等式）から導いていた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://mathtrain.jp/woodbury"&gt;逆行列の補助定理（Woodbury の恒等式）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://sigmagic.net/math/inverse-mat/"&gt;逆行列の公式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;そのときに、論文では勾配ベクトルの分散行列を求めていることに気づく。そして自分が間違っている事がわかった。
情報行列は勾配ベクトルの分散行列だった。（データベクトルの分散行列ではない ...）
いままで入力データの分散行列を計算していたので、これは明確な誤り。&lt;/p&gt;
&lt;p&gt;フィッシャー情報行列はスコア関数（対数尤度関数の勾配）の分散行列で定義される。よって、情報行列とデータの分散行列は一般に一致しない。&lt;/p&gt;
&lt;p&gt;もう一つ自然勾配とフィッシャー情報行列に関する有益な情報源あり :&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tfjgeorge.github.io/articles/note/2018/11/09/empirical-fisher.html"&gt;What is the empirical Fisher ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今一度 Jupyter から出戻りしてみる。
思ったけど、今考えているのは絶対値誤差最小化のために Signed-LMS だけど、データ側を符号とする LMS や、Sign-SignLMS の解析もありじゃないかと思ってきた。データ側を符号とする LMS は何を最小化しているのか？などが気になる。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>LMS Filterの挙動観察中(7)</title><link href="/lms-filternoju-dong-guan-cha-zhong-7.html" rel="alternate"></link><published>2020-05-03T11:00:00+09:00</published><updated>2020-05-03T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-03:/lms-filternoju-dong-guan-cha-zhong-7.html</id><content type="html">&lt;p&gt;実データ適用で、どうも上手く行かない。やっぱり分散行列の逆行列が発散している。&lt;/p&gt;
&lt;p&gt;適応的自然勾配をやめて、真面目に（毎サンプル平均を求めて）計算するようにしているけども結果がよろしくない。分散行列を標本平均ではなくて指数移動平均（α=0.1）に置き換えたらそれなりの性能が出ることを確認。しかし、ラプラス分布の計量を取り入れていない ...。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>LMS Filterの挙動観察中(6)</title><link href="/lms-filternoju-dong-guan-cha-zhong-6.html" rel="alternate"></link><published>2020-05-02T11:00:00+09:00</published><updated>2020-05-02T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-02:/lms-filternoju-dong-guan-cha-zhong-6.html</id><content type="html">&lt;p&gt;今日は実装整理して実データへ適用してみる。気になってるのが適応的自然勾配の更新式。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;確率の重み付けは正規化しないと使えそうにないということ。&lt;/li&gt;
&lt;li&gt;確率の重み付けをしても問題ないか？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実データ適用、うーん性能が良くない！発散する！実装を確認しながら進行中。情報行列の逆行列を正規化すると発散はしないけど、逆行列がほぼ単位行列とほぼ同一で、元の SignedLMS と性能が同等。。。
まずは、適応的自然勾配じゃなくて負荷でかいけど真面目に計算する方針で行ってみる。&lt;/p&gt;
&lt;p&gt;また、フィルタ処理を for 文でやるより numpy の演算にした方が格段に早かった。numpy 大事。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>LMS Filterの挙動観察中(5)</title><link href="/lms-filternoju-dong-guan-cha-zhong-5.html" rel="alternate"></link><published>2020-05-01T11:00:00+09:00</published><updated>2020-05-01T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-05-01:/lms-filternoju-dong-guan-cha-zhong-5.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;別のことをしているときに、ふと適応的自然勾配学習法を弄ってて、なんとなく IRLS に応用できそうな印象が。
以下のような式でヘッセ行列（というか、重み付きの分散行列）を更新する。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{H} \leftarrow \ve{H} + \frac{1}{|y_{i} - \ve{\beta}^{\mathsf{T}} \ve{x …&lt;/div&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;別のことをしているときに、ふと適応的自然勾配学習法を弄ってて、なんとなく IRLS に応用できそうな印象が。
以下のような式でヘッセ行列（というか、重み付きの分散行列）を更新する。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{H} \leftarrow \ve{H} + \frac{1}{|y_{i} - \ve{\beta}^{\mathsf{T}} \ve{x}|} \ve{x} \ve{x}^{\mathsf{T}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;他にも、ICA（独立成分分析）の尖度最大化（優ガウス分布化）の学習がなんか使えないかと考えつつある。でもこれは ICA によるノイズ除去にだいぶ近い話になりそう。&lt;/p&gt;
&lt;p&gt;分散行列と自己相関行列、だいぶ定義が近いな … 間違ってないかなと思って再確認。分散行列と言ってるものはもしかしたら自己相関行列の誤りかもしれない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://mathtrain.jp/correlationmatrix"&gt;相関行列の定義と分散共分散行列との関係&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.ip.info.eng.osaka-cu.ac.jp/~kazunori/paper/rcs201610_handout.pdf"&gt;初学者のための無線通信信号処理入門&lt;/a&gt; に明確に定義されてる。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;平均 0 化していたら分散行列と自己相関行列は同一になりそうな雰囲気。雰囲気じゃだめでちゃんと確認すべき。&lt;/p&gt;
&lt;p&gt;寄り道しすぎたので、改めて結果をまとめていく。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>LMS Filterの挙動観察中(4)</title><link href="/lms-filternoju-dong-guan-cha-zhong-4.html" rel="alternate"></link><published>2020-04-30T11:00:00+09:00</published><updated>2020-04-30T11:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-30:/lms-filternoju-dong-guan-cha-zhong-4.html</id><summary type="html">&lt;p&gt;本日も引き続き発散の原因を追う。
→ ステップサイズを小さくしたら発散しなくなった …。職人芸じゃないかこんなの。NLMS みたく発散しない条件がほしいな。&lt;/p&gt;
&lt;p&gt;本当に既存研究がないか、再度調査。&lt;/p&gt;
&lt;p&gt;自然勾配を適応的に計算する方法を試している。無論、定義式通りに計算するのは問題ないことは確かめているが、計算量が気になるのです。&lt;/p&gt;
&lt;p&gt;パラメータを色々といじりつつ、論文も参照してそれなりのパラメータを見つける。
パラメータについては &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;ved=2ahUKEwi4ufHi4o_pAhUY_GEKHd-gDBwQFjAEegQIBRAB&amp;amp;url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F9%2F21%2F4568%2Fpdf&amp;amp;usg=AOvVaw0KgakdcC8U_T71ks8hZKDW"&gt;Adaptive Natural Gradient Method for Learning of Stochastic Neural Networks in Mini-Batch Mode&lt;/a&gt; を皮切りに調査開始。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.8668&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons&lt;/a&gt; が甘利先生。（福水先生もいるぞ）&lt;ul&gt;
&lt;li&gt;この論文で適応的更新式の導出が述べられる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bsi-ni.brain.riken.jp/database/file/215/221.pdf"&gt;Adaptive natural gradient learning algorithms for various stochastic models …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;本日も引き続き発散の原因を追う。
→ ステップサイズを小さくしたら発散しなくなった …。職人芸じゃないかこんなの。NLMS みたく発散しない条件がほしいな。&lt;/p&gt;
&lt;p&gt;本当に既存研究がないか、再度調査。&lt;/p&gt;
&lt;p&gt;自然勾配を適応的に計算する方法を試している。無論、定義式通りに計算するのは問題ないことは確かめているが、計算量が気になるのです。&lt;/p&gt;
&lt;p&gt;パラメータを色々といじりつつ、論文も参照してそれなりのパラメータを見つける。
パラメータについては &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;ved=2ahUKEwi4ufHi4o_pAhUY_GEKHd-gDBwQFjAEegQIBRAB&amp;amp;url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F9%2F21%2F4568%2Fpdf&amp;amp;usg=AOvVaw0KgakdcC8U_T71ks8hZKDW"&gt;Adaptive Natural Gradient Method for Learning of Stochastic Neural Networks in Mini-Batch Mode&lt;/a&gt; を皮切りに調査開始。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.8668&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons&lt;/a&gt; が甘利先生。（福水先生もいるぞ）&lt;ul&gt;
&lt;li&gt;この論文で適応的更新式の導出が述べられる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bsi-ni.brain.riken.jp/database/file/215/221.pdf"&gt;Adaptive natural gradient learning algorithms for various stochastic models&lt;/a&gt; も甘利先生。（福水先生もいるぞ）&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://downloads.hindawi.com/archive/2011/407497.pdf"&gt;A Simplified Natural Gradient Learning Algorithm&lt;/a&gt; 更にシンプルにしたもの。2011 年。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Toy-problem として中央値の逐次推定とかアリではと、少しだけ思った。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="LAD"></category></entry><entry><title>LMS Filterの挙動観察中(3)</title><link href="/lms-filternoju-dong-guan-cha-zhong-3.html" rel="alternate"></link><published>2020-04-29T23:40:00+09:00</published><updated>2020-04-29T23:40:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-29:/lms-filternoju-dong-guan-cha-zhong-3.html</id><content type="html">&lt;p&gt;LMS はヘッセ行列の逆行列込みの学習ができているが、Signed-LMS は上手く行かない。分散行列が特異になったり、要素が大きくなりすぎて発散してしまう。。。&lt;/p&gt;
</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="LAD"></category></entry><entry><title>LMS Filterの挙動観察中(2)</title><link href="/lms-filternoju-dong-guan-cha-zhong-2.html" rel="alternate"></link><published>2020-04-28T23:40:00+09:00</published><updated>2020-04-28T23:40:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-28:/lms-filternoju-dong-guan-cha-zhong-2.html</id><summary type="html">&lt;p&gt;つまるところ、以下の計算をどうやるか？に尽きる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right] = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1, \varepsilon(n) = 0}^{N} x(n - m) x(n - k)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\varepsilon(n)\)&lt;/span&gt; は i.i.d.（独立に同一の分布）から発生しているので、&lt;span class="math"&gt;\(x(n-m …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;つまるところ、以下の計算をどうやるか？に尽きる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right] = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1, \varepsilon(n) = 0}^{N} x(n - m) x(n - k)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\varepsilon(n)\)&lt;/span&gt; は i.i.d.（独立に同一の分布）から発生しているので、&lt;span class="math"&gt;\(x(n-m), x(n-k)\)&lt;/span&gt; には依存しない（予測係数にも依らず）で勝手に揺れると考える。&lt;/p&gt;
&lt;p&gt;でもそんな計算は見たことがない。そもそも LAD の文脈でこの話は出ているはずで、「Laplace Distribution linear regression」で検索掛けていたら、&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://downloads.hindawi.com/journals/jam/2014/856350.pdf"&gt;Robust Mean Change-Point Detecting through Laplace Linear Regression Using EM Algorithm&lt;/a&gt; を見つけた。&lt;ul&gt;
&lt;li&gt;イントロで「 &lt;strong&gt;ラプラス分布は正規分布の混合で表せる&lt;/strong&gt; 」と「 &lt;strong&gt;混合を前提にした EM アルゴリズムが存在する&lt;/strong&gt; 」というのを見つけて、論文探しが改めて動く。混合ガウス分布を EM アルゴリズムで学習する話はよく聞くから、本質的なのは正規分布の混合で表せていることか。&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://core.ac.uk/download/pdf/16705381.pdf"&gt;ROBUST MIXTURE REGRESSION MODEL FITTING BY LAPLACE DISTRIBUTION&lt;/a&gt; は 2013 年の論文。印象的なのは、 &lt;strong&gt;IRLS は EM アルゴリズムの一種だということ。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi287j_j4rpAhXKA4gKHUlmAIUQFjAAegQIBBAB&amp;amp;url=https%3A%2F%2Fkwansei.repo.nii.ac.jp%2Findex.php%3Faction%3Dpages_view_main%26active_action%3Drepository_action_common_download%26item_id%3D26097%26item_no%3D1%26attribute_id%3D22%26file_no%3D1%26page_id%3D30%26block_id%3D85&amp;amp;usg=AOvVaw0TY8ejg2Duf0nYYdMvrVD_"&gt;ラプラス確率的フロンティアモデルのベイズ推定&lt;/a&gt; は日本語でラプラス分布の混合について述べた論文&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S16/bayeslasso.pdf"&gt;The Bayesian Lasso&lt;/a&gt; は LASSO を、パラメータの事前分布をラプラス分布としたものとして定式化している。ラプラス分布は直接扱わず、混合を考えている。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実験で試している、勾配ベクトルに分散行列（ヘッセ行列）の逆行列を掛ける行為は、ウィーナーフィルタに等しい。しかしウィーナーフィルタは観測分散行列 &lt;span class="math"&gt;\(\ve{XX}^{\mathsf{T}}\)&lt;/span&gt; が正則でないと計算できない。そこで、観測分散行列の低ランク近似を行ってその擬似逆行列を使ってフィルタ係数を更新していく手法がある。それを Reduced rank adaptive filters というらしい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://arxiv.org/pdf/1504.06054.pdf"&gt;A New Approach to Adaptive Signal Processing&lt;/a&gt; で触れていた。この論文は適応フィルタを広汎的に見ており、有益。&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www-users.york.ac.uk/~rcdl500/SPL_JIO_2007.pdf"&gt;Reduced-Rank Adaptive Filtering Based on Joint Iterative Optimization of Adaptive Filters&lt;/a&gt; では Reduced rank adaptive filters のフィルタバンク版&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;邪念が動いて、パーティクルフィルター（粒子フィルター）でパラメータ決められんか考えてる。でも、LMS は状態空間モデルの範疇に入るのだろうか？（カルマンフィルタの一部だから当てはまったはず）。また、一般の状態空間モデルと違って状態は常に観測できるよな。またパーティクルフィルターもシミュレーションベースなので負荷が高そう。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.allisone.co.jp/html/Notes/DSP/Filter/particle-filter/index.html"&gt;パーティクル・フィルタをやさしく解説&lt;/a&gt; が確かに優しい。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.terrapub.co.jp/journals/jjssj/pdf/4401/44010189.pdf"&gt;粒子フィルタの基礎と応用 : フィルタ・平滑化・パラメータ推定&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="mathrm-e-left-delta-varepsilon-n-x-n-m-x-n-k-right"&gt;
&lt;h2&gt;&lt;span class="math"&gt;\(\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]\)&lt;/span&gt; の解釈&lt;/h2&gt;
&lt;p&gt;結局 &lt;span class="math"&gt;\(\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]\)&lt;/span&gt; の解釈から逃げている ...。もう少し考えていたら、残差 &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; は入力ベクトル &lt;span class="math"&gt;\(x\)&lt;/span&gt; と独立であることを思い出した。ここから、次が言える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E} \left[\delta(\varepsilon(n)) x(n - m) x(n - k) \right] = \mathrm{E} \left[ \delta(\varepsilon(n)) \right] \mathrm{E} \left[ x(n - m) x(n - k) \right]
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここで、 &lt;span class="math"&gt;\(\mathrm{E} \left[ \delta(\varepsilon(n)) \right]\)&lt;/span&gt; はお察しの通りで、以下の通りに、やはり残差が 0 となる確率が出てくる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathrm{E} \left[ \delta(\varepsilon(n)) \right] &amp;amp;= \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1}^{N} \delta(\varepsilon(n)) = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1, \varepsilon(n) = 0}^{N} 1 \\
&amp;amp;= P(\varepsilon = 0)
\end{align*}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E} \left[\delta(\varepsilon(n)) x(n - m) x(n - k) \right] = P(\varepsilon = 0) \mathrm{E} \left[ x(n - m) x(n - k) \right]
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(\varepsilon = 0)\)&lt;/span&gt; を考える。まず注意したいのは、連続型確率分布においては一点 0 をとる確率は 0 ということ（測度 0 だから）。近似するしかなく、方針としては、&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;残差の閾値を定めて、それ以下の数値を残差 0 とみなして確率を求める&lt;/li&gt;
&lt;li&gt;離散型確率分布で考える&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ラプラス分布の基本的なことをおさらいすると、確率密度関数 &lt;span class="math"&gt;\(f(x, \mu, \sigma)\)&lt;/span&gt; は、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
f(x, \mu, \sigma) = \frac{1}{2 \sigma} \exp\left( - \frac{|x - \mu|}{\sigma} \right)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;で、観測 &lt;span class="math"&gt;\(x_{1}, ..., x_{N}\)&lt;/span&gt; が得られた時の尤度関数 &lt;span class="math"&gt;\(L(\mu, \sigma)\)&lt;/span&gt; と対数尤度関数は、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
L(\mu, \sigma) &amp;amp;= \prod_{i = 1}^{N} \frac{1}{2 \sigma} \exp\left( - \frac{|x_{i} - \mu|}{\sigma} \right) = \frac{1}{(2 \sigma)^{N}} \prod_{i = 1}^{N} \exp\left( - \frac{|x_{i} - \mu|}{\sigma} \right) \\
\log L(\mu, \sigma) &amp;amp;= -N\log(2\sigma) -\sum_{i = 1}^{N} \frac{|x_{i} - \mu|}{\sigma} = -N\log(2\sigma) - \frac{1}{\sigma}\sum_{i = 1}^{N} |x_{i} - \mu|
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; の最尤推定量は標本中央値となる。&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; が求まったとして、次は &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; の最尤推定値を考える。対数尤度関数を &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; で偏微分すると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{\partial}{\partial \sigma} \log L(\mu, \sigma) = -2 \frac{N}{2\sigma} + \frac{1}{\sigma^{2}} \sum_{i = 1}^{N} |x_{i} - \mu| = -\frac{N}{\sigma} + \frac{1}{\sigma^{2}} \sum_{i = 1}^{N} |x_{i} - \mu|
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac{\partial}{\partial \sigma} \log L(\mu, \sigma) = 0\)&lt;/span&gt; とおいて &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; について解くと、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{N}{\sigma} = \frac{1}{\sigma^{2}} \sum_{i = 1}^{N} |x_{i} - \mu| \Rightarrow \sigma = \frac{1}{N} \sum_{i = 1}^{N} |x_{i} - \mu|
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; の最尤推定値は偏差の絶対値の標本平均となる。次に離散ラプラス分布を考える（ &lt;a class="reference external" href="https://shodhganga.inflibnet.ac.in/bitstream/10603/30871/11/11_chapter%206.pdf"&gt;ここ&lt;/a&gt; を参考にしている）。離散ラプラス分布は次の確率（質量）関数 &lt;span class="math"&gt;\(P\)&lt;/span&gt; を持つ :&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
P(X = k) &amp;amp;= \frac{f(k, 0, \sigma)}{\sum_{j = -\infty}^{\infty} f(j, 0, \sigma)} = \frac{\exp\left( -\frac{|k|}{\sigma} \right)}{\sum_{j = -\infty}^{\infty} \exp\left( -\frac{|j|}{\sigma} \right)} \\
&amp;amp;= \frac{\exp\left( -\frac{|k|}{\sigma} \right)}{1 + 2 \sum_{j = 1}^{\infty} \exp\left( -\frac{j}{\sigma} \right)}
\end{align*}
&lt;/div&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\sum_{j = 1}^{\infty} \exp\left( -\frac{j}{\sigma} \right) = \lim_{n \to \infty} \frac{\exp(-1/\sigma)(1 - \exp(-n/\sigma))}{1 - \exp(-1/\sigma)} = \frac{\exp(-1/\sigma)}{1 - \exp(-1/\sigma)}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
P(X = k) &amp;amp;= \frac{\exp\left( -\frac{|k|}{\sigma} \right)}{1 + 2 \frac{\exp(-1/\sigma)}{1 - \exp(-1/\sigma)}} = \frac{1 - \exp(-1/\sigma)}{1 + \exp(-1/\sigma)} \exp\left(-\frac{|k|}{\sigma}\right) \\
&amp;amp;= \frac{1 - p}{1 + p} p^{|k|}, \quad p = \exp(-1/\sigma)
\end{align*}
&lt;/div&gt;
&lt;p&gt;これは離散型確率分布であることに注意。&lt;/p&gt;
&lt;p&gt;連続版かつ &lt;span class="math"&gt;\(\mu=0\)&lt;/span&gt; で、 &lt;span class="math"&gt;\(|x|\)&lt;/span&gt; がある閾値 &lt;span class="math"&gt;\(\delta &amp;gt; 0\)&lt;/span&gt; 以下となる確率は次のように計算できる :&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
P(|x| \leq \delta) &amp;amp;= \int^{\delta}_{-\delta} f(x, \mu, \sigma) dx = \frac{1}{2 \sigma} \int^{\delta}_{-\delta} \exp\left(-\frac{|x|}{\sigma} \right) dx \\
&amp;amp;= \frac{2}{2\sigma} \int^{\delta}_{0} \exp\left(-\frac{x}{\sigma} \right) dx = \frac{1}{\sigma} (-\sigma) \int^{\delta}_{0} \left\{ \exp\left(-\frac{x}{\sigma} \right) \right\}^{\prime} dx \\
&amp;amp;= -\left[ \exp\left(-\frac{x}{\sigma} \right) \right]^{\delta}_{0} = \exp(0) - \exp\left( - \frac{\delta}{\sigma} \right) \\
&amp;amp;= 1 - \exp\left( - \frac{\delta}{\sigma} \right)
\end{align*}
&lt;/div&gt;
&lt;p&gt;この式により分散行列にかける係数を決めることを考えると、次が考察される。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; が大きい（分散 &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; が小さい）と確率が 1 に近づき、分散行列は LMS のそれと近くなる。&lt;/li&gt;
&lt;li&gt;逆に &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; が小さい（分散 &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; が大きい）と分散行列に小さいスカラーを乗じる。分散行列の逆行列をとると、大きいスカラーを乗じることになり、勾配ベクトルのノルムが大きくなりそう。&lt;/li&gt;
&lt;li&gt;ノイズレベル（&lt;span class="math"&gt;\(\approx\)&lt;/span&gt; 分散）が小さいときは勾配が小さくなり極値付近を精密に調べ、大きい場合は勾配が大きくなりダイナミックに探索空間を動き回りそう。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="lms"&gt;
&lt;h2&gt;LMS の性能解析に関する文献&lt;/h2&gt;
&lt;p&gt;色々さまよっているうちに出てきた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.ee.cityu.edu.hk/~hcso/it6303_4.pdf"&gt;Adaptive Filter Theory and Applications&lt;/a&gt; に LMS のステップサイズのとり方に関する記述あり。証明に有益。&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.dsp-book.narod.ru/DSPMW/19.PDF"&gt;Convergence Issues in the LMS Adaptive Filter&lt;/a&gt; も結構有益。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="todo"&gt;
&lt;h3&gt;TODO&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;評価を続ける。評価がまとまったら結果共有に入りたい。&lt;ul&gt;
&lt;li&gt;LMS の適応動作は、単層パーセプトロンの学習にも該当する。NN の観点からも引き続き論文調査を行うべし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OMP を使う。&lt;/li&gt;
&lt;li&gt;メッセージパッシング使えない？&lt;ul&gt;
&lt;li&gt;何らかの確率モデル化をせよ、というふうに受け取った。&lt;/li&gt;
&lt;li&gt;AMP, Survay-Propagation（三村さん、樺島さん）がありえる。&lt;/li&gt;
&lt;li&gt;→ AMP, Survay-Propagation について調査すべし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;いろんな論文で自然勾配をどうやって定義しているか要観察。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;優先度低&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;パーティクルフィルター使えない？&lt;ul&gt;
&lt;li&gt;今日検討した結果、ちょっと今は保留。大量のサンプルが必要そうに見える。計算負荷を気にした結果、優先度を低くした。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="LAD"></category><category term="IRLS"></category></entry><entry><title>LMS Filterの挙動観察中(1)</title><link href="/lms-filternoju-dong-guan-cha-zhong-1.html" rel="alternate"></link><published>2020-04-27T23:40:00+09:00</published><updated>2020-04-27T23:40:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-27:/lms-filternoju-dong-guan-cha-zhong-1.html</id><summary type="html">&lt;p&gt;引き続き観察中。勾配の計算ミスがあったりして厳しかった。&lt;/p&gt;
&lt;p&gt;問題は、やはりというか SignLMS でのヘッセ行列。&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon((n))x(n-m)x(n-k)]\)&lt;/span&gt; の計算でインパルス応答の扱いをどうするのか ... 連続信号では厳密に 0 を取る確率は 0 だ。だからといって離散的に考えていいのか？&lt;/p&gt;
&lt;p&gt;誤差の絶対値を取って閾値以下ならば分散行列に加算する処理を入れたが、分散行列が特異になること多し。&lt;/p&gt;
&lt;p&gt;デジタル的に考えれば、残差が 0 になる確率で重み付けしていいのでは無いかと思う。 またデジタル的に考えた時
残差が 0 になる確率は、離散ラプラス分布（ &lt;a class="reference external" href="https://shodhganga.inflibnet.ac.in/bitstream/10603/30871/11/11_chapter%206.pdf"&gt;参考資料&lt;/a&gt; ）を元にサンプリング / もしくは重み付けで求める。（サンプリングの場合は [0,1] 乱数を発生させて残差が 0 になる確率よりも小さければ採択する。まじの MC。というか、サンプリングしても重み付けしても同じでは？）分散パラメータは観測分散で求める。&lt;/p&gt;
&lt;p&gt;一旦残差 0 の重み付けで実験を進めているが、まだ残差 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;引き続き観察中。勾配の計算ミスがあったりして厳しかった。&lt;/p&gt;
&lt;p&gt;問題は、やはりというか SignLMS でのヘッセ行列。&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon((n))x(n-m)x(n-k)]\)&lt;/span&gt; の計算でインパルス応答の扱いをどうするのか ... 連続信号では厳密に 0 を取る確率は 0 だ。だからといって離散的に考えていいのか？&lt;/p&gt;
&lt;p&gt;誤差の絶対値を取って閾値以下ならば分散行列に加算する処理を入れたが、分散行列が特異になること多し。&lt;/p&gt;
&lt;p&gt;デジタル的に考えれば、残差が 0 になる確率で重み付けしていいのでは無いかと思う。 またデジタル的に考えた時
残差が 0 になる確率は、離散ラプラス分布（ &lt;a class="reference external" href="https://shodhganga.inflibnet.ac.in/bitstream/10603/30871/11/11_chapter%206.pdf"&gt;参考資料&lt;/a&gt; ）を元にサンプリング / もしくは重み付けで求める。（サンプリングの場合は [0,1] 乱数を発生させて残差が 0 になる確率よりも小さければ採択する。まじの MC。というか、サンプリングしても重み付けしても同じでは？）分散パラメータは観測分散で求める。&lt;/p&gt;
&lt;p&gt;一旦残差 0 の重み付けで実験を進めているが、まだ残差 0 確率が怪しい感じ。（0.93 とかいう現実離れした数値。実際の音声では約 0.09 とかそんなん）&lt;/p&gt;
&lt;p&gt;見やすいようにパラメータを 2 つにしている。2 つにした時でも同じ出力を与える組み合わせがあり、それが直線上に並んでいる事がわかっている。&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>LMS Filterの挙動観察</title><link href="/lms-filternoju-dong-guan-cha.html" rel="alternate"></link><published>2020-04-24T11:40:00+09:00</published><updated>2020-04-24T11:40:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-24:/lms-filternoju-dong-guan-cha.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;今日は Jupyter を使って残差・残差勾配を観察していく。もう夜遅いので notebook 上げるの挑戦できず。
勾配の計算にミスがあり、残差分布と勾配の結果が一致していなかった …3 時間ほど飛ばす。&lt;/p&gt;
&lt;p&gt;ラプラス分布の観測分散が怪しい ...&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://math.stackexchange.com/questions/922521/deriving-mean-and-variance-of-laplace-distribution"&gt;Deriving Mean and Variance of Laplace Distribution&lt;/a&gt; に 1 次元の場合がある。&lt;/li&gt;
&lt;/ul&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_ …&lt;/script&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;今日は Jupyter を使って残差・残差勾配を観察していく。もう夜遅いので notebook 上げるの挑戦できず。
勾配の計算にミスがあり、残差分布と勾配の結果が一致していなかった …3 時間ほど飛ばす。&lt;/p&gt;
&lt;p&gt;ラプラス分布の観測分散が怪しい ...&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://math.stackexchange.com/questions/922521/deriving-mean-and-variance-of-laplace-distribution"&gt;Deriving Mean and Variance of Laplace Distribution&lt;/a&gt; に 1 次元の場合がある。&lt;/li&gt;
&lt;/ul&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="Hessian"></category></entry><entry><title>古い記事の移行/Jupyterの環境整備</title><link href="/gu-iji-shi-noyi-xing-jupyternohuan-jing-zheng-bei.html" rel="alternate"></link><published>2020-04-23T23:00:00+09:00</published><updated>2020-04-23T23:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/gu-iji-shi-noyi-xing-jupyternohuan-jing-zheng-bei.html</id><content type="html">&lt;p&gt;評価の前に古い記事の移行と Python の環境整備。
Python は Jupyter を使う。Vim キーバインドで。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/nakasan/items/ec7741f075f1062350f4#jupyter コマンドモードのショートカットキーまとめ "&gt;Jupyter コマンドモードのショートカットキーまとめ&lt;/a&gt; がよくまとまっていた。これ読んで進めていく。&lt;/li&gt;
&lt;/ul&gt;
</content><category term="雑記"></category><category term="Jupyter"></category></entry><entry><title>Signed-LMSの2階微分 その2</title><link href="/signed-lmsno2jie-wei-fen-sono2.html" rel="alternate"></link><published>2020-04-22T11:34:00+09:00</published><updated>2020-04-22T12:10:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-22:/signed-lmsno2jie-wei-fen-sono2.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;早速既存研究が無いか見ている。二乗誤差最小化の LMS でもヘッセ行列の逆行列の計算負荷が高いから使わん、という論調がほとんど。Signed-LMS については今の所、微分してるところも見てない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.slideshare.net/mentelibre/neural-network-widrowhoff-learning-adaline-hagan-lms"&gt;NEURAL NETWORK Widrow-Hoff Learning Adaline Hagan LMS&lt;/a&gt; 観測分散行列がヘッセ行列に一致することが書いてあった。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www1.coe.neu.edu/~erdogmus/publications/J013_NEUNET_SpIssueIJCNN03_EWCLMS_Yadu.pdf"&gt;Stochastic error whitening algorithm for linear filter estimation with noisy …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;早速既存研究が無いか見ている。二乗誤差最小化の LMS でもヘッセ行列の逆行列の計算負荷が高いから使わん、という論調がほとんど。Signed-LMS については今の所、微分してるところも見てない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.slideshare.net/mentelibre/neural-network-widrowhoff-learning-adaline-hagan-lms"&gt;NEURAL NETWORK Widrow-Hoff Learning Adaline Hagan LMS&lt;/a&gt; 観測分散行列がヘッセ行列に一致することが書いてあった。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www1.coe.neu.edu/~erdogmus/publications/J013_NEUNET_SpIssueIJCNN03_EWCLMS_Yadu.pdf"&gt;Stochastic error whitening algorithm for linear filter estimation with noisy data&lt;/a&gt; 評価関数として絶対値が入ったものを使っている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faculty.cord.edu/kamel/09S-380/Presentations/LMS.pdf"&gt;The Least Mean Squares Algorithm&lt;/a&gt; 分かりやすめな解説。そうか、ウィーナーフィルタか。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;行列 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; が正則にならない件について、これ正則化すればいいんじゃねと思い立つ。要は &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; を正則化パラメータとして &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; に対して逆行列を求めていく。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;多分、係数側に正則項を追加することになるはず。&lt;span class="math"&gt;\(\min \mathrm{E}[|\varepsilon(n)|] + \lambda ||\ve{h}||_{2}\)&lt;/span&gt; のような定式化か？&lt;/li&gt;
&lt;li&gt;それでも逆行列 &lt;span class="math"&gt;\((\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I})^{-1}\)&lt;/span&gt; を求めるのは骨が折れそう。そこで、自然勾配学習で使っていた適応的自然勾配学習法（ &lt;a class="reference external" href="https://bsi-ni.brain.riken.jp/database/file/274/280.pdf"&gt;Singularities Affect Dynamics of Learning in Neuromanifolds&lt;/a&gt; より）が使えそう。具体的には、次の式で自然勾配を適応的に求めていく。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{G}_{t+1}^{-1} = (1 + \varepsilon_{t}) \ve{G}_{t}^{-1} - \varepsilon_{t} \ve{G}_{t}^{-1} \parfrac{J(\ve{h})}{\ve{h}} \left( \ve{G}_{t}^{-1} \parfrac{J(\ve{h})}{\ve{h}} \right)^{\mathsf{T}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\varepsilon_{t}\)&lt;/span&gt; は小さな定数。『情報幾何の新展開』では、カルマンフィルタ由来らしい。うーん、もう試してみたいな。&lt;/p&gt;
&lt;div class="section" id="ve-x-ve-x-mathsf-t-lambda-ve-i"&gt;
&lt;h2&gt;（念の為） &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; が正則行列になる理由&lt;/h2&gt;
&lt;p&gt;すぐに思い出せなくてヒヤッとしたのでここで示しておく。&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; は対称行列だから、直交行列 &lt;span class="math"&gt;\(\ve{P}\)&lt;/span&gt; （&lt;span class="math"&gt;\(\ve{P}^{-1} = \ve{P}^{\mathsf{T}}\)&lt;/span&gt; ）と固有値を並べた対角行列 &lt;span class="math"&gt;\(\ve{\Lambda}\)&lt;/span&gt; を用いて、&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} = \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P}\)&lt;/span&gt; と対角化できる。よって、&lt;span class="math"&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; なる定数を用いた時、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I} &amp;amp;= \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} + \lambda \ve{P}^{\mathsf{T}} \ve{P} \\
&amp;amp;= \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} + \ve{P}^{\mathsf{T}} \lambda \ve{I} \ve{P} \\
&amp;amp;= \ve{P}^{\mathsf{T}} (\ve{\Lambda} + \lambda \ve{I}) \ve{P}
\end{align*}
&lt;/div&gt;
&lt;p&gt;また、任意のベクトル &lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt; を使った時、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{v}^{\mathsf{T}} \ve{X} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= (\ve{X}^{\mathsf{T}} \ve{v})^{\mathsf{T}} \ve{X}^{\mathsf{T}} \ve{v} = ||\ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} \\
\ve{v}^{\mathsf{T}} \ve{X} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= \ve{v}^{\mathsf{T}} \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} \ve{v} = \sum_{i}^{N} \ve{\Lambda}_{ii} (\ve{Pv})_{i}^{2} \\
\Rightarrow ||\ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} &amp;amp;= \sum_{i}^{N} \ve{\Lambda}_{ii} (\ve{Pv})_{i}^{2} \geq 0
\end{align*}
&lt;/div&gt;
&lt;p&gt;の関係式が成り立つ。最後の不等式が成り立つには、全ての &lt;span class="math"&gt;\(i\)&lt;/span&gt; に対して &lt;span class="math"&gt;\(\ve{\Lambda}_{ii} \geq 0\)&lt;/span&gt; でなければならない。よって &lt;span class="math"&gt;\(\ve{XX}^{\mathsf{T}}\)&lt;/span&gt; の固有値は全て非負。&lt;/p&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\ve{P}^{\mathsf{T}} (\ve{\Lambda} + \lambda \ve{I}) \ve{P}\)&lt;/span&gt; に注目すると、全ての固有値に &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; が足されていることが分かる。&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; は正だから、 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; の固有値は全て正になり正定値行列となる。正定値行列は正則だから、 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; は正則行列。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;フィッシャー情報行列とヘッセ行列と分散行列の絡みについて&lt;/h2&gt;
&lt;p&gt;以下の記事が非常にわかりやすい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/"&gt;Fisher Information Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/"&gt;Natural Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;結論、ラプラス分布に従う残差を仮定した最尤推定において、観測分散行列はフィッシャー情報行列に一致し、その逆行列は自然勾配に該当するはず。つうかニュートン法の特殊ケースに見えるがどうなんでしょ。フィッシャー情報行列がヘッセ行列に見えるんだが、定義通り（対数尤度のヘッセ行列）そうだよな。指数族の最尤推定をニュートン法で解こうとしたら全部自然勾配学習法にならね？&lt;/p&gt;
&lt;div class="section" id="todo"&gt;
&lt;h3&gt;TODO&lt;/h3&gt;
&lt;p&gt;評価のことを考えて行きたい。固定した信号（答えが分かっている信号。乱数固定。）を使ったときに、誤差平面と勾配はどうなっている？フィルタの次元は 2 ぐらいにして、フィルタを固定して各統計量がどうなっているかプロットする。まずは絶対値残差と勾配の観察が重要に思える（もちろん、2 次の最小化ケースも重要）。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;評価がまとまったら結果共有に入りたい。&lt;/li&gt;
&lt;li&gt;OMP を使う。&lt;/li&gt;
&lt;li&gt;メッセージパッシング使えない？&lt;ul&gt;
&lt;li&gt;何らかの確率モデル化をせよ、というふうに受け取った。&lt;/li&gt;
&lt;li&gt;AMP, Survay-Propagation（三村さん、樺島さん）がありえる。&lt;/li&gt;
&lt;li&gt;→ AMP, Survay-Propagation について調査すべし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;いろんな論文で自然勾配をどうやって定義しているか要観察。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category><category term="Fisher Information Matrix"></category><category term="Hessian"></category><category term="Natural Gradient"></category></entry><entry><title>残差勾配 \(\mathrm{E}[\varepsilon(n) x(n - m)]\) の挙動観察/Signed-LMSの目的関数の2階微分</title><link href="/can-chai-gou-pei-mathrmevarepsilonn-xn-m-noju-dong-guan-cha-signed-lmsnomu-de-guan-shu-no2jie-wei-fen.html" rel="alternate"></link><published>2020-04-21T12:10:00+09:00</published><updated>2020-04-21T12:10:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-21:/can-chai-gou-pei-mathrmevarepsilonn-xn-m-noju-dong-guan-cha-signed-lmsnomu-de-guan-shu-no2jie-wei-fen.html</id><summary type="html">&lt;div class="section" id="mathrm-e-varepsilon-n-x-n-m"&gt;
&lt;h2&gt;残差勾配 &lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; の挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が大きいときは無視できるのでは？ なお、長時間平均値は 0 に収束していることを見た。
&lt;span class="math"&gt;\(m\)&lt;/span&gt; をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;ガチャガチャ弄ってるってるけど示唆があんまりない。&lt;/li&gt;
&lt;li&gt;低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; は &lt;span class="math"&gt;\(m\)&lt;/span&gt; を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。&lt;/li&gt;
&lt;li&gt;同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一の sin 波を等価させたときはわかりやすい。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="440.0Hz の sin 波に対する各タップの平均勾配変化グラフ " src="./images/sin_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;440.0Hz の sin 波に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt=" ボイス対する各タップの平均勾配変化グラフ " src="./images/voice_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ボイス対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt=" ピアノ演奏に対する各タップの平均勾配変化グラフ " src="./images/ruriko_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ピアノ演奏に対する各タップの平均勾配変化 …&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="mathrm-e-varepsilon-n-x-n-m"&gt;
&lt;h2&gt;残差勾配 &lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; の挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が大きいときは無視できるのでは？ なお、長時間平均値は 0 に収束していることを見た。
&lt;span class="math"&gt;\(m\)&lt;/span&gt; をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;ガチャガチャ弄ってるってるけど示唆があんまりない。&lt;/li&gt;
&lt;li&gt;低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; は &lt;span class="math"&gt;\(m\)&lt;/span&gt; を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。&lt;/li&gt;
&lt;li&gt;同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一の sin 波を等価させたときはわかりやすい。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="440.0Hz の sin 波に対する各タップの平均勾配変化グラフ " src="./images/sin_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;440.0Hz の sin 波に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt=" ボイス対する各タップの平均勾配変化グラフ " src="./images/voice_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ボイス対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt=" ピアノ演奏に対する各タップの平均勾配変化グラフ " src="./images/ruriko_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ピアノ演奏に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="signed-lms2"&gt;
&lt;h2&gt;Signed-LMS の目的関数の 2 階微分&lt;/h2&gt;
&lt;p&gt;勇気を出してやってみる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;符号関数を &lt;span class="math"&gt;\(\tanh(Tx)\)&lt;/span&gt; で近似して微分してみる（&lt;span class="math"&gt;\(T\)&lt;/span&gt; は温度パラメータで、&lt;span class="math"&gt;\(\tanh(Tx)\)&lt;/span&gt; を &lt;span class="math"&gt;\(T \to \infty\)&lt;/span&gt; ならしめれば符号関数に近づく）と、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{d}{dx} \tanh(Tx) = T (\tanh(Tx))^{\prime} = T(1 - \tanh^{2}(Tx))
\end{equation*}
&lt;/div&gt;
&lt;p&gt;さて、 &lt;span class="math"&gt;\(1 - \tanh^{2}(Tx)\)&lt;/span&gt; に注目すると、&lt;span class="math"&gt;\(T\)&lt;/span&gt; の極限では &lt;span class="math"&gt;\(x = 0\)&lt;/span&gt; を除き 0 を取るが、&lt;span class="math"&gt;\(x = 0\)&lt;/span&gt; において 1 を取る。よってこれはインパルス関数になる（極限と微分操作を交換したけどやかましいことは暗黙で ...）。&lt;/p&gt;
&lt;p&gt;符号関数を微分するとインパルス関数が出てくることについては &lt;a class="reference external" href="https://teenaka.at.webry.info/201301/article_10.html"&gt;超関数的微分 _δ 関数関連（２）&lt;/a&gt; を見るのが早いかも。以下では、その話に従って、&lt;span class="math"&gt;\(\frac{d}{dx} \mathrm{sign}(x) = 2\delta(x)\)&lt;/span&gt; とする。&lt;/p&gt;
&lt;p&gt;さて、今一度評価関数 &lt;span class="math"&gt;\(\mathrm{E}[|\varepsilon(n)|]\)&lt;/span&gt; の偏微分と 2 階の偏導関数を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{}{h(m)} \mathrm{E}[|\varepsilon(n)|] &amp;amp;= \mathrm{E}\left[ \parfrac{}{h(m)} |\varepsilon(n)| \right] \\
&amp;amp;= \mathrm{E}\left[ \left\{ \parfrac{}{h(m)} \varepsilon(n) \right\} \mathrm{sign}[\varepsilon(n)] \right] \\
&amp;amp;= -\mathrm{E}\left[ \mathrm{sign}[\varepsilon(n)]  x(n - m) \right] \\
\frac{\partial^{2}}{\partial h(m) \partial h(k)} \mathrm{E}[|\varepsilon(n)|] &amp;amp;= - \parfrac{}{h(k)} \mathrm{E}\left[ \mathrm{sign}[\varepsilon(n)]  x(n - m) \right] \\
&amp;amp;= - \mathrm{E}\left[ \left\{ \parfrac{}{h(k)} \varepsilon(n) \right\} 2\delta(\varepsilon(n)) x(n - m) \right] \\
&amp;amp;= 2\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]
\end{align*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]\)&lt;/span&gt; に注目する。これは &lt;span class="math"&gt;\(\varepsilon(n) = 0\)&lt;/span&gt; のときだけ和を取る演算だ。&lt;span class="math"&gt;\(\sum\)&lt;/span&gt; を用いると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right] = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1, \varepsilon(n) = 0}^{N} x(n - m) x(n - k)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;という計算に該当する。厳密計算は &lt;span class="math"&gt;\(\varepsilon(n) = 0\)&lt;/span&gt; なる &lt;span class="math"&gt;\(n\)&lt;/span&gt; を見つけたら足していく感じでいいと思うけど、今は &lt;span class="math"&gt;\(\varepsilon(n)\)&lt;/span&gt; はラプラス分布に従うと仮定している。だからラプラス分布に従って &lt;span class="math"&gt;\(P(\varepsilon(n) = 0) = \frac{1}{2\lambda}\)&lt;/span&gt; （分散 &lt;span class="math"&gt;\(2\lambda^{2}\)&lt;/span&gt; ）の重み付けをして計算してしまって良いように見えるのだがどうなんだろう。なんか怪しくて考え続けている。&lt;/p&gt;
&lt;p&gt;もし適応フィルタに組み込むなら、残差が 0 になったら上の式に従ってヘッセ行列を更新し、ニュートン法を使い続ける。これは試してみたい。問題はヘッセ行列が逆行列を持つかというところ …4-20 で半正定値であることは確認したが正定値とは限らない。共役勾配法を検討する必要があるかも。&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; は正則になるとは思えない …。（軽く試したけどすぐにだめな例が見つかった。）&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h3&gt;他の頂いたアイディア&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;周波数領域に一旦飛ばすのはあり？&lt;ul&gt;
&lt;li&gt;ありだけど計算量が高い。圧縮率が上がるのであれば大アリ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;確率的 PCA とか使えない？辞書は小さくて済む。&lt;/li&gt;
&lt;li&gt;線形ダイナミクスにより上手く定式化できない？&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="id2"&gt;
&lt;h4&gt;優先度低&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;出す学会については先生に聞くこと。&lt;ul&gt;
&lt;li&gt;相談する機会はどこかで絶対に必要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;著作権処理済み音源データベースについて相談&lt;ul&gt;
&lt;li&gt;→ 自分で情報をまとめて、申し込んでいいかというところまで進めるべし。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://staff.aist.go.jp/m.goto/PAPER/SIGMUS200205goto.pdf"&gt;RWC 研究用音楽データベース : 音楽ジャンルデータベースと楽器音データベース&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://staff.aist.go.jp/m.goto/RWC-MDB/index-j.html"&gt;RWC 研究用音楽データベース&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;→ 進めた。動けるようになったら書類をまとめていく。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Donoho さんなどが圧縮センシングの文脈で既にやりきってない？&lt;ul&gt;
&lt;li&gt;ありえる。調査すべし。&lt;/li&gt;
&lt;li&gt;→ ライス大学では成果をすべて公開しているから見るだけ見たほうが良い。&lt;/li&gt;
&lt;li&gt;→ &lt;a class="reference external" href="http://dsp.rice.edu/cs/"&gt;http://dsp.rice.edu/cs/&lt;/a&gt; を見よ。&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://hal.archives-ouvertes.fr/hal-00424165/document"&gt;Compressed sensing block MAP-LMS adaptive filter for sparse channel estimation and a bayesian Cramer-Rao bound&lt;/a&gt; 残差はガウス分布としてるけどクラメル - ラオ下限との絡みを述べている。何か重要そう。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.dbabacan.info/papers/babacan_CS.pdf"&gt;Bayesian Compressive Sensing Using Laplace Priors&lt;/a&gt; これもパラメータの事前分布にラプラス分布を導入してベイズ推定するもの。残差ではないはず。&lt;/li&gt;
&lt;li&gt;「L1」, 「Laplace」, 「residual」, 「lossless」で検索したけどスパース解を求めるものばかり。今のところはセーフ？&lt;/li&gt;
&lt;li&gt;→ 継続して調査はする。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="SignedLMS"></category></entry><entry><title>IRLSの更新式について</title><link href="/irlsnogeng-xin-shi-nitsuite.html" rel="alternate"></link><published>2020-04-20T14:10:00+09:00</published><updated>2020-04-21T12:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-20:/irlsnogeng-xin-shi-nitsuite.html</id><summary type="html">&lt;p&gt;MathJax の環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 &lt;tt class="docutils literal"&gt;newcommand&lt;/tt&gt; を行えばずっと使えるみたい。便利。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逐次的更新の件について。IRLS では以下の評価関数 &lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; の最小化を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
J(\ve{\beta}) = \sum^{M}_{i = 1} w_{i} (y_ …&lt;/div&gt;</summary><content type="html">&lt;p&gt;MathJax の環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 &lt;tt class="docutils literal"&gt;newcommand&lt;/tt&gt; を行えばずっと使えるみたい。便利。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逐次的更新の件について。IRLS では以下の評価関数 &lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; の最小化を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
J(\ve{\beta}) = \sum^{M}_{i = 1} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})^{2}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(M\)&lt;/span&gt; は観測数。これは二次式だから評価関数は凸関数になる。早速 &lt;span class="math"&gt;\(\ve{\beta}\)&lt;/span&gt; で偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{}{\ve{\beta}} J(\ve{\beta}) &amp;amp;= \sum^{M}_{i = 1} w_{i} 2 \left(- \frac{\partial}{\partial \ve{\beta}} \innerp{\ve{\beta}}{\ve{x}_{i}} \right) (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \\
 &amp;amp;= -2 \sum^{M}_{i = 1} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \ve{x}_{i}
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\parfrac{}{\ve{\beta}} J(\ve{\beta}) = 0\)&lt;/span&gt; とおくと、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\sum_{i = 1}^{M} w_{i} \innerp{\ve{\beta}}{\ve{x}_{i}} \ve{x}_{i} &amp;amp;= \sum_{i = 1}^{M} w_{i} y_{i} \ve{x}_{i} \\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{c}
   w_{1} \innerp{\ve{\beta}}{\ve{x}_{1}} \\
   \vdots     \\
   w_{M} \innerp{\ve{\beta}}{\ve{x}_{M}}
 \end{array}
\right]
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{c}
   w_{1}y_{1} \\
   \vdots     \\
   w_{M}y_{M}
 \end{array}
\right]
\\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   \innerp{\ve{\beta}}{\ve{x}_{1}} \\
   \vdots     \\
   \innerp{\ve{\beta}}{\ve{x}_{M}}
 \end{array}
\right]
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   y_{1} \\
   \vdots     \\
   y_{M}
 \end{array}
\right]
\\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   \ve{x}_{1}^{\mathsf{T}} \\
   \vdots     \\
   \ve{x}_{M}^{\mathsf{T}}
 \end{array}
\right]
\ve{\beta}
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\ve{y}
\\
\iff
\ve{X} \ve{W} \ve{X}^{\mathsf{T}} \ve{\beta} &amp;amp;= \ve{X} \ve{W} \ve{y}
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\ve{X}\ve{W}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; が正則（TODO: &lt;span class="math"&gt;\(\ve{X}\)&lt;/span&gt; が行フルランク、かつ &lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; が正則なら行けそうに見えるけど本当か？）の場合は閉形式で係数が求まる :&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{\beta} = (\ve{X} \ve{W} \ve{X}^{\mathsf{T}})^{-1} \ve{X} \ve{W} \ve{y}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここまでは一般論。さて、更新式に注目する。&lt;span class="math"&gt;\(\beta_{j}\)&lt;/span&gt; だけで偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{J(\ve{\beta})}{\beta_{j}} &amp;amp;= \sum_{i = 1}^{M} \parfrac{}{\beta_{j}} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})^{2} \\
&amp;amp;= -2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})
\end{align*}
&lt;/div&gt;
&lt;p&gt;残差の L1 ノルム最小化を考えるときは &lt;span class="math"&gt;\(w_{i} = \frac{1}{|y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}|}\)&lt;/span&gt; とおくので代入すると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\parfrac{J(\ve{\beta})}{\beta_{j}} = -2 \sum_{i = 1}^{M} (\ve{x}_{i})_{j} \mathrm{sign}(y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;瞬間値（&lt;/strong&gt; &lt;span class="math"&gt;\(M=1\)&lt;/span&gt; &lt;strong&gt;とする）を考えると Signed-LMS の更新式そのものになっている。&lt;/strong&gt; 和を取ると平均操作に近いから、LMS アルゴリズムと考えていることは同じ。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\parfrac{J(\ve{\beta})}{\beta_{j}}\)&lt;/span&gt; を更に &lt;span class="math"&gt;\(\beta_{k}\)&lt;/span&gt; で偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial^{2} J(\ve{\beta})}{\partial \beta_{j} \partial \beta_{k}} &amp;amp;= -2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} \parfrac{}{\beta_{k}} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \\
&amp;amp;= 2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (\ve{x}_{i})_{k} \\
&amp;amp;= 2 \left[ (\ve{x}_{1})_{j} \dots (\ve{x}_{M})_{j} \right]
 \left[
  \begin{array}{c}
    w_{1} (\ve{x}_{1})_{k} \\
    \vdots     \\
    w_{M} (\ve{x}_{M})_{k}
  \end{array}
 \right]
 = 2 \left[ (\ve{x}_{1})_{j} \dots (\ve{x}_{M})_{j} \right] \ve{W}
 \left[
  \begin{array}{c}
    (\ve{x}_{1})_{k} \\
    \vdots     \\
    (\ve{x}_{M})_{k}
  \end{array}
 \right]
\end{align*}
&lt;/div&gt;
&lt;p&gt;2 次式が出てくるのがわかる（&lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; は計量だ）。そして &lt;span class="math"&gt;\((\ve{H})_{jk} = \frac{\partial^{2} J(\ve{\beta})}{\partial \beta_{j} \partial \beta_{k}}\)&lt;/span&gt; なるヘッセ行列 &lt;span class="math"&gt;\(\ve{H}\)&lt;/span&gt; は以下 :&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{H} = 2 \ve{X} \ve{W} \ve{X}^{\mathsf{T}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ヘッセ行列の性質により関数の最小値・最大値の存在がわかる。対称行列なのは間違いない（&lt;span class="math"&gt;\((\ve{X})_{ij} = (\ve{X})_{ji}\)&lt;/span&gt; は自明）。（固有値分解とは見れない。&lt;span class="math"&gt;\(\ve{H}\)&lt;/span&gt; は &lt;span class="math"&gt;\(N \times N\)&lt;/span&gt; の行列であるのに対して、&lt;span class="math"&gt;\(\ve{X}\)&lt;/span&gt; は &lt;span class="math"&gt;\(N \times M\)&lt;/span&gt; の行列。&lt;span class="math"&gt;\(\ve{X} \ve{X}^{\mathsf{T}}\)&lt;/span&gt; は平均化、除算を抜いた分散共分散行列になり半正定値行列。）また、任意のベクトル &lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt; に対して、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{v}^{\mathsf{T}} \ve{X} \ve{W} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= \ve{v}^{\mathsf{T}} \ve{X} \ve{W}^{1/2} \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} \\
&amp;amp;= (\ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v})^{\mathsf{T}} \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} \\
&amp;amp;= || \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} \geq 0
\end{align*}
&lt;/div&gt;
&lt;p&gt;だから、&lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; が半正定値（&lt;span class="math"&gt;\(\iff\)&lt;/span&gt; すべての重みが非負）ならばヘッセ行列は半正定値行列で、極小値が最小値になる。また、&lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; は凸関数（半正定値だから狭義の凸関数ではない）。
もう少しヘッセ行列を見る。ヘッセ行列を上手く使えたらニュートン法で解けそうな気がして。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
(\ve{H})_{jk} = 2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (\ve{x}_{i})_{k}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;より、スペクトル分解的に見ると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{1}{2} \ve{H} &amp;amp;=
w_{1} \left[
  \begin{array}{ccc}
    (\ve{x}_{1})_{1}^{2}  &amp;amp; \dots &amp;amp; (\ve{x}_{1})_{1} (\ve{x}_{1})_{N} \\
    \vdots &amp;amp; \ddots &amp;amp; \vdots \\
    (\ve{x}_{1})_{N} (\ve{x}_{1})_{1} &amp;amp; \dots &amp;amp; (\ve{x}_{1})_{N}^{2} \\
  \end{array}
 \right]
 + \dots +
 w_{M} \left[
  \begin{array}{ccc}
    (\ve{x}_{M})_{1}^{2}  &amp;amp; \dots &amp;amp; (\ve{x}_{M})_{1} (\ve{x}_{M})_{N} \\
    \vdots &amp;amp; \ddots &amp;amp; \vdots \\
    (\ve{x}_{M})_{N} (\ve{x}_{M})_{1} &amp;amp; \dots &amp;amp; (\ve{x}_{M})_{N}^{2} \\
  \end{array}
 \right] \\
 &amp;amp;= w_{1} \ve{x}_{1} \ve{x}_{1}^{\mathsf{T}} + \dots + w_{M} \ve{x}_{M} \ve{x}_{M}^{\mathsf{T}} \\
 &amp;amp;= \sum_{i = 1}^{M} w_{i} \ve{x}_{i} \ve{x}_{i}^{\mathsf{T}}
\end{align*}
&lt;/div&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;信号処理的には &lt;span class="math"&gt;\(\ve{x}_{1}, \ve{x}_{2}, \dots \ve{x}_{M}\)&lt;/span&gt; は系列で現れる。&lt;/li&gt;
&lt;li&gt;LMS フィルタでは &lt;span class="math"&gt;\(i = 1\)&lt;/span&gt; の時だけを考えていたと考えられれる。 &lt;span class="math"&gt;\(i = 2,\dots,M\)&lt;/span&gt; のときの影響は少ないのではないかと思う。&lt;/li&gt;
&lt;li&gt;FIR フィルタを考えるのならば、各 &lt;span class="math"&gt;\(\ve{x}_{1}\)&lt;/span&gt; は入ってきた 1 次元信号データを時系列順に並べたものだから、直前のベクトル &lt;span class="math"&gt;\(\ve{x}_{2}\)&lt;/span&gt; を使えそうな構造に見える。&lt;/li&gt;
&lt;li&gt;上の仮定を使ってヘッセ行列の逆行列 &lt;span class="math"&gt;\(\ve{H}^{-1}\)&lt;/span&gt; を逐次近似計算できない？&lt;/li&gt;
&lt;li&gt;分散共分散行列がほぼヘッセ行列になってるけどこれは何？&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.iim.cs.tut.ac.jp/~kanatani/papers/jcov.pdf"&gt;金谷さんの解説&lt;/a&gt; にそれとなく解説がある。フィッシャー情報行列との関連もある。。。クラメル・ラオの下限についてわかりやすい説明あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://web.econ.keio.ac.jp/staff/bessho/lecture/09/091014ML.pdf"&gt;最尤法&lt;/a&gt; にもそれとなく解説あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://oku.edu.mie-u.ac.jp/~okumura/stat/141115.html"&gt;奥村さん&lt;/a&gt; もあり。観測からヘッセ行列を構成できる？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;そして自然勾配のアイディアが出てくる。自然勾配を使った LMS アルゴリズムは … あった …&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://scholarsmine.mst.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&amp;amp;httpsredir=1&amp;amp;article=2780&amp;amp;context=ele_comeng_facwork"&gt;Normalized Natural Gradient Adaptive Filtering for Sparse and Nonsparse Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.7538&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;甘利先生による解説&lt;/a&gt; で、LMS アルゴリズム含めて大まかなところはだいたい言ってる。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/profile/Ligang_Liu3/publication/44098179_On_Improvement_of_Proportionate_Adaptive_Algorithms_for_Sparse_Impulse_Response/links/00b495315266ab9cfd000000.pdf"&gt;高知工科大学の博論&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ワンチャンス L1 残差最小化はやってないかも。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;前の MTG で言われたことの整理&lt;/li&gt;
&lt;li&gt;分散行列、ヘッセ行列、フィッシャー情報行列、自然勾配の整理&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/"&gt;Fisher Information Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OMP が気になる。試してみたい。&lt;/li&gt;
&lt;/ul&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="IRLS"></category><category term="L1ノルム"></category><category term="LAD"></category></entry><entry><title>IRLS(Iteratively Reweighted Least Squares) その2</title><link href="/irlsiteratively-reweighted-least-squares-sono2.html" rel="alternate"></link><published>2020-04-19T19:30:00+09:00</published><updated>2020-04-20T14:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-19:/irlsiteratively-reweighted-least-squares-sono2.html</id><summary type="html">&lt;p&gt;理論ばっかり追っていて悶々してきたので、IRLS で L1 残差最小化が解けないか実験してみる。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/kibo35/sparse-modeling/blob/master/ch05.ipynb"&gt;第 5 章 厳密解から近似解へ&lt;/a&gt; に『スパースモデリング』5 章の Python 実装あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/kibo35/items/66ec4479b0899ea4987d#irls の概要 "&gt;スパースモデリング：第 3 章 追跡アルゴリズム&lt;/a&gt; は『スパースモデリング』3 章の Python 実装。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IRLS の実装は &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; を参考に。Python で簡単にできた。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;

&lt;span class="c1"&gt;# IRLS 法により Phi @ x = y のスパース解を求める&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;EPSILON&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.0 …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;理論ばっかり追っていて悶々してきたので、IRLS で L1 残差最小化が解けないか実験してみる。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/kibo35/sparse-modeling/blob/master/ch05.ipynb"&gt;第 5 章 厳密解から近似解へ&lt;/a&gt; に『スパースモデリング』5 章の Python 実装あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/kibo35/items/66ec4479b0899ea4987d#irls の概要 "&gt;スパースモデリング：第 3 章 追跡アルゴリズム&lt;/a&gt; は『スパースモデリング』3 章の Python 実装。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IRLS の実装は &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; を参考に。Python で簡単にできた。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;

&lt;span class="c1"&gt;# IRLS 法により Phi @ x = y のスパース解を求める&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;EPSILON&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# 重みの計算&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# 小さくなりすぎた重みは打ち切る&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;EPSILON&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EPSILON&lt;/span&gt;
    &lt;span class="c1"&gt;# 対角行列に展開&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;order&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# 更新後の係数 : Phi.T @ W @ Phi @ x = Phi.T @ W @ y の解&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;DIMENSION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

    &lt;span class="c1"&gt;# 解ベクトル&lt;/span&gt;
    &lt;span class="n"&gt;X_ANSWER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;xhistory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# 観測を生成&lt;/span&gt;
    &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;X_ANSWER&lt;/span&gt;
    &lt;span class="c1"&gt;# 加法的雑音を重畳&lt;/span&gt;
    &lt;span class="c1"&gt;# yrand = y + numpy.random.normal(0, 0.3, (NUM_SAMPLES, 1))&lt;/span&gt;
    &lt;span class="n"&gt;yrand&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;laplace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;emp_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# IRLS を繰り返し適用&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yrand&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;xhistory&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;
        &lt;span class="n"&gt;emp_error&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yrand&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;実装は楽だったけど、誤差解析が沼。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;誤差を重畳してみると、真の誤差と経験誤差が当然一致しない。&lt;/li&gt;
&lt;li&gt;経験誤差的には局所解に入っている印象。&lt;/li&gt;
&lt;li&gt;サンプル数が少ないと大域最小解に入らないケースあり（経験誤差曲面の最小値が真の誤差の曲面の最小値に不一致）&lt;/li&gt;
&lt;li&gt;経験誤差の曲面は二次曲線に見える。（2 次式の最小化を考えているから当然のはず。）&lt;/li&gt;
&lt;li&gt;最小二乗解よりも誤差が悪い時がある。最小二乗解は order=2 とすれば良くて、その時重み行列 W は単位行列になり、普通の最小二乗法と一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;思いつき :&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;IRLS は評価関数の最小化を考える時閉形式で求まるので何も考えない。パラメータに関してもう一度微分できるのでニュートン法使えそう。&lt;/li&gt;
&lt;li&gt;フィルタのときのように逐次的に求められない？&lt;ul&gt;
&lt;li&gt;パラメータ全てではなく 1 こずつ。サンプルについても 1 こずつ。更新していく。評価関数の最小化は平均値の最小化に見受けられるので、逐次的に更新しても良いように見える。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今日は遅いのでもう寝る。&lt;/p&gt;
</content><category term="雑記"></category><category term="LAD"></category><category term="IRLS"></category><category term="L1ノルム"></category></entry><entry><title>IRLS(Iteratively Reweighted Least Squares)</title><link href="/irlsiteratively-reweighted-least-squares.html" rel="alternate"></link><published>2020-04-18T17:30:00+09:00</published><updated>2020-04-19T00:19:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-18:/irlsiteratively-reweighted-least-squares.html</id><summary type="html">&lt;p&gt;LAD(Least Absolute Deviation) を近似的・逐次的に解く方法としての IRLS について調査。そういえば基本的な原理を抑えていなかった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://retrofocus28.blogspot.com/2015/09/iteratively-reweighted-least-squares.html"&gt;Iteratively Reweighted Least Squares　についてサクッと。&lt;/a&gt; 文字通りサクッとしたまとめ。OMP を使って解いているというのがとても気になる&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cnx.org/contents/krkDdys0&amp;#64;12/Iterative-Reweighted-Least-Squares"&gt;Iterative Reweighted Least Squares&lt;/a&gt; 導入から解法まで。しかしなぜ解が求まるのかは不明。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.3-IRLS.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; バッファロー大の講義資料？これも何故解けるのかはちゃんと書いてない。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.maths.lth.se/matematiklth/personal/fredrik/Session3.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; これが一番いいかも。なぜ解けるかもざっくり証明がある。&lt;ul&gt;
&lt;li&gt;そこで出てきた supergradient（優勾配？劣勾配に対応している？）がよくわからん。資料のすぐ下に解説があったけど。 &lt;a class="reference external" href="http://www.its.caltech.edu/~kcborder/Notes/Supergrad.pdf"&gt;Supergradients&lt;/a&gt; に定義はあったけど幾何学的イメージが欲しい。&lt;/li&gt;
&lt;li&gt;Weiszfeld Algorithms という幾何中央値を求めるアルゴリズムは &lt;a class="reference external" href="http://users.cecs.anu.edu.au/~trumpf/pubs/aftab_hartley_trumpf_PAMI2014.pdf"&gt;Generalized Weiszfeld Algorithms for …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;LAD(Least Absolute Deviation) を近似的・逐次的に解く方法としての IRLS について調査。そういえば基本的な原理を抑えていなかった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://retrofocus28.blogspot.com/2015/09/iteratively-reweighted-least-squares.html"&gt;Iteratively Reweighted Least Squares　についてサクッと。&lt;/a&gt; 文字通りサクッとしたまとめ。OMP を使って解いているというのがとても気になる&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cnx.org/contents/krkDdys0&amp;#64;12/Iterative-Reweighted-Least-Squares"&gt;Iterative Reweighted Least Squares&lt;/a&gt; 導入から解法まで。しかしなぜ解が求まるのかは不明。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.3-IRLS.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; バッファロー大の講義資料？これも何故解けるのかはちゃんと書いてない。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.maths.lth.se/matematiklth/personal/fredrik/Session3.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; これが一番いいかも。なぜ解けるかもざっくり証明がある。&lt;ul&gt;
&lt;li&gt;そこで出てきた supergradient（優勾配？劣勾配に対応している？）がよくわからん。資料のすぐ下に解説があったけど。 &lt;a class="reference external" href="http://www.its.caltech.edu/~kcborder/Notes/Supergrad.pdf"&gt;Supergradients&lt;/a&gt; に定義はあったけど幾何学的イメージが欲しい。&lt;/li&gt;
&lt;li&gt;Weiszfeld Algorithms という幾何中央値を求めるアルゴリズムは &lt;a class="reference external" href="http://users.cecs.anu.edu.au/~trumpf/pubs/aftab_hartley_trumpf_PAMI2014.pdf"&gt;Generalized Weiszfeld Algorithms for Lq Optimization&lt;/a&gt; に解説あり。しかしこの論文いいこと言ってる。「Generalized Weiszfeld Algorithms」は圧縮センシングとは異なりスパース表現を求めるわけではない。スパース性は担保されなくても、より L1 ノルムの意味で小さい解を求める。&lt;/li&gt;
&lt;li&gt;なぜ、IRLS と LMS アルゴリズムを結びつける研究がないのか。IRLS の逐次適用によってもフィルタ係数を更新していけそうだけど。試してみるし、類似研究が無いか引き続き調べる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;『スパースモデリング』の 5 章にも記述はある。しかし残差の L1 最小化ではない。&lt;/p&gt;
</content><category term="雑記"></category><category term="LAD"></category><category term="IRLS"></category><category term="L1ノルム"></category></entry><entry><title>LAD(Least Absolute Deviation)</title><link href="/ladleast-absolute-deviation.html" rel="alternate"></link><published>2020-04-17T23:00:00+09:00</published><updated>2020-04-17T23:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-17:/ladleast-absolute-deviation.html</id><summary type="html">&lt;p&gt;LAD(Least Absolute Deviation) を見ている。これは、残差を L1 ノルムにした回帰問題一般のこと。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://core.ac.uk/download/pdf/81785239.pdf"&gt;最尤推定による近似的手法&lt;/a&gt; は軽く読んだ。各傾きと切片を固定して逐次更新していく。更新時は中央値を拾ってくる。うーん中央値だと高速推定が厳しい。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ラプラス分布の最尤推定しようとしてもがく。対数尤度とって見てみても、単純な絶対値和が出て止まるし、反復スケーリング法を参考に、パラメータの増分を加えた時の対数尤度の下限を求めようとしたが上手く行かず。4 時間飛ばす。&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt=" 最尤推定の計算のあがき " src="./images/IMG_3828.jpg" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;最尤推定の計算のあがき&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;あがいて「A maximum likelihood approach to least absolute deviation regression」を引用している文献を漁ったら辞書学習を L1 にしているやつが、やっぱりいた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://winsty.net/papers/onndl.pdf"&gt;Online Robust Non-negative Dictionary Learning for Visual Tracking&lt;/a&gt; パーティクルフィルターを使っておる。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上の文献で使ってる Huber Loss …&lt;/p&gt;</summary><content type="html">&lt;p&gt;LAD(Least Absolute Deviation) を見ている。これは、残差を L1 ノルムにした回帰問題一般のこと。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://core.ac.uk/download/pdf/81785239.pdf"&gt;最尤推定による近似的手法&lt;/a&gt; は軽く読んだ。各傾きと切片を固定して逐次更新していく。更新時は中央値を拾ってくる。うーん中央値だと高速推定が厳しい。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ラプラス分布の最尤推定しようとしてもがく。対数尤度とって見てみても、単純な絶対値和が出て止まるし、反復スケーリング法を参考に、パラメータの増分を加えた時の対数尤度の下限を求めようとしたが上手く行かず。4 時間飛ばす。&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt=" 最尤推定の計算のあがき " src="./images/IMG_3828.jpg" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;最尤推定の計算のあがき&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;あがいて「A maximum likelihood approach to least absolute deviation regression」を引用している文献を漁ったら辞書学習を L1 にしているやつが、やっぱりいた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://winsty.net/papers/onndl.pdf"&gt;Online Robust Non-negative Dictionary Learning for Visual Tracking&lt;/a&gt; パーティクルフィルターを使っておる。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上の文献で使ってる Huber Loss 結構すごくね？この誤差に基づく LMS アルゴリズムねえの？→「Robust Huber adaptive filter」だけど中身を読めず …&lt;/p&gt;
&lt;p&gt;また、 &lt;a class="reference external" href="https://www.ml.uni-saarland.de/Lectures/CVX-SS10/ConvexOptimization-07-07-10.pdf"&gt;Convex Optimization and Modeling&lt;/a&gt; を読んでたら Huber 損失は L1 と L2 の中間的な性質を示すようで、0 に集中しなくなりそうな印象を受けた。&lt;/p&gt;
</content><category term="雑記"></category><category term="LAD"></category><category term="L1ノルム"></category></entry><entry><title>LMSフィルターの挙動観察</title><link href="/lmshuirutanoju-dong-guan-cha.html" rel="alternate"></link><published>2020-04-16T23:20:00+09:00</published><updated>2020-04-16T23:20:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-16:/lmshuirutanoju-dong-guan-cha.html</id><summary type="html">&lt;p&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt; の挙動を追いたい。色々な信号に対して、&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が十分大きいとき、0 に近づくかどうか&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を知りたい。もし 0 に近づくならば有効な過程として解法に使える。
しかしその前に、LMS フィルター自体の挙動を追いたい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;残差はどの様に減る？残差の時系列は？&lt;ul&gt;
&lt;li&gt;ステップサイズにより収束の度合い（残差の分布）が違う ...&lt;/li&gt;
&lt;li&gt;当然、フィルタ次数でも収束の度合い（残差の分布）が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;残差分布はどうなってる？Signed-LMS でラプラス分布に近づいてる？&lt;ul&gt;
&lt;li&gt;これは本当のようで、Signed-LMS の方が裾が細い残差分布が得られている。&lt;/li&gt;
&lt;li&gt;単純な正弦波に対しては LMS のほうが残差が小さくなるが、ボイスやピアノ音源に対しては圧倒的に SignLMS の方が性能が良い（残差のヒストグラムを見ると、裾が狭い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n …&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt; の挙動を追いたい。色々な信号に対して、&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が十分大きいとき、0 に近づくかどうか&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を知りたい。もし 0 に近づくならば有効な過程として解法に使える。
しかしその前に、LMS フィルター自体の挙動を追いたい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;残差はどの様に減る？残差の時系列は？&lt;ul&gt;
&lt;li&gt;ステップサイズにより収束の度合い（残差の分布）が違う ...&lt;/li&gt;
&lt;li&gt;当然、フィルタ次数でも収束の度合い（残差の分布）が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;残差分布はどうなってる？Signed-LMS でラプラス分布に近づいてる？&lt;ul&gt;
&lt;li&gt;これは本当のようで、Signed-LMS の方が裾が細い残差分布が得られている。&lt;/li&gt;
&lt;li&gt;単純な正弦波に対しては LMS のほうが残差が小さくなるが、ボイスやピアノ音源に対しては圧倒的に SignLMS の方が性能が良い（残差のヒストグラムを見ると、裾が狭い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathrm{E}[e(n)x(n-m)]\)&lt;/span&gt; は両方とも 0。&lt;ul&gt;
&lt;li&gt;逐次計算していったら、音源非依存で 0 に近づいていく&lt;/li&gt;
&lt;li&gt;当然だよな … そもそもの過程として入力と雑音は無相関と仮定しているのだから。&lt;ul&gt;
&lt;li&gt;仮定しているのだからは正しくなくて、無相関にするようにフィルタ係数を更新しているが正しい。&lt;/li&gt;
&lt;li&gt;無相関になったときに勾配が 0 で最急勾配法が止まる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;なんか絶対値誤差最小化ってどっかで見たよな … と思っていたら、&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Least_absolute_deviations"&gt;https://en.wikipedia.org/wiki/Least_absolute_deviations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;修士のときに一回戦っていた。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(L_{1}\)&lt;/span&gt; ノルム最小化を近接オペレータの繰り返し適用で解けんじゃね？と思っている&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://yamagensakam.hatenablog.com/entry/2018/02/14/075106"&gt;近接勾配法と proximal operator&lt;/a&gt; を読んだが、パラメータ正則化だけだな&lt;/li&gt;
&lt;li&gt;パラメータ正則化はあるけど、残差をスパースにするのがない。なんで？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="LMS"></category><category term="LMS Algorithm"></category></entry><entry><title>続・古いロスレス音声コーデックの調査</title><link href="/sok-gu-irosuresuyin-sheng-kodetsukunodiao-cha.html" rel="alternate"></link><published>2020-04-10T23:18:00+09:00</published><updated>2020-04-10T23:18:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-10:/sok-gu-irosuresuyin-sheng-kodetsukunodiao-cha.html</id><content type="html">&lt;p&gt;古いロスレス音声コーデックと理論の概要を取りまとめた雑誌の特集があった :&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eie.polyu.edu.hk/~enyhchan/ce_ac_p1.pdf"&gt;Lossless Compression of Digital Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理論としてもその通りだし、雑誌発行時点 (1998) からさしたるブレークスルーが無いように見える。&lt;/p&gt;
&lt;p&gt;AudioPak, OggSquish, Philips, Sonarc, WA という謎のコーデック現る …。いったい何個あるんだ。&lt;/p&gt;
</content><category term="雑記"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category></entry><entry><title>古いロスレス音声コーデックの調査/スパース適応フィルタ</title><link href="/gu-irosuresuyin-sheng-kodetsukunodiao-cha-supasushi-ying-huiruta.html" rel="alternate"></link><published>2020-04-08T16:45:00+09:00</published><updated>2020-04-08T23:45:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-08:/gu-irosuresuyin-sheng-kodetsukunodiao-cha-supasushi-ying-huiruta.html</id><summary type="html">&lt;p&gt;ロスレス音声の歴史を探るために古いロスレス音声コーデックの情報を探っている。以下のサイトが &lt;a class="reference external" href="https://wiki.hydrogenaud.io/index.php?title=Lossless_comparison"&gt;Hydrogenaudio での比較&lt;/a&gt; よりも古い内容を扱っている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html"&gt;Lossless Compression of Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;見つけたロスレス音声コーデックを一覧する。というかほぼ &lt;a class="reference external" href="https://www.rarewares.org/rrw/about.php"&gt;Really Rare Wares&lt;/a&gt; 様へのリンク。&lt;/p&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;古めのロスレス音声コーデック&lt;/h2&gt;
&lt;div class="section" id="rkau-rk-audio"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/rkau.php"&gt;RKAU(RK Audio)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;古い比較において優秀な圧縮率を誇っていた。当時の Monkey's Audio よりも上。サイトを覗いたら exe と dll のみの配布だった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020124045327/http://rksoft.virtualave.net/rkau.html"&gt;RKAU のホームページ（魚拓）&lt;/a&gt; を見ても特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="audiozip"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/audiozip.php"&gt;AudioZip&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;これも圧縮率が比較的優秀。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020207080740/http://www.csp.ntu.edu.sg:8000/MMS/MMCProjects.htm"&gt;AudioZip のホームページ（魚拓）&lt;/a&gt; を見てもこちらも特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="wavarc"&gt;
&lt;h3&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/wavarc/0readme.html"&gt;WavArc&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;こちらも最大圧縮率 (-c5) を選択するとそれなりに優秀な結果を出していた。このページに exe とドキュメントをまとめた zip もあり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavezip"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/wavezip.php"&gt;WaveZip&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;圧縮率よりは速度重視のコーデックのようだ …&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;ロスレス音声の歴史を探るために古いロスレス音声コーデックの情報を探っている。以下のサイトが &lt;a class="reference external" href="https://wiki.hydrogenaud.io/index.php?title=Lossless_comparison"&gt;Hydrogenaudio での比較&lt;/a&gt; よりも古い内容を扱っている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html"&gt;Lossless Compression of Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;見つけたロスレス音声コーデックを一覧する。というかほぼ &lt;a class="reference external" href="https://www.rarewares.org/rrw/about.php"&gt;Really Rare Wares&lt;/a&gt; 様へのリンク。&lt;/p&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;古めのロスレス音声コーデック&lt;/h2&gt;
&lt;div class="section" id="rkau-rk-audio"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/rkau.php"&gt;RKAU(RK Audio)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;古い比較において優秀な圧縮率を誇っていた。当時の Monkey's Audio よりも上。サイトを覗いたら exe と dll のみの配布だった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020124045327/http://rksoft.virtualave.net/rkau.html"&gt;RKAU のホームページ（魚拓）&lt;/a&gt; を見ても特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="audiozip"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/audiozip.php"&gt;AudioZip&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;これも圧縮率が比較的優秀。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020207080740/http://www.csp.ntu.edu.sg:8000/MMS/MMCProjects.htm"&gt;AudioZip のホームページ（魚拓）&lt;/a&gt; を見てもこちらも特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="wavarc"&gt;
&lt;h3&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/wavarc/0readme.html"&gt;WavArc&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;こちらも最大圧縮率 (-c5) を選択するとそれなりに優秀な結果を出していた。このページに exe とドキュメントをまとめた zip もあり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavezip"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/wavezip.php"&gt;WaveZip&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;圧縮率よりは速度重視のコーデックのようだ。MUSICompress というアルゴリズムの実装。 &lt;a class="reference external" href="https://www.rarewares.org/rrw/files/lossless/musi_txt.txt"&gt;WaveZip のデータシート&lt;/a&gt; によると符号化には LZ(Lampel-Ziv) を使用しているようだ。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html#wavezip"&gt;WaveZip の概要&lt;/a&gt; が比較サイトに掲載されていた。どうやら、入力波形を近似波形と誤差波形に分けて符号化するようだ。WaveZip では Hu&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lpac-ltac"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/lpac.php"&gt;LPAC/LTAC&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;LPAC は MPEG4-ALS の前身。LPAC の前身が LTAC。LPAC の平均的な圧縮率は優秀なようだ。 &lt;a class="reference external" href="https://web.archive.org/web/20060213124711/http://www.nue.tu-berlin.de/wer/liebchen/lpac.html"&gt;LPAC（魚拓）&lt;/a&gt; に以前公開していたサイトあり。&lt;/p&gt;
&lt;p&gt;LTAC(Lossless Transform Audio Compression) は名前の通り変換符号化に基づくロスレス音声圧縮コーデック、LPAC(Lossless Predictive Audio Compression) は予測に基づくロスレス音声圧縮コーデック。&lt;/p&gt;
&lt;p&gt;LPAC に ベルリン工科大学、Real Networks、NTT の改良が加わって MPEG4-ALS が出来上がり、それ以降 LPAC の開発は停止されている。この経緯については &lt;a class="reference external" href="https://web.archive.org/web/20060212123059/http://www.nue.tu-berlin.de/forschung/projekte/lossless/mp4als.html"&gt;MPEG4-ALS（魚拓）&lt;/a&gt; に記述あり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="shorten"&gt;
&lt;h3&gt;&lt;a class="reference external" href="https://archive.is/Z8k97"&gt;Shorten（魚拓）&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;おそらくロスレス音声の最古参にして基礎。なんと執筆時点（2020-04-08）でも &lt;tt class="docutils literal"&gt;brew&lt;/tt&gt; でインストールできた（ &lt;a class="reference external" href="https://linux.die.net/man/1/shorten"&gt;Shorten の man ページ&lt;/a&gt; もあるから各 Linux ディストリビューションで使えるものと想像する）。エンコード速度はピカイチ。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9797AA37C32F12179AF0803D8C2B22D2?doi=10.1.1.53.7337&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Shorten の論文&lt;/a&gt; （テクニカルレポート）もある。この論文で、今のロスレス音声につながる重要な事実に幾つか触れている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;音声信号は準定常（短い区間では定常とみなせる）だからブロックに分けてエンコード / デコードすべき。&lt;/li&gt;
&lt;li&gt;音声のモデル化には線形予測 (LPC, Linear Predictive Coding) が使える。&lt;/li&gt;
&lt;li&gt;残差信号はガウス分布よりもラプラス分布に従っていると見える。その符号化にはライス符号を使うのが良い。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この時点で既にラプラス分布を仮定したパラメータ設定を行っているからかなりの慧眼。他のロスレス音声コーデックは Shorten を発展させたものに過ぎないと見える。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h2&gt;所感&lt;/h2&gt;
&lt;p&gt;どうも 2000 年代前半までは各自でロスレス音声コーデックを作り、各自で最強を謳っていたらしい。&lt;/p&gt;
&lt;p&gt;歴史を雑にまとめると、1994 年に Shorten の論文が出てから、それよりも圧縮率の良いもの、圧縮速度（展開速度）が早いものが開発されて混沌に突入し上記のコーデックが現れた。その後、Monkey's Audio, WavPack, FLAC, LPAC（MPEG4-ALS）が生き残り、2000 年以降は La（更新停止）, TAK, TTA, ALAC（更新停止）, WMAL(Windows Media Audio Lossless), 2010 年以降は OptimFROG が出現しているようだ。&lt;/p&gt;
&lt;p&gt;気になるのは比較サイトの &lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html#rice"&gt;Rice Coding, AKA Rice Packing, Elias Gamma codes and other approaches&lt;/a&gt; である。Rice 符号よりも効率の良いとされる Pod 符号の紹介がある。要観察。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id14"&gt;
&lt;h2&gt;スパース適応フィルタ&lt;/h2&gt;
&lt;p&gt;LPC の定式化をスパースにする試みは多くなされている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.cs.tut.fi/~tabus/2013GhidoTabus.pdf"&gt;Sparse Modeling for Lossless Audio Compression&lt;/a&gt; : Ghido さん（OptimFROG の人）の試み&lt;ul&gt;
&lt;li&gt;貪欲法によりスパース解を求めている。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.jstage.jst.go.jp/article/jasj/71/11/71_KJ00010109335/_pdf/-char/ja"&gt;スパース表現に基づく音声音響符号化&lt;/a&gt; : NTT の試み&lt;ul&gt;
&lt;li&gt;最小二乗解を求めるのではなく L1 最小化に置き換えた定式化を行う。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;でも、TTA がやっているような適応フィルタをスパース解に近づける手法はまだロスレス音声に対してやっていないように見える。
スパースな解を目指してフィルタ係数を更新する適応フィルタはスパース適応フィルタ (Sparse Adaptive Filters) というようで、2000 年代以降に研究が進んでいるようだ。&lt;/p&gt;
&lt;p&gt;最も基本的な適応フィルタである LMS(Least Mean Square) フィルタは名前の通り二乗誤差最小化に立脚している。
スパース適応フィルタの主な用途はエコーキャンセル、ブラインド話者分離、複数話者特定ではあるが、やはり変換後の分布がスパースになるというのは大きい。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://signal.ee.bilkent.edu.tr/defevent/papers/cr1256.pdf"&gt;スパース適応フィルタの最近のサーベイ論文&lt;/a&gt; を流し読みした。スパース適応フィルタは、変数更新のときに 1 部の変数だけ更新する方法と、スパース最適化に従って更新するやり方の 2 つがあった。PNLMS(Proportionate NLMS), IPNLMS(Improved PNLMS) が後者の定式化で興味あり。引き続き見ていく。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/pdf/1012.5066.pdf"&gt;Regularized Least-Mean-Square Algorithms&lt;/a&gt; には正則化を入れた LMS アルゴリズムの解説あり。LASSO にモチベーションを受けた最適化アルゴリズムが &lt;a class="reference external" href="https://wiki.eecs.umich.edu/global/data/hero/images/7/7b/Yilun-icassp2-09.pdf"&gt;ZA-LMS&lt;/a&gt; や &lt;a class="reference external" href="http://azadproject.ir/wp-content/uploads/2017/01/2018-Online-Sparse-System-Identification-and-Signal-Reconstruction-Using-Projections-.pdf"&gt;APWL1&lt;/a&gt; として提案されている。&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category></entry><entry><title>ブログ導入</title><link href="/burogudao-ru.html" rel="alternate"></link><published>2020-04-02T18:00:00+09:00</published><updated>2020-04-02T21:34:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-02:/burogudao-ru.html</id><summary type="html">&lt;p&gt;GitHub io + Pelican を使ってみた。しばらくこちらで日報を書きたい。
GitHub io + Pelican は以下の記事を参考にしている。まだあんまり分かってない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/yusukew62/items/7b01d2370cdbe170b28d"&gt;Python 製静的 HTML ジェネレータの Pelican で GitHub Pages を公開する方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/ririli/items/0e06b21cb709beae4514"&gt;GitHub Pages で静的サイトを簡単に作る&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/saira/items/71faa202efb4320cb41d"&gt;Python 製 Pelican を使ってサクッとブログを公開する&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pelicanthemes.com"&gt;Pelican のテーマ集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/getpelican/pelican-themes/issues/460#issuecomment-346652986"&gt;テーマ導入時にハマったので参考にした issue comment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今日は（というか 3 月末）から SLA の高速化作業とまとめをしていた。&lt;/p&gt;
&lt;p&gt;格子型フィルタ演算はどうしても 1 乗算型にできず。次数演算を 4 次数にして SSE 演算するのがやっと。
SSE 化するときに、スカラー演算とベクトル演算が混じったときに処理負荷が大きく上がってハマった。
&lt;a class="reference external" href="https://stackoverflow.com/questions/10313397/where-does-the-sse-instructions-outperform-normal-instructions"&gt;StackOverFlow …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;GitHub io + Pelican を使ってみた。しばらくこちらで日報を書きたい。
GitHub io + Pelican は以下の記事を参考にしている。まだあんまり分かってない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/yusukew62/items/7b01d2370cdbe170b28d"&gt;Python 製静的 HTML ジェネレータの Pelican で GitHub Pages を公開する方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/ririli/items/0e06b21cb709beae4514"&gt;GitHub Pages で静的サイトを簡単に作る&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/saira/items/71faa202efb4320cb41d"&gt;Python 製 Pelican を使ってサクッとブログを公開する&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pelicanthemes.com"&gt;Pelican のテーマ集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/getpelican/pelican-themes/issues/460#issuecomment-346652986"&gt;テーマ導入時にハマったので参考にした issue comment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今日は（というか 3 月末）から SLA の高速化作業とまとめをしていた。&lt;/p&gt;
&lt;p&gt;格子型フィルタ演算はどうしても 1 乗算型にできず。次数演算を 4 次数にして SSE 演算するのがやっと。
SSE 化するときに、スカラー演算とベクトル演算が混じったときに処理負荷が大きく上がってハマった。
&lt;a class="reference external" href="https://stackoverflow.com/questions/10313397/where-does-the-sse-instructions-outperform-normal-instructions"&gt;StackOverFlow の記事&lt;/a&gt; では &lt;cite&gt;_mm_set_epi32&lt;/cite&gt; のコストが高い旨記述あり。 &lt;cite&gt;_mm_loadu_si128&lt;/cite&gt; の使用に置き換えた。
&lt;a class="reference external" href="https://stackoverflow.com/questions/24446516/performance-worsens-when-using-sse-simple-addition-of-integer-arrays"&gt;他の記事&lt;/a&gt; で言及があってようやく分かった。全てをベクトル演算化したところ、処理負荷は 4/5 倍になった。あんまり早くなっていない。遺憾。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://herumi.in.coocan.jp/prog/gcc-and-vc.html"&gt;gcc と VC&lt;/a&gt; には gcc と Visual Studio の挙動の差異について色々と書いてあった。&lt;/p&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="SSE"></category><category term="test"></category><category term="pelican"></category><category term="githubio"></category></entry></feed>