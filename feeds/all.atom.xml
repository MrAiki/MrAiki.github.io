<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aiki's Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-04-23T15:30:00+09:00</updated><entry><title>パーセプトロン昔話</title><link href="/paseputoronxi-hua.html" rel="alternate"></link><published>2020-04-23T15:30:00+09:00</published><updated>2020-04-23T15:30:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/paseputoronxi-hua.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;ニューラルネットワーク（Neural Network, 以下NN）&lt;/strong&gt;は機械学習の歴史と共に歩んできたと言っても過言ではない.
戦後間もないウィーナーの時代&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt;からモデルが構築され始め,
幾つかの冬の時代（挫折）を超えて,
そして現在流行りのディープラーニング（深層学習）は多層構造のNNによって構成されている.
ここでは, NNの歴史に少しずつ触れながら,
多層パーセプトロンの学習則（逆誤差伝搬）までを解説していく.
本稿は主に&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;を参照している.&lt;/p&gt;
&lt;div class="math"&gt;$$
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\tvec}[1]{\bvec{#1}^{\mathsf{T}}}
\newcommand{\ve}[1]{\bvec{#1}}
\newcommand{\inpro}[2]{{\left\langle #1 , #2 \right\rangle}}
\newcommand …&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;ニューラルネットワーク（Neural Network, 以下NN）&lt;/strong&gt;は機械学習の歴史と共に歩んできたと言っても過言ではない.
戦後間もないウィーナーの時代&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt;からモデルが構築され始め,
幾つかの冬の時代（挫折）を超えて,
そして現在流行りのディープラーニング（深層学習）は多層構造のNNによって構成されている.
ここでは, NNの歴史に少しずつ触れながら,
多層パーセプトロンの学習則（逆誤差伝搬）までを解説していく.
本稿は主に&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;を参照している.&lt;/p&gt;
&lt;div class="math"&gt;$$
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\tvec}[1]{\bvec{#1}^{\mathsf{T}}}
\newcommand{\ve}[1]{\bvec{#1}}
\newcommand{\inpro}[2]{{\left\langle #1 , #2 \right\rangle}}
\newcommand{\norm}[1]{{\left\| #1 \right\|}}
\newcommand{\dint}[2]{\int\!\!\!\int_{#1} #2 }
\newcommand{\tint}[2]{\int\!\!\!\int\!\!\!\int_{#1} #2 }
\newcommand{\dif}[3]{\frac{d^{#1}#2}{d #3^{#1}}}
\newcommand{\pard}[3]{\frac{\partial^{#1}#2}{\partial #3^{#1}}}
\newcommand{\difrac}[2]{{\frac{d #1}{d #2}}}
\newcommand{\parfrac}[2]{{\frac{\partial #1}{\partial #2}}}
\newcommand{\tparfrac}[2]{{\tfrac{\partial #1}{\partial #2}}}
\newcommand{\Div}{{\rm div}}
\newcommand{\Rot}{{\rm rot}}
\newcommand{\Curl}{{\rm curl}}
\newcommand{\innprod}[2]{\langle #1, #2 \rangle}
\newcommand{\n}{\ \\}
\newcommand{\cm}{{\  , \ }}
\def\diag{\mathop{\rm diag}\nolimits}
\def\sign{\mathop{\rm sign}\nolimits}
$$&lt;/div&gt;
&lt;h1&gt;脳機能のモデル化&lt;/h1&gt;
&lt;p&gt;NNは脳の神経回路網を数学的なモデルで表現したものである. その為,
理論の出発点は生理学となる. その知識によると,
神経回路の構成要素であるニューロン（神経細胞）はシナプスを介して他の細胞と結合しており,
電気信号によって情報を伝達しあっている.
1つのニューロンは外部からの電気的刺激を受けると膜電位（細胞内外の電位差）を上昇させていき,
刺激の総量がある一定値（閾値）を超えると瞬間的に電位パルス（インパルス,
スパイク）を放出する.
放出したパルスは他のニューロンに影響を与えることができる.
この相互作用を大域的に見ることで脳活動が実現されると考えられている.&lt;/p&gt;
&lt;p&gt;上記の生理学の知見をを事実として受け入れてみると,
次の単純なモデル化が考えられる. 1つのニューロンにおいて,
他のニューロンからの刺激（入力）の総量を&lt;span class="math"&gt;\(u\)&lt;/span&gt;と表し,
その入力を受けて出力を決める&lt;strong&gt;活性化関数（activation
function）&lt;/strong&gt;&lt;span class="math"&gt;\(f\)&lt;/span&gt;をおき, 出力を&lt;span class="math"&gt;\(y = f(u)\)&lt;/span&gt;と表す.
また総入力&lt;span class="math"&gt;\(u\)&lt;/span&gt;は他のニューロンからの刺激の重ねあわせによって決まるので,
単純に入力に重みを掛け合わせ和をとった総量と考えられる. 即ち,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  u = \sum_{i=1}^{n} w_{i}x_{i} + b = \ve{w}^{\mathsf{T}} \ve{x} + b\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
と表せるものとする. ここで,
&lt;span class="math"&gt;\(\ve{x} = [x_{1},\dots,x_{n}]^{\mathsf{T}}\)&lt;/span&gt;は入力（他のニューロンからの出力）ベクトル,
&lt;span class="math"&gt;\(\ve{w} = [w_{1},\dots,w_{n}]^{\mathsf{T}}\)&lt;/span&gt;は入力の重み（係数）ベクトルであり,
生理学的にはシナプスの結合の強さ（影響の度合い）を表している.
そして&lt;span class="math"&gt;\(-b\)&lt;/span&gt;はニューロン発火の条件を与える&lt;strong&gt;しきい値（bias,
threshold）&lt;/strong&gt;を表している.
以上によってモデル化されるニューロンの機能の単位は図にまとめられ,
&lt;strong&gt;ユニット（unit）&lt;/strong&gt;と呼ばれる&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="NNを構成する単位：ユニット" src="./images/unit.eps"&gt;&lt;/p&gt;
&lt;h2&gt;活性化関数の例&lt;/h2&gt;
&lt;p&gt;実際に良く使われる活性化関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;としては, 次が挙げられる:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;単位ステップ関数（ハードリミタ）&lt;span class="math"&gt;\(U(u)\)&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0\)&lt;/span&gt;か&lt;span class="math"&gt;\(1\)&lt;/span&gt;かを出力し, 決定的な識別を行う: &lt;div class="math"&gt;$$\begin{aligned}
      f(u) = 
      \left\{ \begin{array}{ll}
        0 &amp;amp; u &amp;lt; 0 \\
        1 &amp;amp; u &amp;gt; 0
      \end{array} \right.
    \end{aligned}$$&lt;/div&gt;
&lt;span class="math"&gt;\(u=0\)&lt;/span&gt;で不連続となり,
&lt;span class="math"&gt;\(U(u)\)&lt;/span&gt;等で参照される事がある.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;符号関数&lt;span class="math"&gt;\(\sign(u)\)&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;二値のみを出力するのは単位ステップ関数と同じだが,
&lt;span class="math"&gt;\(-1\)&lt;/span&gt;か&lt;span class="math"&gt;\(1\)&lt;/span&gt;を出力する: &lt;div class="math"&gt;$$\begin{aligned}
      f(u) = 
      \left\{ \begin{array}{ll}
        -1 &amp;amp; u &amp;lt; 0 \\
        1  &amp;amp; u &amp;gt; 0
      \end{array} \right.
    \end{aligned}$$&lt;/div&gt;
単位ステップ関数とは表現を変えたい文脈で用いられる.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;線形関数&lt;span class="math"&gt;\(u\)&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;入力をそのまま出力する線形関数も活性化関数に用いられる事がある:
&lt;div class="math"&gt;$$\begin{aligned}
      f(u) = u
    \end{aligned}$$&lt;/div&gt; 入力が有界でない場合出力が発散する場合がある.
線形関数は微分可能なので学習規則の導出の際に役立つ.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;シグモイド（ロジスティック）関数&lt;span class="math"&gt;\(\varphi(u)\)&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
      f(u) = \frac{1}{1 + \exp(-u)}
    \end{aligned}$$&lt;/div&gt; 明らかに&lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;で単調増加する関数である.
グラフが単位ステップ関数に類似し, 関数の形が単純であり,
しかも微分可能である事から非常に重要な関数である. 実際,
&lt;span class="math"&gt;\(u\)&lt;/span&gt;で微分してみると, &lt;div class="math"&gt;$$\begin{aligned}
       \difrac{ }{u} f(u) &amp;amp;= - \frac{-\exp(-u)}{\{1+\exp(-u)\}^{2}} = \frac{1}{1 + \exp(-u)}\left( 1 - \frac{1}{1 + \exp(-u)} \right)  \\
       &amp;amp;= f(u) (1-f(u))
     \end{aligned}$$&lt;/div&gt;
となって微分も簡易に計算できることも高評価の理由である.
シグモイド関数は文献によっては&lt;span class="math"&gt;\(\varphi(u)\)&lt;/span&gt;で参照される事がある.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;（タンジェントハイパボリック）関数&lt;span class="math"&gt;\(\tanh(u)\)&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
      f(u) = \tanh(u) = \frac{\exp(u) - \exp(-u)}{\exp(u) + \exp(-u)}
    \end{aligned}$$&lt;/div&gt; これは&lt;span class="math"&gt;\((-1,1)\)&lt;/span&gt;で単調増加する関数であり,
値域を&lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;とする為に &lt;div class="math"&gt;$$\begin{aligned}
      f(u) = \frac{\tanh(u) + 1}{2}
    \end{aligned}$$&lt;/div&gt; とする場合がある.
&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;の微分値も簡潔に表現できる: &lt;div class="math"&gt;$$\begin{aligned}
      \difrac{ }{u} f(u) &amp;amp;= \frac{\{\exp(u)+\exp(-u)\}^{2} - \{\exp(u)-\exp(-u)\}^{2}}{\{\exp(u)+\exp(-u)\}^{2}} \\
      &amp;amp;= 1 - \tanh^{2}(u) = 1 - f^{2}(u)
    \end{aligned}$$&lt;/div&gt; また,
上述のシグモイド関数は&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;を用いて表すこともできる:
&lt;div class="math"&gt;$$\begin{aligned}
      \frac{1}{1 + \exp(-u)} &amp;amp;= \frac{1}{2} \frac{2 \exp(u/2)}{\exp(u/2) + \exp(-u/2)} \\
      &amp;amp;= \frac{1}{2} \left( \frac{\exp(u/2) + \exp(-u/2)}{\exp(u/2) + \exp(-u/2)} + \frac{\exp(u/2) - \exp(-u/2)}{\exp(u/2) + \exp(-u/2)} \right) \\
      &amp;amp;= \frac{1}{2} (1 + \tanh(u/2))
    \end{aligned}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;図に関数のグラフを示す.&lt;/p&gt;
&lt;p&gt;&lt;img alt="よく使われる活性化関数のグラフ" src="./images/act_funcs.eps"&gt;&lt;/p&gt;
&lt;h2&gt;形式ニューロン&lt;/h2&gt;
&lt;p&gt;最初にニューロンをモデル化して研究を行ったのはMcCulloch-Pitts（ウォーレン・マカロック-ウォルター・ピッツ）であり,
彼らは1943年に&lt;strong&gt;形式ニューロン（formal neuron）&lt;/strong&gt;を提案した.
形式ニューロンでは活性化関数は単にステップ関数&lt;span class="math"&gt;\(U(u)\)&lt;/span&gt;となる:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  U(u) =
  \left\{ \begin{array}{ll}
    1 &amp;amp; u &amp;gt; 0 \\
    0 &amp;amp; u &amp;lt; 0
  \end{array} \right.\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
これによって入力および出力は&lt;span class="math"&gt;\(0\)&lt;/span&gt;か&lt;span class="math"&gt;\(1\)&lt;/span&gt;（all-or-none）となる.
形式ニューロンでは, 重みと閾値の組み合わせによって論理素子を実現できる:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;NOT&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U(-x_{1} + 0.5)\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;AND&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U(x_{1} + x_{2} - 1.5)\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;OR&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U(x_{1} + x_{2} - 0.5)\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;NAND&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U(-x_{1} -x_{2} + 1.5)\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;XOR&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U(x_{1} + x_{2} - 2U(x_{1} + x_{2} - 1.5) - 0.5)\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;実際に入力値に値を代入して真理値表U 作ると,
ユニットが正しく動作する事を確かめられる.
この素子の組み合わせによって任意の（フリップフロップを含めた）論理回路が実現できるのはもちろんのこと,
形式ニューロンはチューリングマシンと同等の計算能力（チューリング完全）を持つ事が示されている.
形式ニューロンはニューロンの最初のモデルとしてNNの大本の基礎となったが,
現在のNNにあるような学習能力を持ちあわせてはいない. しかし,
重みや閾値を変更することでユニットの動作が変わるという観察から,
それらを能動的に変更することで学習が実現されうるという示唆は既に生まれていたものと考えられる.&lt;/p&gt;
&lt;h1&gt;単純パーセプトロンと単層パーセプトロン（パーセプトロン）&lt;/h1&gt;
&lt;p&gt;1957年にRosenblatt（ローゼンブラッド）は形式ニューロンを入力層（Sensory
Layer, S層）, 中間層（Associative Layer, A層）, 出力層（Response Layer,
R層）の3つに分けて階層的に結合し,
図&lt;a href="#fig:simple_perceptron"&gt;1&lt;/a&gt;の構造を持つ&lt;strong&gt;単純パーセプトロン（simple
perceptron）&lt;/strong&gt;を提案した. ここで, S層とA層の間の重みはランダムに固定し,
A層とR層の間の重みは&lt;strong&gt;学習&lt;/strong&gt;によって決めるようになっている.&lt;/p&gt;
&lt;p&gt;&lt;img alt="単純パーセプトロン" id="fig:simple_perceptron" src="./images/simple_perceptron.eps"&gt;&lt;/p&gt;
&lt;p&gt;単純パーセプトロンの学習は,
微積分といった解析的な知見ではなく&lt;strong&gt;ヘブ則（Hebbian
rule）&lt;/strong&gt;と呼ばれる生理学の法則を用いている.
即ちそれは&lt;strong&gt;「同時に発火したニューロン間のシナプス結合は強められる」&lt;/strong&gt;という法則であり,
多くの神経学者及び心理学者が受け入れている事実である. 後に述べるが,
ヘブ則による学習は解が存在すれば有限回数の学習で正しい解に収束することが示されており,
次節に述べるデルタ則（これは数値解析的に学習する）と併せて有用な学習法と言える.
単純パーセプトロンはNNの学習可能性を初めて示し,
史上初のNN研究ブームを引き起こすきっかけとなった.&lt;/p&gt;
&lt;p&gt;ところで, 単純パーセプトロンは3層の階層構造をなしているが,
重みの学習の際に本質的に関与するのは中間層と出力層の間だけである.
この学習する部分のみを抜き出すと,
図&lt;a href="#fig:single_layer_perceptron"&gt;2&lt;/a&gt;の様に,
学習するニューロンの単純な入出力関係が得られる.
これを&lt;strong&gt;単層パーセプトロン（single-layer
perceptron）&lt;/strong&gt;あるいは単に&lt;strong&gt;パーセプトロン（perceptron）&lt;/strong&gt;と呼ぶ.
そして単純パーセプトロンの学習の際には,
S層とA層間の重みをランダムに決定した後は単層パーセプトロンの学習だけを考えば良い事になる.&lt;/p&gt;
&lt;p&gt;&lt;img alt="単層パーセプトロン（パーセプトロン）" id="fig:single_layer_perceptron" src="./images/single_layer_perceptron.eps"&gt;&lt;/p&gt;
&lt;h2&gt;単層パーセプトロンの学習則 - ヘブ則とデルタ則&lt;/h2&gt;
&lt;p&gt;単層パーセプトロンの学習にはサンプルが必要となるため,
まずはサンプルの表記から行う. 学習の中でも特に教師あり学習（supervised
learning）はサンプルのデータにラベルが付いている.
ラベルは一般的にはなんでも良いが,
基本的にはデータがある性質を満たす場合（正例）はラベルを&lt;span class="math"&gt;\(1\)&lt;/span&gt;に,
満たさない場合（負例）はラベルを&lt;span class="math"&gt;\(-1\)&lt;/span&gt;とする&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.
そして&lt;span class="math"&gt;\(N\)&lt;/span&gt;個のデータからなるサンプルの集合&lt;span class="math"&gt;\(Z\)&lt;/span&gt;は,
データ&lt;span class="math"&gt;\(\ve{x}\)&lt;/span&gt;とその（教師）ラベル&lt;span class="math"&gt;\(t\)&lt;/span&gt;の組の集合で表される:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  Z = \{ (\ve{x}_{1}, t_{1}), (\ve{x}_{2}, t_{2}), \dots, (\ve{x}_{N}, t_{N}) \}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;以下,
出力層が1つのユニットだけからなる単層パーセプトロン（図式的には図と等価）の学習を考える.
ユニットが複数存在する場合でも出力層の内部でユニットは互いに独立に動作する（&lt;span class="math"&gt;\(\because\)&lt;/span&gt;結合が無いため）ので拡張は容易である.
また, 表記を簡単にするため, ユニットへの入力&lt;span class="math"&gt;\(u\)&lt;/span&gt;はしきい値&lt;span class="math"&gt;\(b\)&lt;/span&gt;を省き,
重みと入力の内積&lt;span class="math"&gt;\(\ve{w}^{\mathsf{T}}\ve{x}\)&lt;/span&gt;のみで表現する:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  u &amp;amp;= \sum_{i=1}^{n} w_{i} x_{i} + b = \sum_{i=1}^{n+1} w_{i} x_{i} \quad (w_{n+1} = b,\ x_{n+1} = 1) \\
  &amp;amp;\equiv \ve{w}^{\mathsf{T}}\ve{x}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
これは常に&lt;span class="math"&gt;\(1\)&lt;/span&gt;を入力するユニットを仮定し,
その結合重みを&lt;span class="math"&gt;\(b\)&lt;/span&gt;とすることで説明できる.&lt;/p&gt;
&lt;h3&gt;ヘブ則&lt;/h3&gt;
&lt;p&gt;前節で述べたとおり,
ヘブ則は「同時に発火したニューロン間のシナプス結合は強められる」というものであった.
これはラベルを&lt;span class="math"&gt;\(t_{l} \in \{ 1, 0 \}\ (l=1,\dots,N)\)&lt;/span&gt;（正例を1, 負例を0）,
サンプルデータ&lt;span class="math"&gt;\(\ve{x}_{l}\)&lt;/span&gt;を入力した時の出力を&lt;span class="math"&gt;\(y_{l} = U(u_{l}) = U\left( \ve{w}^{\mathsf{T}} \ve{x}_{l}\right)\)&lt;/span&gt;とすれば,
重み（シナプス結合）の更新量&lt;span class="math"&gt;\(\Delta w_{i}\)&lt;/span&gt;は次の様に表せる:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta w_{i} &amp;amp;= 
  \left\{ \begin{array}{ll}
    \eta (\ve{x}_{l})_{i} &amp;amp; \text{if}\ t_{l} = y_{l} = 1 \\
    0          &amp;amp; \text{otherwise}
  \end{array} \right. \\
  &amp;amp;= \eta t_{l}y_{l}(\ve{x}_{l})_{i}\end{aligned}$$&lt;/div&gt;
&lt;p&gt; ここで,
&lt;span class="math"&gt;\((\ve{x}_{l})_{i}\)&lt;/span&gt;はベクトル&lt;span class="math"&gt;\(\ve{x}_{l}\)&lt;/span&gt;の第&lt;span class="math"&gt;\(i\)&lt;/span&gt;要素,
&lt;span class="math"&gt;\(\eta &amp;gt; 0\)&lt;/span&gt;は学習の早さを決める係数であり, &lt;strong&gt;学習率（learning
rate）&lt;/strong&gt;と呼ばれる.
学習の際には&lt;span class="math"&gt;\(\ve{w} = \ve{0}\)&lt;/span&gt;で初期化してサンプルを順次入力し,
上の更新量に沿って重みを更新していけば良い.
第&lt;span class="math"&gt;\(s\)&lt;/span&gt;ステップの時の重みベクトルを&lt;span class="math"&gt;\(\ve{w}^{(s)}\)&lt;/span&gt;と表すと, 更新規則は,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \ve{w}^{(s+1)} = \ve{w}^{(s)} + \Delta \ve{w} = \ve{w}^{(s)} + \eta t_{l} y_{l} \ve{x}_{l}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
と表せる. ここで,
&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;更新量を並べたベクトル&lt;span class="math"&gt;\(\Delta \ve{w} = [\Delta w_{1},\dots,\Delta w_{n}]^{\mathsf{T}}\)&lt;/span&gt;である.&lt;/p&gt;
&lt;p&gt;素朴なヘブ則の実装では,
上の&lt;span class="math"&gt;\(\Delta w_{i}\)&lt;/span&gt;を観察すれば即座に分かるように,
重みが際限なく大きくなって発散してしまって学習が停止しない場合がある.
従って, 重みの発散を防ぐために重みは抑制する方向に更新するようにとる.
即ち, ラベルを&lt;span class="math"&gt;\(\{ 1, -1 \}\)&lt;/span&gt;, 活性化関数を符号関数&lt;span class="math"&gt;\(\sign\)&lt;/span&gt;とし,
出力&lt;span class="math"&gt;\(y_{l} = \sign(u_{l})\)&lt;/span&gt;とラベル&lt;span class="math"&gt;\(t_{l}\)&lt;/span&gt;が異なる（サンプルを誤識別した）場合にのみ重みを更新する:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta w_{i} &amp;amp;= 
  \left\{ \begin{array}{ll}
    -\eta (\ve{x}_{l})_{i} = \eta t_{l}y_{l}(\ve{x}_{l})_{i} &amp;amp; \text{if}\ t_{l} \neq y_{l} \\
    0           &amp;amp; \text{otherwise}
  \end{array} \right.\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
この更新規則もヘブ則と呼ばれる事がある.&lt;/p&gt;
&lt;p&gt;ヘブ則の重要な性質に,
最適な重みが存在するならば有限ステップで学習が停止（サンプルの誤識別がなくなる）する事が示されている.
ここでは, &lt;sup id="fnref2:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;に従ってその証明を行う. まず,
存在が仮定された最適な重みを&lt;span class="math"&gt;\(\ve{w}^{\ast}\)&lt;/span&gt;と表し,
&lt;span class="math"&gt;\(||\ve{w}^{\ast}||^{2} = \ve{w}^{\ast\mathsf{T}}\ve{w}^{\ast} = \sum_{i=1}^{n} w_{i}^{\ast 2} = 1\)&lt;/span&gt;となる様に正規化しておく&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;.
ここで, &lt;span class="math"&gt;\(||\ve{v}||\)&lt;/span&gt;はベクトル&lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt;の2乗ノルムである. また,
&lt;span class="math"&gt;\(\gamma = \displaystyle \min_{l} y_{l} u_{l} = \min_{l} y_{l} \ve{w}^{\ast\mathsf{T}} \ve{x}_{l}\)&lt;/span&gt;なる定数&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;をおき,
&lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;とする. この時, 更新式により, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  ||\ve{w}^{(s+1)}||^{2} &amp;amp;= \ve{w}^{(s+1)\mathsf{T}} \ve{w}^{(s+1)} \\
  &amp;amp;= (\ve{w}^{(s)} + \Delta \ve{w})^{\mathsf{T}}(\ve{w}^{(s)} + \Delta \ve{w}) \\
  &amp;amp;= (\ve{w}^{(s)} + \eta t_{l} y_{l} \ve{x}_{l})^{\mathsf{T}}(\ve{w}^{(s)} + \eta t_{l} y_{l} \ve{x}_{l}) \\
  &amp;amp;= \ve{w}^{(s)\mathsf{T}} \ve{w}^{(s)} + 2\eta t_{l} y_{l} \ve{w}^{(s)\mathsf{T}} \ve{x}_{l} + \eta^{2} \ve{x}_{l}^{\mathsf{T}}\ve{x}_{l} \quad (\because t_{l}^{2} = y_{l}^{2} = 1) \\
  &amp;amp;\leq ||\ve{w}^{(s)}||^{2} + \eta^{2} ||\ve{x}_{l}||^{2} \quad (\because \eta t_{l} y_{l} (\ve{x}_{l})_{i} = \Delta w_{i} \leq 0)\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
が任意の&lt;span class="math"&gt;\(l \in \{ 1,\dots,N \}\)&lt;/span&gt;で成り立つ.
この関係式をステップ毎に繰り返し適用すれば, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  ||\ve{w}^{(s)}||^{2} &amp;amp;\leq ||\ve{w}^{(s-1)}||^{2} + \eta^{2}||\ve{x}_{l^{(s-1)}}||^{2} \\
  &amp;amp;\leq ||\ve{w}^{(s-2)}||^{2} + \eta^{2}(||\ve{x}_{l^{(s-1)}}||^{2} + ||\ve{x}_{l^{(s-2)}}||^{2}) \\
  &amp;amp;\dots \\
  &amp;amp;\leq ||\ve{w}^{(0)}||^{2} + \eta^{2} \sum_{k=0}^{s-1} ||\ve{x}_{l^{(k)}}||^{2} \\
  &amp;amp;\leq s\eta^{2} \max_{l} ||\ve{x}_{l}||^{2} \quad (\because \ve{w}^{(0)} = \ve{0})\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
を得る. ここで,
&lt;span class="math"&gt;\(l^{(s)}\)&lt;/span&gt;はステップ&lt;span class="math"&gt;\(s\)&lt;/span&gt;の更新の時に選ばれたサンプルの番号（インデックス）を表している.
また,
全ての&lt;span class="math"&gt;\(\ve{x}_{l}\ (l=1,\dots,N)\)&lt;/span&gt;は現実的に有界（いずれの要素も&lt;span class="math"&gt;\((-\infty, \infty)\)&lt;/span&gt;にある）と考えられるので,
全てのデータを包む事ができる球（超球）の最小の半径を&lt;span class="math"&gt;\(R\)&lt;/span&gt;とおけば,
&lt;span class="math"&gt;\(\displaystyle \max_{l} ||\ve{x}_{l}||^{2} \leq R^{2}\)&lt;/span&gt;が成り立つので,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  ||\ve{w}^{(s)}||^{2} \leq s\eta^{2} \max_{l} ||\ve{x}_{l}||^{2} \leq s \eta^{2} R^{2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となる. 一方, &lt;span class="math"&gt;\(\ve{w}^{\ast}\)&lt;/span&gt;と&lt;span class="math"&gt;\(\ve{w}^{(s+1)}\)&lt;/span&gt;の内積をとると,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s+1)} &amp;amp;= \ve{w}^{\ast \mathsf{T}} ( \ve{w}^{(s)} + \eta t_{l} y_{l} \ve{x}_{l}) \\
  &amp;amp;= \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s)} + \eta t_{l} y_{l} \ve{w}^{\ast \mathsf{T}} \ve{x}_{l} \\
  &amp;amp;\geq \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s)} + \eta \gamma\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
が成立し, この関係式もステップ毎に繰り返し適用すると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s)} &amp;amp;\geq \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s-1)} + \eta \gamma \\
  &amp;amp;\geq \ve{w}^{\ast \mathsf{T}} \ve{w}^{(s-2)} + 2\eta \gamma \\
  &amp;amp;\dots \\
  &amp;amp;\geq \ve{w}^{\ast \mathsf{T}} \ve{w}^{(0)} + s\eta \gamma = s\eta \gamma\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
を得て, この式の両辺を二乗すると次の結果を得る: &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  s^{2} \eta^{2} \gamma^{2} &amp;amp;\leq (\ve{w}^{\ast \mathsf{T}}\ve{w}^{(s)})^{2} \\
  &amp;amp;\leq (\ve{w}^{\ast \mathsf{T}} \ve{w}^{\ast}) (\ve{w}^{(s) \mathsf{T}} \ve{w}^{(s)}) \quad (\because シュワルツの不等式) \\
  &amp;amp;= ||\ve{w}^{\ast}||^{2} ||\ve{w}^{(s)}||^{2} = ||\ve{w}^{(s)}||^{2} \\
  &amp;amp;\leq s\eta^{2}R^{2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt; ここで,
不等式中央の内積&lt;span class="math"&gt;\(\ve{w}^{\ast \mathsf{T}}\ve{w}^{(s)}\)&lt;/span&gt;は最適解&lt;span class="math"&gt;\(\ve{w}^{\ast}\)&lt;/span&gt;と現在の重み&lt;span class="math"&gt;\(\ve{w}^{(s)}\)&lt;/span&gt;との類似度とも捉えられる&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;ので,
この不等式によりステップ数&lt;span class="math"&gt;\(s\)&lt;/span&gt;増加の度に類似度の下限&lt;span class="math"&gt;\(s^{2}\eta^{2}\gamma^{2}\)&lt;/span&gt;が上限&lt;span class="math"&gt;\(s\eta^{2}R^{2}\)&lt;/span&gt;よりも早く増加する事が観察できる.
即ち類似度は単調増加し, 重みは最適解に近づいて行くことが分かる.
またステップ数&lt;span class="math"&gt;\(s\)&lt;/span&gt;について解くと&lt;span class="math"&gt;\(\displaystyle s \leq \frac{R^{2}}{\gamma^{2}}\)&lt;/span&gt;が成立し,
&lt;span class="math"&gt;\(\gamma, R\)&lt;/span&gt;は有限のために&lt;span class="math"&gt;\(s\)&lt;/span&gt;もまた有限となる. これらの結果により,
有限ステップで&lt;span class="math"&gt;\(\ve{w}^{\ast}\)&lt;/span&gt;が得られ, 学習が停止することが示された.&lt;/p&gt;
&lt;h3&gt;デルタ則&lt;/h3&gt;
&lt;p&gt;デルタ則（デルタルール）は現在の重み&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;でのサンプルによる出力とラベルの誤差（経験誤差）&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;を定義し,
&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;を極小にする様に重みを更新していく学習則である.
誤差&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\displaystyle\parfrac{E(\ve{w})}{\ve{w}}\)&lt;/span&gt;は勾配,
即ち最も&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;の変化する方向（最急勾配）を表すので,
重みの更新量&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;は学習率&lt;span class="math"&gt;\(\eta &amp;gt; 0\)&lt;/span&gt;を用いて &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta \ve{w} = - \eta \parfrac{E(\ve{w})}{\ve{w}}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
とすれば,
更新の度に誤差を最小にする様に重み&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;を更新することができる.
学習の収束は,
&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;の大きさ（&lt;span class="math"&gt;\(||\Delta \ve{w}||^{2}\)&lt;/span&gt;等）が十分に小さくなった時とすれば良く,
そのときは極小解&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;が得られている. この手法は&lt;strong&gt;最急勾配法（steepest
gradient method）&lt;/strong&gt;と呼ばれる基本的な数値最適化の手法の一種である.
ここでは, ユニットの活性化関数を単位ステップ関数&lt;span class="math"&gt;\(U(u)\)&lt;/span&gt;,
ラベルを&lt;span class="math"&gt;\(\{ 1, 0 \}\)&lt;/span&gt;として考える.&lt;/p&gt;
&lt;p&gt;さて, 誤差は様々なものが考えられるが, 単純に二乗誤差 &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  E(\ve{w}) = \frac{1}{2} \sum_{l=1}^{N} (t_{l} - y_{l})^{2} = \frac{1}{2} \sum_{l=1}^{N} \left\{ t_{l} - U(\ve{w}^{\mathsf{T}}\ve{x}_{l}) \right\}^{2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
とする&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;と, 後に示す様に局所最適解に嵌ってしまう可能性がある. 第一,
単位ステップ関数&lt;span class="math"&gt;\(U(u)\)&lt;/span&gt;はもとより微分可能では無く,
このままでは学習則を導出できない. そこで, まず,
ユニットの活性化関数を一旦微分可能なシグモイド関数&lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;とし,
その出力を&lt;span class="math"&gt;\(y_{l}=1\)&lt;/span&gt;となる確率&lt;span class="math"&gt;\(p(y_{l}=1)\)&lt;/span&gt;として定義する:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  p(y_{l} = 1) = \varphi(u_{l}/T) = \frac{1}{1+\exp(-u_{l}/T)}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
ここで, &lt;span class="math"&gt;\(T \geq 0\)&lt;/span&gt;は温度パラメタと呼ばれ,
図のグラフで見れるように&lt;span class="math"&gt;\(T \to 0\)&lt;/span&gt;とすると単位ステップ関数に漸近することが分かる.&lt;/p&gt;
&lt;p&gt;&lt;img alt="様々な温度パラメタ\(T\)におけるシグモイド関数\(\varphi(u/T)\)のグラフ" src="./images/sigmoids.eps"&gt;&lt;/p&gt;
&lt;p&gt;同時にラベル&lt;span class="math"&gt;\(t_{l}\)&lt;/span&gt;もある確率分布&lt;span class="math"&gt;\(q\)&lt;/span&gt;に従って生成される確率変数と考える事ができ,
&lt;span class="math"&gt;\(t_{l}\)&lt;/span&gt;が&lt;span class="math"&gt;\(1\)&lt;/span&gt;を取る確率は&lt;span class="math"&gt;\(q(t_{l}=1) = t_{l}\)&lt;/span&gt;で定義することができる.
この様に定義した出力とラベルの確率分布&lt;span class="math"&gt;\(q,p\)&lt;/span&gt;間の"違い"を誤差&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;とする.
特に機械学習では,
確率分布間の違いを測る尺度として非常に重要な&lt;strong&gt;KLダイバージェンス（Kullback-Leibler
divergence）&lt;/strong&gt;&lt;span class="math"&gt;\(KL(q||p)\)&lt;/span&gt;がある: &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  KL(q||p) = \sum_{l=1}^{N} q(t_{l}) \log\left[ \frac{q(t_{l})}{p(t_{l})} \right]\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
すぐに分かるように&lt;span class="math"&gt;\(KL(q||p) = 0\)&lt;/span&gt;となるのは&lt;span class="math"&gt;\(q\)&lt;/span&gt;と&lt;span class="math"&gt;\(p\)&lt;/span&gt;が完全に一致する時（&lt;span class="math"&gt;\(q(t_{l}) = p(t_{l})\ (l=1,\dots,N)\)&lt;/span&gt;）のみである.&lt;/p&gt;
&lt;p&gt;それでは誤差&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;をKLダイバージェンスとして,
その&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;による偏微分を計算する事を考える. まず,
&lt;span class="math"&gt;\(KL(q||p)\)&lt;/span&gt;は定義式から次の様に展開できる: &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  E(\ve{w}) &amp;amp;= KL(q||p) = \sum_{l=1}^{N} q(t_{l}) \log\left[ \frac{q(t_{l})}{p(t_{l})} \right] \\
  &amp;amp;= \sum_{l=1}^{N} \left\{ q(t_{l}=0) \log\left[ \frac{q(t_{l}=0)}{p(t_{l}=0)} \right] + q(t_{l}=1) \log\left[ \frac{q(t_{l}=1)}{p(t_{l}=1)} \right] \right\} \\
  &amp;amp;= \sum_{l=1}^{N} \left\{ (1-t_{l}) \log\left( \frac{1-t_{l}}{1-\varphi(u_{l}/T)} \right) + t_{l} \log \left( \frac{t_{l}}{\varphi(u_{l}/T)} \right) \right\}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
見通しを良くする為に和の内部を&lt;span class="math"&gt;\(e_{l}(\ve{w})\)&lt;/span&gt;とおき,
&lt;span class="math"&gt;\(e_{l}(\ve{w})\)&lt;/span&gt;を&lt;span class="math"&gt;\(w_{i}\)&lt;/span&gt;で偏微分すると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{}{w_{i}} e_{l}(\ve{w}) &amp;amp;= \parfrac{e_{l}(\ve{w})}{\varphi(u_{l}/T)} \parfrac{\varphi(u_{l}/T)}{w_{i}} \quad (\because 合成関数の微分) \\
  &amp;amp;= \left\{ (1-t_{l})\frac{1}{1-\varphi(u_{l}/T)} - \frac{t_{l}}{\varphi(u_{l}/T)} \right\} \parfrac{\varphi(u_{l}/T)}{u_{l}} \parfrac{u_{l}}{w_{i}} \quad (\because 合成関数の微分) \\
  &amp;amp;= \frac{\varphi(u_{l}/T)(1 - t_{l}) - t_{l} \left\{ 1 - \varphi(u_{l}/T) \right\}}{\varphi(u_{l}/T) \left\{ 1 - \varphi(u_{l}/T) \right\}} \frac{1}{T} \varphi(u_{l}/T) \left\{ 1 - \varphi(u_{l}/T) \right\} (\ve{x}_{l})_{i} \\
  &amp;amp;= \frac{1}{T} \left\{ \varphi(u_{l}/T) - t_{l} \right\} (\ve{x}_{l})_{i}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
が得られ, 更新量&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;は &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta \ve{w} &amp;amp;= - \eta \parfrac{E(\ve{w})}{\ve{w}} = - \eta \frac{1}{T} \sum_{l=1}^{N} \parfrac{}{\ve{w}} e_{l}(\ve{w}) \\
  &amp;amp;= - \frac{\eta}{T} \sum_{l=1}^{N}(\varphi(u_{l}/T) - t_{l}) \ve{x}_{l} = - \frac{\eta}{T} \sum_{l=1}^{N} \delta_{l} \ve{x}_{l}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
とまとめられる. ここで,
&lt;span class="math"&gt;\(\delta_{l} = \varphi(u_{l}/T) - t_{l}\)&lt;/span&gt;は誤差信号と呼ばれる.
シグモイド関数から単位ステップ関数に戻すために&lt;span class="math"&gt;\(T \to 0\)&lt;/span&gt;とするが,
同時に&lt;span class="math"&gt;\(\eta \to 0\)&lt;/span&gt;として&lt;span class="math"&gt;\((\eta / T) \to \epsilon\)&lt;/span&gt;となる様な&lt;span class="math"&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;をとって&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;が発散しないようにすれば,
デルタ則による重みの更新量&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;は, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta \ve{w} = -\epsilon \sum_{l=1}^{N} \delta_{l} \ve{x}_{l} = \epsilon \sum_{l=1}^{N} \left\{ t_{l} - U(u_{l}) \right\} \ve{x}_{l}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となる. この&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;も学習率と呼ばれ,
実践においては&lt;span class="math"&gt;\(0.1\)&lt;/span&gt;から&lt;span class="math"&gt;\(0.5\)&lt;/span&gt;あたりに設定される.
この学習則は&lt;span class="math"&gt;\(\sum_{l=1}^{N}\)&lt;/span&gt;の存在により,
全てのサンプルを提示して更新するのでこれを特に一括（斉時）学習（batch
learning）と呼ぶが, 1つのサンプル毎に重みを更新するやり方もある:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta \ve{w} = \epsilon \left\{ t_{l} - U(u_{l}) \right\} \ve{x}_{l}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
これは逐次学習（on-line learning）と呼ばれる.
一般に逐次学習の方が収束が早い事が知られている&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;この更新則による学習が局所最適に陥らないことを示す.
&lt;span class="math"&gt;\(\displaystyle\parfrac{e_{l}(\ve{w})}{w_{i}}\)&lt;/span&gt;を更に&lt;span class="math"&gt;\(w_{j}\)&lt;/span&gt;で偏微分し2階の偏導関数を求めると,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{ }{w_{j}}\parfrac{e_{l}(\ve{w})}{w_{i}} &amp;amp;= \parfrac{{}^{2} e_{l}(\ve{w})}{w_{i} \partial w_{j}} \\
  &amp;amp;= \frac{1}{T} \parfrac{\varphi(u_{l}/T)}{w_{j}} x_{i} = \frac{1}{T^{2}} \varphi(u_{l}/T) \left\{ 1 - \varphi(u_{l}/T) \right\} (\ve{x}_{l})_{i}(\ve{x}_{l})_{j}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となり,
&lt;span class="math"&gt;\((H)_{ij} = \displaystyle \parfrac{{}^{2} e_{l}(\ve{w})}{w_{i} \partial w_{j}}\)&lt;/span&gt;なる&lt;span class="math"&gt;\(e_{l}(\ve{w})\)&lt;/span&gt;のヘッセ行列（Hessian
matrix）&lt;span class="math"&gt;\(H\)&lt;/span&gt;は &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  H = \frac{1}{T^{2}} \varphi(u_{l}/T) \left\{ 1 - \varphi(u_{l}/T) \right\} \ve{x}_{l} \ve{x}_{l}^{\mathsf{T}}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
で計算できる.
明らかに&lt;span class="math"&gt;\(\displaystyle\frac{1}{T^{2}}\varphi(u_{l}/T) \left\{ 1 - \varphi(u_{l}/T) \right\} &amp;gt; 0\)&lt;/span&gt;であり,
行列&lt;span class="math"&gt;\(\ve{x}_{l}\ve{x}_{l}^{\mathsf{T}}\)&lt;/span&gt;は任意のベクトル&lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt;に対して二次形式&lt;span class="math"&gt;\(\ve{v}^{\mathsf{T}} (\ve{x}_{l}\ve{x}_{l}^{\mathsf{T}}) \ve{v}\)&lt;/span&gt;が,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \ve{v}^{\mathsf{T}} (\ve{x}_{l}\ve{x}_{l}^{\mathsf{T}} ) \ve{v} &amp;amp;= (\ve{x}_{l}^{\mathsf{T}} \ve{v})^{\mathsf{T}} (\ve{x}_{l}^{\mathsf{T}} \ve{v}) = (\ve{x}_{l}^{\mathsf{T}} \ve{v})^{2} \geq 0\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となるので半正定値行列である. 従って, ヘッセ行列&lt;span class="math"&gt;\(H\)&lt;/span&gt;も半正定値となり,
&lt;span class="math"&gt;\(e_{l}(\ve{w})\)&lt;/span&gt;は凸関数であることが分かり,
極小値が大域的な最小値に一致する（局所最小値が存在しない）ことが確かめられた.&lt;/p&gt;
&lt;p&gt;最後に誤差&lt;span class="math"&gt;\(E(\ve{w})\)&lt;/span&gt;として二乗誤差を用いた場合の更新量&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;を求めておく.
今度は&lt;span class="math"&gt;\(e_{l}(\ve{w}) = \displaystyle \frac{1}{2} (t_{l} - y_{l})^{2}\)&lt;/span&gt;とおき,
ユニットの活性化関数を一般に微分可能な関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;とすると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{ }{w_{i}} e_{l}(\ve{w}) &amp;amp;= \parfrac{e_{l}(\ve{w})}{y_{l}} \parfrac{y_{l}}{u_{l}}\parfrac{u_{l}}{w_{i}} \quad (\because 合成関数の微分) \\
  &amp;amp;= -(t_{l} - y_{l}) f^{\prime} (u_{l}) x_{i} \quad (f^{\prime} (u_{l}) \equiv \parfrac{y_{l}}{u_{l}} = \parfrac{ }{u_{l}} f(u_{l})) \\
  &amp;amp;= \delta_{l} f^{\prime}(u_{l}) x_{i}\end{aligned}$$&lt;/div&gt;
&lt;p&gt; となる.
従って更新量&lt;span class="math"&gt;\(\Delta \ve{w}\)&lt;/span&gt;は &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \Delta \ve{w} = - \eta \sum_{l=1}^{N} \delta_{l} f^{\prime}(u_{l}) \ve{x}_{l}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となる. さて,
この学習則は局所最小値におちいる場合がある事に上で言及したが,
これは&lt;span class="math"&gt;\(e_{l}(\ve{w})\)&lt;/span&gt;の2階の偏導関数が &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{{}^{2} e_{l}(\ve{w})}{w_{i} \partial w_{j}} &amp;amp;= \parfrac{ }{w_{j}} y_{l} f^{\prime}(u_{l}) x_{i} - (t_{l} - y_{l}) \parfrac{ }{w_{j}} f^{\prime}(u_{l}) x_{i} \\
  &amp;amp;= \left\{ (f^{\prime}(u_{l}))^{2} - (t_{l} - y_{l}) f^{\prime\prime} (u_{l}) \right\} x_{i} x_{j}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となるが,
&lt;span class="math"&gt;\((f^{\prime}(u_{l}))^{2} - (t_{l} - y_{l}) f^{\prime\prime} (u_{l})\)&lt;/span&gt;が常に非負になるとは限らないからである.
実際, &lt;span class="math"&gt;\(f\)&lt;/span&gt;をシグモイド関数とすると2階微分は &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  f^{\prime\prime}(u_{l}) &amp;amp;= f^{\prime}(u_{l}) (1-f(u_{l})) - f(u_{l}) f^{\prime}(u_{l}) = f^{\prime}(u_{l}) (1 - 2 f(u_{l})) 
  \\
  &amp;amp;= f(u_{l}) (1 - f(u_{l}))(1-2f(u_{l}))\end{aligned}$$&lt;/div&gt;
&lt;p&gt; であり,
&lt;span class="math"&gt;\(t_{l} = 1\)&lt;/span&gt;とすると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  (f^{\prime}(u_{l}))^{2} - (1 - y_{l}) f^{\prime\prime} (u_{l}) &amp;amp;= (f^{\prime}(u_{l}))^{2} + f^{\prime}(u_{l})(1-2f(u_{l})) - 1 \\
  &amp;amp;= f^{\prime}(u_{l}) (f^{\prime}(u_{l}) + 1 - 2f(u_{l}) ) - 1 \\
  &amp;amp;= f(u_{l}) (1 - f(u_{l})) (-(f(u_{l}))^{2} + 1 - f(u_{l})) - 1 &amp;lt; 0\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となってしまう.
従って二乗誤差を用いる場合はヘッセ行列が半正定値行列とならず,
誤差が局所最小値におちいる場合がある.&lt;/p&gt;
&lt;h1&gt;多層パーセプトロン&lt;/h1&gt;
&lt;p&gt;単層パーセプトロンはサンプルが直線（平面）で分離できる（線形分離可能な）問題にしか適用できない事&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt;が1969年にMinskey-Papertに指摘された.
線形分離不可能な例としてよく例に引き出されるのが図&lt;a href="#fig:XOR_problem"&gt;3&lt;/a&gt;の&lt;strong&gt;XOR問題&lt;/strong&gt;である.&lt;/p&gt;
&lt;p&gt;&lt;img alt="XOR問題" id="fig:XOR_problem" src="./images/XOR_problem.eps"&gt;&lt;/p&gt;
&lt;p&gt;この問題は1本の直線では分離できず,
従って単層パーセプトロンでは正しく学習することができない.
この線形分離不可能な問題のために,
NN研究の第一次ブームは終焉を迎え最初の冬の時代が訪れた.&lt;/p&gt;
&lt;p&gt;この問題は1986年,
Rumelhart-McClelland（デビット・ラメルハート-ジェームス・マクレランド）によって提案された&lt;strong&gt;多層パーセプトロン（multi-layer
perceptron, MLP）&lt;/strong&gt;によって解決を見た.
多層パーセプトロンは図&lt;a href="#fig:MLP"&gt;4&lt;/a&gt;に表される様に, 入力層（input layer）,
任意個数の中間（隠れ）層（middle(hidden) layer）, 出力層（output
layer）からなる多層構造を持ち,
全てのユニット出力の活性化関数は非線形関数（大体はシグモイド関数）となっている.&lt;/p&gt;
&lt;p&gt;&lt;img alt="多層パーセプトロン" id="fig:MLP" src="./images/multi_layer_perceptron.eps"&gt;&lt;/p&gt;
&lt;p&gt;多層パーセプトロンが線形分離不可能な問題にも適用できるのは,
主に次の2つの理由による:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;階層構造を用いている事: 
    これは形式ニューロンのXOR素子で既に示唆されていたが,
    ニューロンを階層的に繋いで全ての重みを可変にすれば,
    1つのユニットが1つの分離結果を持つため複数の分離結果を合成することができる.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ユニットの出力が非線形であること: 
    ユニットの出力を非線形にすることで,
    線形分離不可能な入力をニューロン内部で非線形変換し,
    線形分離可能な問題に還元できる場合がある.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;多層パーセプトロンは様々な現実的な問題に適用できる為に,
NNの第二次研究ブームを引き起こした. 現在においても,
一口にNNと言うと3層（1つの中間層）からなる多層パーセプトロン（3層NN）の事を指すことが多い.&lt;/p&gt;
&lt;h2&gt;多層パーセプトロンの学習則 - 逆誤差伝搬法&lt;/h2&gt;
&lt;p&gt;全ての重みが可変となった多層パーセプトロンでは,
単層パーセプトロンにおける学習則の様に出力層の重みを更新するだけではなく,
全ての重みを逐次更新していく必要がある.
多層パーセプトロンの学習として標準的に用いられる&lt;strong&gt;逆誤差伝搬法（(error)
back-propagation method）&lt;/strong&gt;は,
出力層での誤差を順次後ろ向きに（出力&lt;span class="math"&gt;\(\to\)&lt;/span&gt;中間&lt;span class="math"&gt;\(\to\)&lt;/span&gt;入力層の順に）伝播させて重みを更新していく手法である.&lt;/p&gt;
&lt;p&gt;それでは学習則を導出していくが,
多層構造を表現する為に次の定義を導入する. まず, 入力層を第&lt;span class="math"&gt;\(1\)&lt;/span&gt;層,
入力層と繋がった中間層を第&lt;span class="math"&gt;\(2\)&lt;/span&gt;層, 第&lt;span class="math"&gt;\(2\)&lt;/span&gt;層と繋がった層を第&lt;span class="math"&gt;\(3\)&lt;/span&gt;層,
&lt;span class="math"&gt;\(\dots\)&lt;/span&gt;と呼び, 出力層は第&lt;span class="math"&gt;\(n\)&lt;/span&gt;層とする.
即ち&lt;span class="math"&gt;\(n\)&lt;/span&gt;層構造の多層パーセプトロンを考える. また,
各層のユニット個数は一般に異なっても良いことにし,
第&lt;span class="math"&gt;\(k\)&lt;/span&gt;層におけるユニットの数を&lt;span class="math"&gt;\(L_{k}\)&lt;/span&gt;と表す.
第&lt;span class="math"&gt;\(k-1\)&lt;/span&gt;層における第&lt;span class="math"&gt;\(i\)&lt;/span&gt;ユニットと第&lt;span class="math"&gt;\(k\)&lt;/span&gt;層における第&lt;span class="math"&gt;\(j\)&lt;/span&gt;ユニットを繋ぐ重みを&lt;span class="math"&gt;\(w_{ij}^{k-1,k}\)&lt;/span&gt;と表し,
第&lt;span class="math"&gt;\(k\)&lt;/span&gt;層の第&lt;span class="math"&gt;\(i\)&lt;/span&gt;ユニットへの入力総量を&lt;span class="math"&gt;\(u_{i}^{k}\)&lt;/span&gt;と,
またその出力を&lt;span class="math"&gt;\(y_{i}^{k} = f(u_{i}^{k})\)&lt;/span&gt;と表す.
&lt;span class="math"&gt;\(f\)&lt;/span&gt;は微分可能な活性化関数ならば何でも良いが,
ここではシグモイド関数とする. また,
出力層に複数ユニットが存在するのでサンプルラベルも各出力ユニットに対応して用意し,
&lt;span class="math"&gt;\(i\)&lt;/span&gt;番目の出力ユニットに与えるラベルを&lt;span class="math"&gt;\(t^{l}_{i}\ (i=1,\dots,N)\)&lt;/span&gt;と表す.&lt;/p&gt;
&lt;p&gt;デルタ則の導出と同様に誤差の勾配を取ることを考える. 無論,
局所最小を回避する為に誤差関数&lt;span class="math"&gt;\(E\)&lt;/span&gt;としてKLダイバージェンスを導入する.
&lt;span class="math"&gt;\(E\)&lt;/span&gt;を&lt;span class="math"&gt;\(w_{ij}^{k-1,k}\)&lt;/span&gt;によって偏微分すると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{E}{w_{ij}^{k-1, k}} &amp;amp;= \parfrac{E}{u_{j}^{k}} \parfrac{u_{j}^{k}}{w_{ij}^{k-1,k}} \quad (\because 合成関数の微分) \\
  &amp;amp;= \parfrac{E}{u_{j}^{k}} \parfrac{ }{w_{ij}^{k-1,k}} \left( \sum_{s=1}^{L_{k-1}} w_{sj}^{k-1,k} y_{s}^{k-1} \right) = \parfrac{E}{u_{j}^{k}} y_{i}^{k-1}  \\
  &amp;amp;= \parfrac{E}{y_{j}^{k}} \parfrac{y_{j}^{k}}{u_{j}^{k}} y_{i}^{k-1} \quad (\because 合成関数の微分) \\
  &amp;amp;= \parfrac{E}{y_{j}^{k}} f^{\prime}(u_{j}^{k}) y_{i}^{k-1} \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
ここで&lt;span class="math"&gt;\(\displaystyle\parfrac{E}{y_{j}^{k}}\)&lt;/span&gt;は出力層の場合（&lt;span class="math"&gt;\(k=n\)&lt;/span&gt;）と中間層の場合（&lt;span class="math"&gt;\(k&amp;lt;n\)&lt;/span&gt;）で結果が異なる.
出力層の場合は, デルタ則の結果から, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{E}{y_{j}^{n}} &amp;amp;= \frac{1-t_{j}^{l}}{1-y_{j}^{n}} - \frac{t_{j}^{l}}{y_{j}^{n}} = \frac{y_{j}^{n}(1-t_{j}^{l}) - t_{j}^{l}(1 - y_{j}^{n})}{y_{j}^{n}(1-y_{j}^{n})} = \frac{y_{j}^{n} - t_{j}^{l}}{f^{\prime}(u_{j}^{n})} \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
となり, 一方中間層の場合は, 偏微分の連鎖律（chain rule）によって,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{E}{y_{j}^{k}} &amp;amp;= \sum_{s=1}^{L_{k+1}} \parfrac{E}{u_{s}^{k+1}} \parfrac{u_{s}^{k+1}}{y_{j}^{k}} \\
  &amp;amp;= \sum_{s=1}^{L_{k+1}} \parfrac{E}{u_{s}^{k+1}} \parfrac{}{y_{j}^{k}} \left( \sum_{t=1}^{L_{k}} w_{ts}^{k, k+1} y_{t}^{k} \right) = \sum_{s=1}^{L_{k+1}} \parfrac{E}{u_{s}^{k+1}} w_{js}^{k,k+1} \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
と展開できる. これらの結果をまとめると, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{E}{w_{ij}^{k-1, k}} = 
  \left\{ \begin{array}{ll}
    \displaystyle \frac{y_{j}^{n} - t_{j}^{l}}{f^{\prime}(u_{j}^{n})} f^{\prime}(u_{j}^{n}) y_{j}^{n-1} = (y_{j}^{n} - t_{l}) y_{i}^{n-1} &amp;amp; (k = n) \\
    \displaystyle \sum_{s=1}^{L_{k+1}} \parfrac{E}{u_{s}^{k+1}} w_{js}^{k,k+1} f^{\prime}(u_{j}^{k}) y_{i}^{k-1} &amp;amp; (k &amp;lt; n)
  \end{array} \right.\end{aligned}$$&lt;/div&gt;
&lt;p&gt; となるが,
次の第&lt;span class="math"&gt;\(k\)&lt;/span&gt;層の&lt;span class="math"&gt;\(i\)&lt;/span&gt;番目のユニットの&lt;strong&gt;誤差信号&lt;/strong&gt;&lt;span class="math"&gt;\(\delta_{i}^{k}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \delta_{i}^{k} &amp;amp;= \parfrac{E}{u_{i}^{k}} = \parfrac{E}{y_{i}^{k}} \parfrac{y_{i}^{k}}{u_{i}^{k}} \\
  &amp;amp;= 
  \left\{ \begin{array}{ll}
    y_{i}^{n} - t_{i}^{l} &amp;amp; (k = n) \\
    \displaystyle \sum_{s=1}^{L_{k+1}} \parfrac{E}{u_{s}^{k+1}}w_{js}^{k,k+1} f^{\prime}(u_{i}^{k}) = \sum_{s=1}^{L_{k+1}} \delta_{s}^{k+1} w_{js}^{k,k+1}  f^{\prime}(u_{i}^{k}) &amp;amp; (k &amp;lt; n)
  \end{array} \right.\end{aligned}$$&lt;/div&gt;
&lt;p&gt; を用いれば,
より簡潔に勾配を表現できる: &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \parfrac{E}{w_{ij}^{k-1, k}} = y_{i}^{k-1} \delta_{j}^{k}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
以上により,
逆誤差伝搬法は次の手順に従って重みを更新すれば良い事が分かる:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k \leftarrow n\)&lt;/span&gt;とする.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;誤差信号&lt;span class="math"&gt;\(\delta_{i}^{k}\ (i = 1,\dots,L_{k})\)&lt;/span&gt;の計算を行う:
    &lt;div class="math"&gt;$$\begin{aligned}
          \delta_{i}^{k} = 
          \left\{ \begin{array}{ll}
            y_{i}^{n} - t_{i}^{l} &amp;amp; (k = n) \\
            \displaystyle \sum_{s=1}^{L_{k+1}} \delta_{s}^{k+1} w_{js}^{k,k+1}  f^{\prime}(u_{i}^{k}) &amp;amp; (k &amp;lt; n)
          \end{array} \right.
        \end{aligned}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k \leftarrow k-1\)&lt;/span&gt;とする.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k = 1\)&lt;/span&gt;ならば次へ, そうでなければ2. に戻る.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重みを更新する: &lt;div class="math"&gt;$$\begin{aligned}
          w_{ij}^{k-1,k} \leftarrow w_{ij}^{k-1,k} - \eta y_{i}^{k-1} \delta_{j}^{k} \quad (k=2,\dots,n,\ i = 1,\dots, L_{k-1},\ j = 1,\dots,L_{k})
        \end{aligned}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;これは基本となる逐次学習法であるが,
一括学習の時は&lt;span class="math"&gt;\(\delta_{i}^{n}\)&lt;/span&gt;の所でサンプルについての和を取って次のようにすれば良い:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
  \delta_{i}^{n} = \sum_{l=1}^{N} (y_{i}^{n} - t_{i}^{l})\end{aligned}$$&lt;/div&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;これは非常に単純なモデルであり,
理解や応用が簡単な為に様々な場面で用いられる.
特に機械学習では無批判にこのユニットを用いる向きがある. しかし,
このモデルは厳密にニューロンの動作を表現できてはいないことに注意が必要である.
例えば,
このモデルでは入力&lt;span class="math"&gt;\(u\)&lt;/span&gt;が強ければ常に高電位を放出する事になるが,
実際にはニューロンはパルスを放出した後は一時的に放出電位が下がる事が実験により知られている.
よりニューロンの動作を精密に表したモデルにホジキン-ハックスレー型のニューロンモデルがある&lt;sup id="fnref:13"&gt;&lt;a class="footnote-ref" href="#fn:13"&gt;13&lt;/a&gt;&lt;/sup&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;正例のラベルを&lt;span class="math"&gt;\(1\)&lt;/span&gt;,
負例のラベルを&lt;span class="math"&gt;\(0\)&lt;/span&gt;としたり問題に応じて都合良く決められるが,
識別できる二値なら何でもよく, 本質的な違いは存在しない.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;この正規化によっても一般性は全く失われない.
&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;は入力の重みの比率を定めているに過ぎず,
実際&lt;span class="math"&gt;\(u = \ve{w}^{\mathsf{T}}\ve{x} + b\)&lt;/span&gt;から見れるように,
&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;は面（識別面という）の法ベクトルとなっている.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;最も面&lt;span class="math"&gt;\(\ve{w}^{\ast}\)&lt;/span&gt;に近いベクトルの距離を表しており,
&lt;strong&gt;マージン&lt;/strong&gt;と呼ばれる. かの有名なSVMのマージンそのものである&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;2つのベクトル&lt;span class="math"&gt;\(\ve{v}_{1}, \ve{v}_{2}\)&lt;/span&gt;がなす角度&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;は&lt;span class="math"&gt;\(\cos\theta = \ve{v}_{1}^{\mathsf{T}}\ve{v}_{2}/(||\ve{v}_{1}||||\ve{v}_{2}||)\)&lt;/span&gt;により求められるので,
&lt;span class="math"&gt;\(\theta=0\)&lt;/span&gt;ならば2つのベクトルは一致している（類似度が最大）と見ることができる.
角度&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;が&lt;span class="math"&gt;\(0\)&lt;/span&gt;に近い（類似度が高い）時は内積&lt;span class="math"&gt;\(\ve{v}_{1}^{\mathsf{T}}\ve{v}_{2}\)&lt;/span&gt;が高い値を取ることが分かる&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;大域的な最小解とは限らない事に注意&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;係数の&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;に本質的な意味は無いが,
微分の際に計算を簡単にする狙いがある.&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;しかし, 局所最適に嵌ってしまうリスクが潜んでいる&amp;#160;&lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\ve{w}\)&lt;/span&gt;は識別面の法ベクトルを表すが,
出力を単位ステップ（もしくは符号）関数とすると入力ベクトルが面の上半領域にある場合は&lt;span class="math"&gt;\(1\)&lt;/span&gt;を,
下半領域にある場合は&lt;span class="math"&gt;\(0(-1)\)&lt;/span&gt;を出力する.&amp;#160;&lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;ノーバート・ウィーナー, 池原止戈夫, 彌永昌吉, 室賀三郎, 戸田巌, "ウィーナー サイバネティックス ―動物と機械における制御と通信" 岩波書店, 2011&amp;#160;&lt;a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;庄野逸, &lt;a href="http://www.slideshare.net/HAL9801/20140705"&gt;Deep Learning 勉強会(1)&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;高橋治久, 堀田一弘, "学習理論" コロナ社, 2009&amp;#160;&lt;a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:12" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:13"&gt;
&lt;p&gt;&lt;a href="http://www.mbs.med.kyoto-u.ac.jp/cortex/23_HH_model.pdf"&gt;ホジキン-ハックスレー型のニューロンモデル&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="古事記"></category><category term="機械学習"></category></entry><entry><title>最大エントロピーモデル</title><link href="/zui-da-entoropimoderu.html" rel="alternate"></link><published>2020-04-23T12:40:00+09:00</published><updated>2020-04-23T12:40:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/zui-da-entoropimoderu.html</id><summary type="html">&lt;p&gt;最大エントロピーモデルの導出過程、学習の更新則、素性選択についての理論的側面を述べる。記述の大部分は&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;を参照し、一部&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;も参照している。&lt;/p&gt;
&lt;p&gt;最大エントロピーモデルは、データの特徴を &lt;strong&gt;素性関数(feature function)&lt;/strong&gt; によって記述し、素性関数がある &lt;strong&gt;制約(constraint)&lt;/strong&gt; を満たし、かつ、モデルを表現する確率分布のエントロピーが最大となる（最大エントロピー原理を満たす）モデルである。&lt;/p&gt;
&lt;p&gt;エントロピーを最大にする事により、制約を満たしながら最大エントロピーモデルの確率分布が最も一様に分布する様になり、未知データに対する確率を無下に&lt;span class="math"&gt;\(0\)&lt;/span&gt;にすることが無くなるため、高い汎用性（汎化性能）が期待できる。&lt;/p&gt;
&lt;h2&gt;モデルを表現する確率分布の導出&lt;/h2&gt;
&lt;p&gt;まず、サンプル（事例）データのドメイン（定義域）を&lt;span class="math"&gt;\(X\)&lt;/span&gt;、データに付与されたラベルのドメインを&lt;span class="math"&gt;\(Y\)&lt;/span&gt;と書く。例えば、次に来る単語を予測させたい場合には、サンプル&lt;span class="math"&gt;\(X\)&lt;/span&gt;は1つ前までの単語の並び、ラベル&lt;span class="math"&gt;\(Y …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;最大エントロピーモデルの導出過程、学習の更新則、素性選択についての理論的側面を述べる。記述の大部分は&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;を参照し、一部&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;も参照している。&lt;/p&gt;
&lt;p&gt;最大エントロピーモデルは、データの特徴を &lt;strong&gt;素性関数(feature function)&lt;/strong&gt; によって記述し、素性関数がある &lt;strong&gt;制約(constraint)&lt;/strong&gt; を満たし、かつ、モデルを表現する確率分布のエントロピーが最大となる（最大エントロピー原理を満たす）モデルである。&lt;/p&gt;
&lt;p&gt;エントロピーを最大にする事により、制約を満たしながら最大エントロピーモデルの確率分布が最も一様に分布する様になり、未知データに対する確率を無下に&lt;span class="math"&gt;\(0\)&lt;/span&gt;にすることが無くなるため、高い汎用性（汎化性能）が期待できる。&lt;/p&gt;
&lt;h2&gt;モデルを表現する確率分布の導出&lt;/h2&gt;
&lt;p&gt;まず、サンプル（事例）データのドメイン（定義域）を&lt;span class="math"&gt;\(X\)&lt;/span&gt;、データに付与されたラベルのドメインを&lt;span class="math"&gt;\(Y\)&lt;/span&gt;と書く。例えば、次に来る単語を予測させたい場合には、サンプル&lt;span class="math"&gt;\(X\)&lt;/span&gt;は1つ前までの単語の並び、ラベル&lt;span class="math"&gt;\(Y\)&lt;/span&gt;は今の単語となる。
データとラベルを組にすることで1つの学習サンプルが構成され、また、モデルに与える&lt;span class="math"&gt;\(m\)&lt;/span&gt;個の学習サンプルの集合&lt;span class="math"&gt;\(Z_{m} \subset 2^{X\times Y}\)&lt;/span&gt;を次で表す:&lt;/p&gt;
&lt;div class="math"&gt;$$
  Z_{m} = \{ (x_{1}, y_{1}), (x_{2}, y_{2}), \dots, (x_{m}, y_{m}) \}
$$&lt;/div&gt;
&lt;p&gt;このようなサンプルに対し、&lt;strong&gt;素性関数（素性）&lt;/strong&gt;の集合&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;は次で定義される:&lt;/p&gt;
&lt;div class="math"&gt;$$
  {\cal F} = \{ f_{i} : X \times Y \to \{0,1\}, i \in \{1,2,\dots,n\} \}
$$&lt;/div&gt;
&lt;p&gt;即ち&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;は、データとラベルの組&lt;span class="math"&gt;\((x,y) \in X \times Y\)&lt;/span&gt;を受け取って&lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;いずれかを返す関数の集合である。ここでは&lt;span class="math"&gt;\(f\)&lt;/span&gt;の値域は議論の簡略化のため&lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;としたが、値域は&lt;span class="math"&gt;\(\{0,\alpha\} (\alpha &amp;gt; 0)\)&lt;/span&gt;、即ち&lt;span class="math"&gt;\(0\)&lt;/span&gt;と&lt;span class="math"&gt;\(0\)&lt;/span&gt;以外の正数実数を取るようにもできる。また、素性が条件を満たし正の値を取る時は、素性が活性化しているという。&lt;/p&gt;
&lt;p&gt;素性の例を挙げると、&lt;span class="math"&gt;\(n\)&lt;/span&gt;個の単語列&lt;span class="math"&gt;\(w_{1},\dots,w_{n}\)&lt;/span&gt;から、直前の&lt;span class="math"&gt;\(N-1\)&lt;/span&gt;個の単語列&lt;span class="math"&gt;\(w_{n-N+1},\dots,w_{n-1}\)&lt;/span&gt;のみを用いて今の単語&lt;span class="math"&gt;\(w_{n}\)&lt;/span&gt;を予測する（&lt;span class="math"&gt;\(N\)&lt;/span&gt;-グラムの）場合は、素性は次の様に表現出来る。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  f_{x_{1}x_{2}\dots x_{N}}(w_{1},\dots,w_{n-1},w_{n}) = 
  \left\{
    \begin{array}{ll}
      1 &amp;amp; w_{n-N+1} = x_{1}, w_{n-N+2} = x_{2}, \dots, w_{n-1} = x_{N-1}, w_{n} = x_{N} \\
      0 &amp;amp; {\rm otherwise}
    \end{array} \right.
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(f\)&lt;/span&gt;のインデックス&lt;span class="math"&gt;\(x_{1}\dots x_{N}\)&lt;/span&gt;は整数との対応を適当に取ることで、容易に実現できる。&lt;/p&gt;
&lt;p&gt;最大エントロピーモデルの制約として与えられる条件は、素性の平均（期待値）が、モデルと経験確率で一致することである。この条件を数式で表現する事を考える。&lt;/p&gt;
&lt;p&gt;定義域&lt;span class="math"&gt;\(X\times Y\)&lt;/span&gt;上に定義されるモデルの確率分布を&lt;span class="math"&gt;\(P(x,y)\)&lt;/span&gt;と書き、経験確率分布を&lt;span class="math"&gt;\(\tilde{P}(x,y)\)&lt;/span&gt;と書く。ここで経験確率分布&lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt;は、頻度確率で与える。即ち、学習サンプルに現れた&lt;span class="math"&gt;\((x,y)\)&lt;/span&gt;の組の回数を&lt;span class="math"&gt;\(C(x,y)\)&lt;/span&gt;と書くと、&lt;/p&gt;
&lt;div class="math"&gt;$$
  \tilde{P}(x,y) = \frac{C(x,y)}{m}
$$&lt;/div&gt;
&lt;p&gt;と表せる。モデルの確率分布は後で導出する。
ある素性&lt;span class="math"&gt;\(f_{i}\)&lt;/span&gt;の分布&lt;span class="math"&gt;\(p\)&lt;/span&gt;による平均を&lt;span class="math"&gt;\(E_{p}[f_{i}]\)&lt;/span&gt;と書くと、経験分布とモデルの確率分布のそれぞれの平均は&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  E_{\tilde{P}}[f_{i}] &amp;amp;= \sum_{x,y} \tilde{P}(x,y) f_{i}(x,y) \\  E_{P}[f_{i}] &amp;amp;= \sum_{x,y} P(x,y) f_{i}(x,y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;と表せられ、従って制約を数式で表現すると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  E_{\tilde{P}}[f_{i}] &amp;amp;= E_{P}[f_{i}] \ \ (i=1,\dots,n) \\
  \iff \sum_{x,y} \tilde{P}(x,y) f_{i}(x,y) &amp;amp;= \sum_{x,y} P(x,y) f_{i}(x,y) \ \ (i=1,\dots,n)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となる。最大エントロピーモデルの候補となる集合&lt;span class="math"&gt;\({\cal P}\)&lt;/span&gt;は、全ての素性に関する制約を満たすモデルの集合となる:&lt;/p&gt;
&lt;div class="math"&gt;$$
  {\cal P} = \{ P | E_{P}[f_{i}] = E_{\tilde{P}}[f_{i}], i = \{1,\dots,n\} \}
$$&lt;/div&gt;
&lt;p&gt;明らかに、2つのモデル&lt;span class="math"&gt;\(P,P^{\prime} \in {\cal P}\)&lt;/span&gt;に対して、&lt;span class="math"&gt;\(E_{P}[f_{i}] = E_{\tilde{P}}[f_{i}] = E_{P^{\prime}}[f_{i}]\ \ (i=1,\dots,n)\)&lt;/span&gt;（候補となるモデルの素性の平均は同一）となる。&lt;/p&gt;
&lt;p&gt;更に考慮すべき点は、最大エントロピーモデルの名の通り、モデル（確率分布&lt;span class="math"&gt;\(P\)&lt;/span&gt;）のエントロピーを最大にする必要がある。モデルのエントロピーを&lt;span class="math"&gt;\(H(P)\)&lt;/span&gt;と書くと、確率分布のエントロピーの式から,&lt;/p&gt;
&lt;div class="math"&gt;$$
  H(P) = -\sum_{x,y}P(x,y) \log P(x,y)
$$&lt;/div&gt;
&lt;p&gt;と表現できる。集合&lt;span class="math"&gt;\({\cal P}\)&lt;/span&gt;の中で最もエントロピーが高いものが得るべきモデル&lt;span class="math"&gt;\(P^{\ast}\)&lt;/span&gt;である:&lt;/p&gt;
&lt;div class="math"&gt;$$
  P^{\ast} = \underset{P \in {\cal P}}{\rm argmax}\ H(P)
$$&lt;/div&gt;
&lt;p&gt;この式を &lt;strong&gt;最大エントロピー原理(maximum entropy principle)&lt;/strong&gt; と呼ぶ。集合&lt;span class="math"&gt;\({\cal P}\)&lt;/span&gt;は無限集合だが最大エントロピー原理を満たすモデルは解析的に求められ、かつ一意に存在する（後術）。&lt;/p&gt;
&lt;p&gt;最大エントロピー原理を満たすモデルの確率分布&lt;span class="math"&gt;\(P\)&lt;/span&gt;を求める事を考える。これは制約付き非線形最適化問題であることから、ラグランジェの未定定数法が適用できる。&lt;span class="math"&gt;\(P\)&lt;/span&gt;が満たすべき制約を列挙すると&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  1:&amp;amp; \quad E_{P}[f_{i}] = E_{\tilde{P}}[f_{i}] \ \ (i=1,\dots,n) \\
  2:&amp;amp; \quad P(x,y) \geq 0 \\
  3:&amp;amp; \quad \sum_{x,y}P(x,y) = 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;であり（2,3は&lt;span class="math"&gt;\(P\)&lt;/span&gt;が確率分布となる為の条件）、&lt;span class="math"&gt;\(n\)&lt;/span&gt;個の制約に対応する未定定数を&lt;span class="math"&gt;\(\Lambda = \{\lambda_{1},\dots,\lambda_{n}\}\)&lt;/span&gt;と書くと、ラグランジアン（ラグランジュ関数）&lt;span class="math"&gt;\({\cal L}(P,\Lambda)\)&lt;/span&gt;は&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  {\cal L}(P, \Lambda) &amp;amp;= H(P) + \sum_{i=1}^{n} \lambda_{i} (E_{P}[f_{i}] - E_{\tilde{P}}[f_{i}]) \\
&amp;amp;= -\sum_{x,y}P(x,y)\log P(x,y) + \sum_{i=1}^{n} \lambda_{i} \left\{ \sum_{x,y} P(x,y) f_{i}(x,y) - \sum_{x,y} \tilde{P}(x,y) f_{i}(x,y) \right\}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;と書ける。最大値を得るため、&lt;span class="math"&gt;\(P(x,y)\)&lt;/span&gt;によって偏微分すると,&lt;/p&gt;
&lt;div class="math"&gt;$$
  \frac{\partial {\cal L}(P,\Lambda)}{\partial P(x,y)} = -\log P(x,y) - 1 + \sum_{i=1}^{n} \lambda_{i} f_{i}(x,y)
$$&lt;/div&gt;
&lt;p&gt;この式を&lt;span class="math"&gt;\(0\)&lt;/span&gt;とおいて&lt;span class="math"&gt;\(P(x,y)\)&lt;/span&gt;について解くと&lt;/p&gt;
&lt;div class="math"&gt;$$
  P(x,y) = \exp \left[ -1 + \sum_{i=1}^{n} \lambda_{i} f_{i}(x,y) \right]
$$&lt;/div&gt;
&lt;p&gt;を得る。確率分布が指数関数で表現される為条件2の非負条件は満たされるが、条件3の全確率が1になることが保証されていない。そこで &lt;span class="math"&gt;\(\sum_{x,y}P(x,y) = Z_{\Lambda}\)&lt;/span&gt;なる正規化項(normalization factor)を導入し&lt;span class="math"&gt;\(P(x,y)\)&lt;/span&gt;の&lt;span class="math"&gt;\(x,y\)&lt;/span&gt;についての総和が1になるようにする。従ってモデルの確率分布は次で表される:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  P(x,y) &amp;amp;= \frac{ \exp \left[ -1 + \sum_{i=1}^{n} \lambda_{i} f_{i}(x,y) \right] }{ \sum_{x,y} \exp \left[ -1 + \sum_{i=1}^{n} \lambda_{i} f_{i}(x,y) \right] } \\
  &amp;amp;= \frac{1}{Z_{\Lambda}} \exp \left[ \sum_{i} \lambda_{i} f_{i}(x,y) \right] \\
  Z_{\Lambda} &amp;amp;= \sum_{x,y} \exp \left[ \sum_{i} \lambda_{i} f_{i}(x,y) \right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;（以下、&lt;span class="math"&gt;\(\sum_{i=1}^{n} \equiv \sum_{i}\)&lt;/span&gt;とする）得られた確率分布はMRF(Markov Random Fields、マルコフ確率場)のクリークサイズを1とした時、即ち節点ポテンシャル（連想ポテンシャル）のみを考えた結合確率に一致する。従って最大エントロピーモデルはMRFのサブクラスとして捉えられる。&lt;/p&gt;
&lt;h2&gt;最大のエントロピー原理の性質と最尤推定&lt;/h2&gt;
&lt;p&gt;最大エントロピー原理を満たすモデルは上述の議論で求められたが、このモデルが唯一に定まる事を示す。まず、上述の議論で得られた確率分布を持つモデルの集合を&lt;span class="math"&gt;\({\cal Q}\)&lt;/span&gt;と書く:&lt;/p&gt;
&lt;div class="math"&gt;$$
  {\cal Q} = \left\{ P \left| P(x,y) = \frac{1}{Z_{\Lambda}} \exp\left[ \sum_{i} \lambda_{i}f_{i}(x,y) \right] \right. \right\}
$$&lt;/div&gt;
&lt;p&gt;集合&lt;span class="math"&gt;\({\cal Q}\)&lt;/span&gt;の要素に制約は陽に表れていない。そして、&lt;span class="math"&gt;\({\cal P,Q}\)&lt;/span&gt;と最大エントロピー原理について次の定理が成り立つ:&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;最大エントロピーモデルの唯一存在性&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P^{\ast} \in {\cal P} \cap {\cal Q}\)&lt;/span&gt;ならば,&lt;/p&gt;
&lt;div class="math"&gt;$$
  P^{\ast} = \underset{P \in {\cal P}}{\rm argmax} \ H(P)
$$&lt;/div&gt;
&lt;p&gt;が成り立ち、かつ&lt;span class="math"&gt;\(P^{\ast}\)&lt;/span&gt;は唯一に定まる。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;（証明）まず補助定理として、&lt;span class="math"&gt;\(R, S \in {\cal P}, T \in {\cal Q}\)&lt;/span&gt;ならば,&lt;/p&gt;
&lt;div class="math"&gt;$$
  \sum_{x,y} R(x,y) \log T(x,y) = \sum_{x,y} S(x,y) \log T(x,y)
$$&lt;/div&gt;
&lt;p&gt;を示す。&lt;span class="math"&gt;\(T \in {\cal Q}\)&lt;/span&gt;より&lt;span class="math"&gt;\(T(x,y) = \displaystyle\frac{1}{Z_{\Lambda}} \exp\left[ \sum_{i} \lambda_{i} f_{i}(x,y) \right]\)&lt;/span&gt;と表せるので、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  （左辺） &amp;amp;= \sum_{x,y} R(x,y) \left[ \sum_{i} \lambda_{i} f_{i}(x,y) - \log Z_{\Lambda} \right] = \sum_{i} \lambda_{i} \sum_{x,y} R(x,y) f_{i}(x,y) - \log Z_{\Lambda} \sum_{x,y}R(x,y) \\
  &amp;amp;= \sum_{i} \lambda_{i} E_{R}[f_{i}] - \log Z_{\Lambda} \\
  &amp;amp;= \sum_{i} \lambda_{i} E_{S}[f_{i}] - \log Z_{\Lambda} \ \ (\because E_{R}[f_{i}] = E_{\tilde{P}}[f_{i}] = E_{S}[f_{i}]） \\
  &amp;amp;= \sum_{x,y} S(x,y) \left[\sum_{i} \lambda_{i} f_{i}(x,y) \right] - \sum_{x,y} S(x,y) \log Z_{\Lambda} \\
  &amp;amp;= \sum_{x,y} S(x,y) \log T(x,y) = （右辺）
\end{align}
$$&lt;/div&gt;
&lt;p&gt;補助定理を用いて、定理の証明を行う。&lt;span class="math"&gt;\(P \in {\cal P}, P^{\ast} \in {\cal P} \cap {\cal Q}\)&lt;/span&gt;とすると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  H(P^{\ast}) - H(P) &amp;amp;= -\sum_{x,y} P^{\ast}(x,y) \log P^{\ast}(x,y) + \sum_{x,y} P(x,y) \log P(x,y) \\
  &amp;amp;= -\sum_{x,y} P(x,y) \log P^{\ast}(x,y) + \sum_{x,y} P(x,y) \log P(x,y) \ \ （\because 補助定理） \\
  &amp;amp;= \sum_{x,y} P(x,y) \log \left[ \frac{P(x,y)}{P^{\ast}(x,y)} \right] \\
  &amp;amp;= {\rm KL}(P || P^{\ast}) \geq 0\ \ （{\rm KL}:KLダイバージェンス）
\end{align}
$$&lt;/div&gt;
&lt;p&gt;よって&lt;span class="math"&gt;\(H(P^{\ast}) \geq H(P)\)&lt;/span&gt;が成立する。また&lt;span class="math"&gt;\(H(P^{\ast}) = H(P)\)&lt;/span&gt;ならばKLダイバージェンスの性質により&lt;span class="math"&gt;\(P^{\ast} = P\)&lt;/span&gt;となる。以上により、定理の成立が示された。&lt;/p&gt;
&lt;p&gt;生成モデルの学習に関連して、最大エントロピー原理を満たすモデル&lt;span class="math"&gt;\(P^{\ast}\)&lt;/span&gt;は、経験確率分布&lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt;が与えられた時に最大尤度を持つ事も示されている。モデルの尤度の式を導く事を考えると、まず経験確率分布&lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt;に対するモデル&lt;span class="math"&gt;\(P\)&lt;/span&gt;の経験誤差はKLダイバージェンス&lt;span class="math"&gt;\({\rm KL}(\tilde{P} || P)\)&lt;/span&gt;で与えられる&lt;sup id="fnref2:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;ので,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  {\rm KL}(\tilde{P} || P) &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log \left[ \frac{\tilde{P}(x,y)}{P(x,y)} \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log \tilde{P}(x,y) - \sum_{x,y} \tilde{P}(x,y) \log P(x,y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;なる。大数の弱法則より、サンプル数の極限&lt;span class="math"&gt;\(m\to \infty\)&lt;/span&gt;を取ることにより経験確率分布は標的概念の確率分布に一致し、また経験誤差は汎化誤差に一致する。今&lt;span class="math"&gt;\({\rm KL}(\tilde{P} || P) \geq 0\)&lt;/span&gt;であり、かつ、&lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt;は観測により固定されるので、経験誤差を最小にするには下段の式の第2項を最大化すれば良いことになる。そして、下段式の第2項は対数尤度（経験対数尤度）と呼ばれる。モデル&lt;span class="math"&gt;\(P\)&lt;/span&gt;の対数尤度を&lt;span class="math"&gt;\(L(P)\)&lt;/span&gt;と書くと、&lt;/p&gt;
&lt;div class="math"&gt;$$
  L(P) = \sum_{x,y} \tilde{P}(x,y) \log P(x,y)
$$&lt;/div&gt;
&lt;p&gt;と表すことができる。尤度との関連として、最大エントロピー原理を満たすモデル&lt;span class="math"&gt;\(P^{\ast}\)&lt;/span&gt;は次を満たす:&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;最大尤度を持つ最大エントロピーモデル&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P^{\ast} \in {\cal P} \cap {\cal Q}\)&lt;/span&gt;ならば、&lt;/p&gt;
&lt;div class="math"&gt;$$
  P^{\ast} = \underset{Q \in {\cal Q}}{\rm argmax} \ L(Q)
$$&lt;/div&gt;
&lt;p&gt;が成り立ち、かつ&lt;span class="math"&gt;\(P^{\ast}\)&lt;/span&gt;は唯一に定まる。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;（証明）前の定理と同様の方針と、補助定理により,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  L(P^{\ast}) - L(P) &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log P^{\ast}(x,y) - \sum_{x,y} \tilde{P}(x,y) \log P(x,y) \\
  &amp;amp;= \sum_{x,y} P^{\ast}(x,y) \log P^{\ast}(x,y) - \sum_{x,y} P^{\ast}(x,y) \log P(x,y) \ \ (\because 反射性 E_{\tilde{P}}[f_{i}] = E_{\tilde{P}}[f_{i}]により、\tilde{P} \in {\cal P}) \\
  &amp;amp;= \sum_{x,y} P^{\ast}(x,y) \log \left[ \frac{P^{\ast}(x,y)}{P(x,y)} \right] \\
  &amp;amp;= {\rm KL}(P^{\ast} || P) \geq 0
\end{align}
$$&lt;/div&gt;
&lt;p&gt;よって&lt;span class="math"&gt;\(L(P^{\ast}) \geq L(P)\)&lt;/span&gt;であり、再びKLダイバージェンスの性質により、&lt;span class="math"&gt;\(L(P^{\ast}) = L(P)\)&lt;/span&gt;ならば&lt;span class="math"&gt;\(P^{\ast} = P\)&lt;/span&gt;が成り立つので唯一性も示される。従って定理の成立が示された。&lt;/p&gt;
&lt;p&gt;定理1と2により、次の性質が成り立つ:&lt;/p&gt;
&lt;div class="math"&gt;$$
  P^{\ast} = \underset{P \in {\cal P}}{\rm argmax} \ H(P) = \underset{Q \in {\cal Q}}{\rm argmax} \ L(Q)
$$&lt;/div&gt;
&lt;p&gt;即ち、モデルの最大エントロピー原理は最尤推定の枠組みで捉える事もでき、尤度を最大化したモデルが最大のエントロピーを持つ。よって、モデルの学習には通常の生成モデルの学習と同じ様に、&lt;span class="math"&gt;\({\cal Q}\)&lt;/span&gt;の要素で表現されるモデルの尤度最大化を考えれば良いことになる。&lt;/p&gt;
&lt;h2&gt;最大のエントロピーモデルの学習 - 反復スケーリング法&lt;/h2&gt;
&lt;p&gt;最尤推定法に基づく最大エントロピーモデルの学習は、モデルの尤度が最大になるようにモデル&lt;span class="math"&gt;\(P\)&lt;/span&gt;のパラメタ&lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;を調節してやれば良い。単純なアプローチとしては、対数尤度&lt;span class="math"&gt;\(L(P)\)&lt;/span&gt;をパラメタ&lt;span class="math"&gt;\(\Lambda=\{\lambda_{1},\cdots,\lambda_{n}\}\)&lt;/span&gt;で偏微分し、最急上昇法によって最大値を得る方法がある。実際に計算してみると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \frac{\partial L(P)}{\partial \lambda_{i}} &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \frac{\partial}{\partial \lambda_{i}} \log P(x,y) \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \frac{\partial}{\partial \lambda_{i}} \left[ \sum_{j} \lambda_{j} f_{j}(x,y) - \log Z_{\Lambda} \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \left[ f_{i}(x,y) - \frac{1}{Z_{\Lambda}} \sum_{x^{\prime},y^{\prime}} f_{i}(x^{\prime},y^{\prime}) \exp \left( \sum_{j} \lambda_{j} f_{j}(x^{\prime},y^{\prime}) \right) \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \left( f_{i}(x,y) - E_{P}[f_{i}] \right) \\
  &amp;amp;= E_{\tilde{P}}[f_{i}] - E_{P}[f_{i}]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;であり（最適時には制約が満たされることが分かる）、ステップ&lt;span class="math"&gt;\(t\)&lt;/span&gt;におけるパラメタ&lt;span class="math"&gt;\(\lambda_{i}^{t}\)&lt;/span&gt;の更新規則は次の様に得られる:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \lambda_{i}^{t+1} &amp;amp;= \lambda_{i}^{t} + \eta \frac{\partial L(P)}{\partial \lambda_{i}^{t}} \\
  &amp;amp;= \lambda_{i}^{t} + \eta ( E_{\tilde{P}}[f_{i}] - E_{P}[f_{i}] )
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;は収束の早さを決める学習率(learning rate)であり、ヒューリスティックに決める必要がある。
この様に再急上昇法による学習は単純だが、学習（収束）が遅く、&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;を決めなければならないという問題がある。&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;を大きく設定し過ぎると勾配の谷を越えてしまい発散を招き、逆に小さく設定すると学習がいつまでたっても終わらない。現状、最大エントロピーモデルの学習では、反復スケーリング法(iterative scaling)という学習手法が伝統的に用いられている。&lt;/p&gt;
&lt;p&gt;反復スケーリング法の基本的な考え方は、まずパラメタ&lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;を&lt;span class="math"&gt;\(\Lambda+\Delta\)&lt;/span&gt;に変化させた時の対数尤度の変化量の下限&lt;span class="math"&gt;\(A(\Lambda,\Delta)\)&lt;/span&gt;を計算し、次にこの&lt;span class="math"&gt;\(A(\Lambda,\Delta)\)&lt;/span&gt;を最大にする&lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;を求める事で、結果増加量を最大にするようにしている。この考え方には学習率の様なヒューリスティックは介在せず、かつ毎ステップの対数尤度の増加量を最大にするようにパラメタを更新できる。&lt;/p&gt;
&lt;p&gt;それでは反復スケーリング法の更新式を導くことを考える。各パラメタの更新量を&lt;span class="math"&gt;\(\Delta=\{\delta_{1},\cdots,\delta_{n}\}\)&lt;/span&gt;と表すものとし、まず、パラメタ更新時の対数尤度の変化量&lt;span class="math"&gt;\(L(P_{\Lambda+\Delta})-L(P_{\Lambda})\)&lt;/span&gt;は,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  L(P_{\Lambda+\Delta})-L(P_{\Lambda}) &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log P_{\Lambda+\Delta}(x,y) - \sum_{x,y} \tilde{P}(x,y) \log P_{\Lambda}(x,y) \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log \left[ \frac{P_{\Lambda+\Delta}(x,y)}{P_{\Delta}(x,y)} \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log \left[ \frac{Z_{\Lambda}}{Z_{\Lambda+\Delta}} \frac{\exp\left[ \sum_{i}(\lambda_{i} + \delta_{i}) f_{i}(x,y) \right]}{\exp\left[ \sum_{i}\lambda_{i}f_{i}(x,y) \right] } \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \left[ \sum_{i} \delta_{i} f_{i}(x,y) - \log\left(\frac{Z_{\Lambda+\Delta}}{Z_{\Lambda}} \right) \right] \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) - \log \left(\frac{Z_{\Lambda+\Delta}}{Z_{\Lambda}} \right) \\
  &amp;amp;\geq \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \frac{Z_{\Lambda+\Delta}}{Z_{\Lambda}} \ \ (\because -\log x \geq 1-x) \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \frac{\sum_{x,y}\exp\left[ \sum_{i}(\lambda_{i} + \delta_{i})f_{i}(x,y) \right]}{\sum_{x,y}\exp\left[ \sum_{i}\lambda_{i}f_{i}(x,y) \right]} \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \frac{Z_{\Lambda}\sum_{x,y}P_{\Lambda}(x,y)\exp\left[ \sum_{i}\delta_{i}f_{i}(x,y) \right]}{Z_{\Lambda} \sum_{x,y}P_{\Lambda}(x,y)} \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \sum_{x,y}P_{\Lambda}(x,y)\exp\left[ \sum_{i}\delta_{i}f_{i}(x,y) \right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;素性&lt;span class="math"&gt;\(f_{i}(x,y)\)&lt;/span&gt;の&lt;span class="math"&gt;\(i\)&lt;/span&gt;についての和&lt;span class="math"&gt;\(f^{\#}(x,y) = \sum_{i=1}^{n}f_{i}(x,y)\)&lt;/span&gt;を用いると、&lt;/p&gt;
&lt;div class="math"&gt;$$
  L(P_{\Lambda+\Delta})-L(P_{\Lambda}) = \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \sum_{x,y}P_{\Lambda}(x,y)\exp\left[ \sum_{i}\frac{f_{i}(x,y)}{f^{\#}(x,y)}\delta_{i}f^{\#}(x,y) \right]
$$&lt;/div&gt;
&lt;p&gt;と書ける。今&lt;span class="math"&gt;\(f_{i}(x,y)/f^{\#}(x,y)\)&lt;/span&gt;は確率分布となることから、&lt;span class="math"&gt;\(\sum_{i}\frac{f_{i}(x,y)}{f^{\#}(x,y)}\delta_{i}f^{\#}(x,y)\)&lt;/span&gt;は&lt;span class="math"&gt;\(\delta_{i}f^{\#}(x,y)\)&lt;/span&gt;についての平均と読み取れる。更に&lt;span class="math"&gt;\(\exp\)&lt;/span&gt;は明らかに凸関数であることから、イェンセンの不等式&lt;span class="math"&gt;\(\exp(E[X]) \leq E[\exp(X)]\)&lt;/span&gt;を用いて最終的な下限の式を得る。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  L(P_{\Lambda+\Delta})-L(P_{\Lambda}) &amp;amp;\geq \sum_{x,y} \tilde{P}(x,y) \sum_{i} \delta_{i} f_{i}(x,y) + 1 - \sum_{x,y}P_{\Lambda}(x,y)\sum_{i}\frac{f_{i}(x,y)}{f^{\#}(x,y)}\exp\left[ \delta_{i}f^{\#}(x,y) \right] \\
  &amp;amp;= A(\Lambda, \Delta)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;次に&lt;span class="math"&gt;\(A(\Lambda, \Delta)\)&lt;/span&gt;を&lt;span class="math"&gt;\(\delta_{i}\)&lt;/span&gt;で偏微分することで下限の最大化を考える。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \frac{\partial A(\Lambda, \Delta)}{\partial \delta_{i}} &amp;amp;= \sum_{x,y} \tilde{P}(x,y) f_{i}(x,y) - \sum_{x,y} P_{\Lambda}(x,y) f_{i}(x,y) \exp \left[ \delta_{i}f^{\#}(x,y) \right] \\
  &amp;amp;= E_{\tilde{P}}[f_{i}] - \sum_{x,y} P_{\Lambda}(x,y) f_{i}(x,y) \exp \left[ \delta_{i}f^{\#}(x,y) \right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;この式を&lt;span class="math"&gt;\(0\)&lt;/span&gt;とおき&lt;span class="math"&gt;\(\delta_{i}\)&lt;/span&gt;について解くことで変化量を求める事ができる。この式は&lt;span class="math"&gt;\(\delta_{i}\)&lt;/span&gt;について閉じた形をしていないので、基本的には数値解析によって極値を求める。しかし、もしも任意の&lt;span class="math"&gt;\((x,y)\)&lt;/span&gt;に対し &lt;span class="math"&gt;\(f^{\#}(x,y) = C\)&lt;/span&gt;（定数）となるならば、&lt;span class="math"&gt;\(\delta_{i}\)&lt;/span&gt;について解く事ができ、次の結果を得る。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  &amp;amp; E_{\tilde{P}}[f_{i}] - \sum_{x,y} P_{\Lambda}(x,y) f_{i}(x,y) \exp \left[ \delta_{i}f^{\#}(x,y) \right] = 0 \\
  &amp;amp;\implies \exp \left[C \delta_{i} \right] \sum_{x,y} P_{\Lambda}(x,y) f_{i}(x,y) = E_{\tilde{P}}[f_{i}] \iff \exp \left[C \delta_{i} \right] = \frac{E_{\tilde{P}}[f_{i}]}{E_{P}[f_{i}]} \\
  &amp;amp;\iff \delta_{i} = \frac{1}{C} \log \left( \frac{E_{\tilde{P}}[f_{i}]}{E_{P}[f_{i}]} \right)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;任意の&lt;span class="math"&gt;\((x,y)\)&lt;/span&gt;で&lt;span class="math"&gt;\(f^{\#}(x,y)\)&lt;/span&gt;が定数にならない場合でも、実は&lt;span class="math"&gt;\(C = \displaystyle\max_{x,y} f^{\#}(x,y)\)&lt;/span&gt;とし、新しい素性&lt;span class="math"&gt;\(f_{n+1}(x,y)\)&lt;/span&gt;を&lt;span class="math"&gt;\(f_{n+1}(x,y) = C - f^{\#}(x,y)\)&lt;/span&gt;とおけば、変更後の和&lt;span class="math"&gt;\(f^{\#\prime}(x,y)\)&lt;/span&gt;は&lt;span class="math"&gt;\(f^{\#\prime}(x,y)=C\)&lt;/span&gt;となる事が知られている。&lt;span class="math"&gt;\(f^{\#\prime}(x,y)\)&lt;/span&gt;について検算を行ってみると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  f^{\#\prime}(x,y) &amp;amp;= \sum_{i=1}^{n+1} f_{i}(x,y) = \sum_{i=1}^{n} f_{i}(x,y) + f_{n+1}(x,y)  \\
  &amp;amp;=  f^{\#}(x,y) + C - f^{\#}(x,y) = C
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となって、定数&lt;span class="math"&gt;\(C\)&lt;/span&gt;を取ることが確かめられた。&lt;/p&gt;
&lt;h2&gt;条件付き最大エントロピーモデル&lt;/h2&gt;
&lt;p&gt;前節までのモデルはあるパターン&lt;span class="math"&gt;\((x,y)\)&lt;/span&gt;を生成する結合確率を表現しているが、応用上は何らかの入力&lt;span class="math"&gt;\(x\)&lt;/span&gt;に対して出力&lt;span class="math"&gt;\(y\)&lt;/span&gt;の結果を得たいというケースが多い。例えば、再び単語予測の例を挙げると、一つ前までの単語を&lt;span class="math"&gt;\(x\)&lt;/span&gt;として入力として、今の単語&lt;span class="math"&gt;\(y\)&lt;/span&gt;を予測するというタスクである。そのような場合はモデルの条件付き確率&lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;が用いられる。このモデルは&lt;span class="math"&gt;\(y\)&lt;/span&gt;の識別を行うので生成識別モデルと呼ばれ、条件付き最大エントロピーモデルはCRF(Conditional Random Fields、条件付き確率場)のサブクラスとして捉えられる。&lt;/p&gt;
&lt;p&gt;条件付き最大エントロピーモデルの確率分布&lt;span class="math"&gt;\(P_{\Lambda}(y|x)\)&lt;/span&gt;は、&lt;span class="math"&gt;\(P_{\Lambda}(x,y)\)&lt;/span&gt;とベイズの定理から得られる。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  P_{\Lambda}(y|x) &amp;amp;= \frac{P_{\Lambda}(x,y)}{P_{\Lambda}(x)} \\
  &amp;amp;= \frac{\exp\left[ \sum_{i}\lambda_{i}f_{i}(x,y) \right]}{Z_{\Lambda}} \left( \sum_{y} \frac{\exp\left[ \sum_{i}\lambda_{i}f_{i}(x,y) \right]}{Z_{\Lambda}} \right)^{-1} \\
  &amp;amp;= \frac{1}{Z_{\Lambda}(x)} \exp\left[ \sum_{i}\lambda_{i}f_{i}(x,y) \right] \\
  Z_{\Lambda}(x) &amp;amp;= \sum_{y}\exp\left[\sum_{i}\lambda_{i}f_{i}(x,y)\right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;このモデルを用いた素性の平均&lt;span class="math"&gt;\(E_{P}[f_{i}]\)&lt;/span&gt;は次の様に計算できる。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  E_{P}[f_{i}] &amp;amp;= \sum_{x,y} P(x,y) f_{i}(x,y) = \sum_{x,y} P(y|x)P(x)f_{i}(x,y) \\
  &amp;amp;= \sum_{x} P(x) \sum_{y} P(y|x) f_{i}(x,y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;外側の&lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;の和は、考えうる全ての入力&lt;span class="math"&gt;\(x \in X\)&lt;/span&gt;についての和を取らねばならず、その計算は現実的に不可能である。従って経験確率による近似&lt;span class="math"&gt;\(P(x) \approx \tilde{P}(x)\)&lt;/span&gt;を用いて、平均は&lt;/p&gt;
&lt;div class="math"&gt;$$
  E_{P}[f_{i}] \approx \sum_{x}\tilde{P}(x) \sum_{y} P(y|x) f_{i}(x,y)
$$&lt;/div&gt;
&lt;p&gt;とする。この近似を用いることで、&lt;span class="math"&gt;\(x\)&lt;/span&gt;については学習データに現れるものだけの和を取ればよく、また&lt;span class="math"&gt;\(y\)&lt;/span&gt;についても素性関数が非零の時のみ和を取れば良ため、計算の効率化が望める。&lt;/p&gt;
&lt;p&gt;平均だけでなく、正規化項&lt;span class="math"&gt;\(Z_{\Lambda}(x)\)&lt;/span&gt;の計算もボトルネックな部分であり、効率化が望まれる。そこで、文献&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;による効率的な正規化項の計算手法を見ていく。まず、素性関数の集合&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;を次の2つに分割する:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  {\cal F}_{m} &amp;amp;= \{ f_{i} | \forall{w,x,y} \ f_{i}(x,y) = f_{i}(w,y) \} \ \ \text{（周辺素性(marginalized feature)の集合）} \\
  {\cal F}_{c} = {\cal F}_{m}^{c} &amp;amp;= \{ f_{i} | \exists{w,x,y} \ f_{i}(x,y) \neq f_{i}(w,y) \} \ \ \text{（条件付き素性(conditional feature)の集合）}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;周辺素性は&lt;span class="math"&gt;\(y\)&lt;/span&gt;の値のみによって決まる素性であり、&lt;span class="math"&gt;\(y\)&lt;/span&gt;の関数として捉えられる。集合演算の性質により、&lt;span class="math"&gt;\({\cal F}\_{m} \cap {\cal F}_{c} = \emptyset\)&lt;/span&gt;は自明に成り立つ。次に、&lt;span class="math"&gt;\(y\)&lt;/span&gt;の値域&lt;span class="math"&gt;\(Y\)&lt;/span&gt;についても次の分割を行う:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  Y_{m} &amp;amp;= \{ y | \exists f_{i} \in {\cal F}_{m} \ f_{i}(y) \neq 0 \} \ \ \text{（周辺素性が活性化される$Y$の要素）} \\
  Y(x) &amp;amp;= \{ y | \exists f_{i} \in {\cal F}_{c} \ f_{i}(x,y) \neq 0 \} \ \ \text{（$x$を固定した時に,条件付き素性が活性化される$Y$の要素）}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;定義より&lt;/p&gt;
&lt;div class="math"&gt;$$
Y_{m}^{c} = \{ y | \forall{f_{i}} \in {\cal F}_{m} \ f_{i}(y) = 0 \}
$$&lt;/div&gt;
&lt;p&gt;（どの周辺素性に対しても活性化されない&lt;span class="math"&gt;\(Y\)&lt;/span&gt;の要素）は自明に成り立つ。また、一般には&lt;span class="math"&gt;\(Y_{m} \cap Y(x) \neq \emptyset\)&lt;/span&gt;である。即ち周辺素性と条件付き素性を同時に活性化させる&lt;span class="math"&gt;\(Y\)&lt;/span&gt;の要素は存在する。&lt;/p&gt;
&lt;p&gt;以上の集合分割を考慮しつつ、正規化項&lt;span class="math"&gt;\(Z_{\Lambda}(x) = \sum_{y}\exp\left[\sum_{i}\lambda_{i}f_{i}(x,y)\right]\)&lt;/span&gt;の計算を考えていくが、表記の簡略化の為、文献と同じように次の表記を用いる:&lt;/p&gt;
&lt;div class="math"&gt;$$
  z(y|x) = \exp\left[ \sum_{i}\lambda_{i} f_{i}(x,y) \right] \ ,\ z(y) = \exp\left[ \sum_{f_{i} \in {\cal F}_{m}} \lambda_{i} f_{i}(y) \right]
$$&lt;/div&gt;
&lt;p&gt;正規化項&lt;span class="math"&gt;\(Z_{\Lambda}(x)\)&lt;/span&gt;の計算式は次のように展開される。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  Z_{\Lambda}(x) &amp;amp;= \sum_{y \in Y}z(y|x) \\
  &amp;amp;= \sum_{y \in Y_{m}^{c} \cap Y(x)^{c}} z(y|x) + \sum_{y \in Y_{m} \cap Y(x)^{c}} z(y|x) + \sum_{y \in Y(x)} z(y|x) \\
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  y \in Y(x)^{c} &amp;amp;\implies z(y|x) = z(y) \\
  &amp;amp;\because z(y|x) = \exp\left[ \sum_{f_{i} \in {\cal F}_{m}} \lambda_{i} f_{i}(y) + \sum_{f_{i} \in {\cal F}_{c}} \lambda_{i} 0 \right] = \exp \left[ \sum_{f_{i} \in {\cal F}_{m}} \lambda_{i} f_{i}(y) \right] = z(y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が成立するので、&lt;/p&gt;
&lt;div class="math"&gt;$$
  Z_{\Lambda}(x) =  \sum_{y \in Y_{m}^{c} \cap Y(x)^{c}} z(y) + \sum_{y \in Y_{m} \cap Y(x)^{c}} z(y) +\sum_{y \in Y(x)} z(y|x)
$$&lt;/div&gt;
&lt;p&gt;となり、さらに集合の包含関係に注目すれば、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \sum_{y \in Y} z(y) &amp;amp;= \sum_{y \in Y_{m} \cap Y(x)^{c}} z(y) + \sum_{y \in Y_{m}^{c} \cap Y(x)^{c}} z(y) + \sum_{y \in Y(x)} z(y) \\
  &amp;amp;= \sum_{y \in Y_{m}} z(y) + \sum_{y \in Y_{m}^{c}} z(y) \\
  \therefore \sum_{y \in Y_{m} \cap Y(x)^{c}} z(y) + \sum_{y \in Y_{m}^{c} \cap Y(x)^{c}} z(y) &amp;amp;= \sum_{y \in Y_{m}} z(y) + \sum_{y \in Y_{m}^{c}} z(y) - \sum_{y \in Y(x)} z(y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が成立するので、&lt;/p&gt;
&lt;div class="math"&gt;$$
  Z_{\Lambda}(x) = \sum_{y \in Y_{m}^{c}} z(y) + \sum_{y \in Y_{m}} z(y) + \sum_{y \in Y(x)} \left\{ z(y|x) -z(y) \right\} 
$$&lt;/div&gt;
&lt;p&gt;となり、更に&lt;span class="math"&gt;\(Y_{m}^{c}\)&lt;/span&gt;の要素の性質&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  y \in Y_{m}^{c} &amp;amp;\implies z(y) = 1 \\
  &amp;amp;\because z(y) = \exp\left[ \sum_{f_{i} \in {\cal F}_{m}} \lambda 0 \right] = 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;を用いて、次の最終結果を得る。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  Z_{\Lambda}(x) &amp;amp;= \sum_{y \in Y_{m}^{c}} 1 + \sum_{y \in Y_{m}} z(y) + \sum_{y \in Y(x)} \left\{ z(y|x) -z(y) \right\} \\
  &amp;amp;= |Y-Y_{m}| + \sum_{y \in Y_{m}}z(y) + \sum_{y \in Y(x)} \left\{ z(y|x) -z(y) \right\}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(Y-Y_{m}=Y \cap Y_{m}^{c}\)&lt;/span&gt;は集合演算の意味での差である。この計算式は、第1項と第2項は予め計算しておくことができ、しかも第3項については&lt;span class="math"&gt;\(Y\)&lt;/span&gt;の部分集合&lt;span class="math"&gt;\(Y(x)\)&lt;/span&gt;の和を考えれば良い。結果、ナイーブな計算（計算量&lt;span class="math"&gt;\(O(|X||Y|)\)&lt;/span&gt;）を行うよりも効率的（計算量&lt;span class="math"&gt;\(O(|X||Y(x)|+|X|)\)&lt;/span&gt;）に計算を行うことができる。&lt;/p&gt;
&lt;h2&gt;素性の自動選択&lt;/h2&gt;
&lt;p&gt;前節までは、最大エントロピーモデルの学習について考えてきたが、モデルの構成要素となる素性については触れてなかった。観測された経験確率分布&lt;span class="math"&gt;\(\tilde{P}(x,y)\)&lt;/span&gt;に対し、素性の組み合わせによって実現可能な最大尤度が異なり、従って尤度が最大になる素性集合&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;を選び出さなければならない。&lt;/p&gt;
&lt;p&gt;しかし素性の候補となる集合&lt;span class="math"&gt;\({\cal F}_{0}\)&lt;/span&gt;は非常に大きくなる為に、網羅的に全ての素性の組み合わせを試していくのは現実的に不可能である。また、サンプルで出現頻度が高い素性を選択する手法も存在するが、これでは尤度を厳密に最大化できない。そこで、逐次的にモデルの尤度が増加する様に素性を追加する手法が基本的に用いられており、その手順の概要は以下の様になる。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;モデルの素性集合&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;を空集合とする: &lt;span class="math"&gt;\({\cal F} \leftarrow \emptyset\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;（反復スケーリング法等の学習手法によって）素性集合&lt;span class="math"&gt;\({\cal F}\)&lt;/span&gt;における最大尤度モデル&lt;span class="math"&gt;\(P_{\cal F}\)&lt;/span&gt;を得る。&lt;/li&gt;
&lt;li&gt;素性集合の候補&lt;span class="math"&gt;\({\cal F}\_{0}\)&lt;/span&gt;の各要素&lt;span class="math"&gt;\(f_{0} \in {\cal F}_{0}\)&lt;/span&gt;について、以下を行う。&lt;ol&gt;
&lt;li&gt;素性を加えたモデルを学習し&lt;span class="math"&gt;\(P_{{\cal F} \cup f_{0}}\)&lt;/span&gt;を得る。&lt;/li&gt;
&lt;li&gt;対数尤度の増分&lt;span class="math"&gt;\(\Delta L({\cal F}, f_{0})\)&lt;/span&gt;を計算する: &lt;span class="math"&gt;\(\Delta L({\cal F}, f_{0}) \leftarrow L(P_{{\cal F} \cup f_{0}}) - L(P_{\cal F})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;最大の増分&lt;span class="math"&gt;\(\Delta L({\cal F}, \hat{f})\)&lt;/span&gt;を与える&lt;span class="math"&gt;\(\hat{f} = \underset{f \in {\cal F}_{0}}{\rm argmax}\ \Delta L({\cal F}, f)\)&lt;/span&gt;を選び出し、素性集合に加える: &lt;span class="math"&gt;\({\cal F} \leftarrow {\cal F} \cup \hat{f}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;最大増分がある閾値以下になったら終了し、それ以外は2.に戻る。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;逐次的に計算が行える為に手続き的に実行しやすいものの、結局手順3,4において&lt;span class="math"&gt;\({\cal F}_{0}\)&lt;/span&gt;を走査しているので依然として膨大な計算量が必要になる。文献&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;では対数尤度の増分を近似的に求める手法を述べているが、それでも本質的に計算量を削減できたとは言えず、効率的な素性選択の手法については研究の対象となっていた&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;ここでは元の文献&lt;sup id="fnref4:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref2:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;に述べられていた、増分の近似による手法を見ていく。近似の仮定としては、元のモデル&lt;span class="math"&gt;\(P_{\cal F}\)&lt;/span&gt;とそのパラメタ集合&lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;に&lt;span class="math"&gt;\(f \in {\cal F}\)&lt;/span&gt;とそれに付随するパラメタ&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;を加えたモデル&lt;span class="math"&gt;\(P_{{\cal F} \cup f}\)&lt;/span&gt;においても、最大尤度を与える元のパラメタ&lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;は変化しないというものである。実際には素性を加える事で最大尤度を与えるパラメタ&lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;は変化するが、この変化を無視することでモデル&lt;span class="math"&gt;\(P_{{\cal F} \cup f}\)&lt;/span&gt;の最大尤度&lt;span class="math"&gt;\(L(P_{{\cal F} \cup f})\)&lt;/span&gt;の計算を回避する。素性を追加することにより尤度は増えこそすれ減ることはないので（&lt;span class="math"&gt;\(\because\)&lt;/span&gt;経験分布に適合しない素性に対しては学習の結果&lt;span class="math"&gt;\(\alpha = 0\)&lt;/span&gt;となり、元のモデルと一致するので尤度増分は0） 、近似的増分を最大にする&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;を探索する問題に帰着される。&lt;/p&gt;
&lt;p&gt;仮定の下で、素性集合&lt;span class="math"&gt;\({\cal F} \cup f\)&lt;/span&gt;に対するモデル&lt;span class="math"&gt;\(P_{{\cal F} \cup f}^{\alpha}\)&lt;/span&gt;の確率分布は次の様に書ける。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  P_{ {\cal F} \cup f}^{\alpha}(y|x) &amp;amp;= \frac{1}{Z_{\alpha}(x)} P_{\cal F} (y|x) \exp \left[ \alpha f(x,y) \right] \\
  Z_{\alpha}(x) &amp;amp;= \sum_{y} P_{\cal F}(y|x) \exp \left[ \alpha f(x,y) \right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;対数尤度の近似的増分&lt;span class="math"&gt;\(G_{{\cal F} \cup f}(\alpha)\)&lt;/span&gt;は,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  G_{{\cal F} \cup f}(\alpha) &amp;amp;= L(P_{{\cal F}\cup f}^{\alpha}) - L(P_{\cal F}) \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \log P_{{\cal F} \cup f}^{\alpha}(x,y) - \sum_{x,y} \tilde{P}(x,y) \log P_{\cal F}(x,y) \\
  &amp;amp;= \sum_{x,y} \tilde{P}(x,y) \left\{ \log P_{\cal F}(x,y) + \alpha f(x,y) - \log Z_{\alpha}(x) - \log P_{\cal F}(x,y) \right\} \\
  &amp;amp;= \alpha \sum_{x,y} \tilde{P}(x,y) f(x,y) - \sum_{x} \log Z_{\alpha}(x) \sum_{y} \tilde{P}(x,y) \\
  &amp;amp;= \alpha E_{\tilde{P}}[f] - \sum_{x} \tilde{P}(x) \log Z_{\alpha}(x)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となる、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}(0) = 0\)&lt;/span&gt;は&lt;span class="math"&gt;\(Z_{0}(x) = 1\)&lt;/span&gt;より容易に確かめられる。増分最大化の為、偏微分&lt;span class="math"&gt;\(\frac{\partial G_{{\cal F} \cup f}}{\partial \alpha} = G_{{\cal F} \cup f}^{\prime}(\alpha)\)&lt;/span&gt;を計算すると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  G_{{\cal F} \cup f}^{\prime}(\alpha) &amp;amp;= E_{\tilde{P}}[f] - \sum_{x} P(x) \frac{\partial \log Z_{\alpha}(x)}{\partial \alpha} \\
  &amp;amp;= E_{\tilde{P}}[f] - \sum_{x} \tilde{P}(x) \frac{1}{Z_{\alpha}(x)} \sum_{y} P_{\cal F}(y|x) \exp\left[ \alpha f(x,y) \right] f(x,y) \\
  &amp;amp;= E_{\tilde{P}}[f] - \sum_{x} \tilde{P}(x) \sum_{y} P_{ {\cal F} \cup f}^{\alpha}(y|x) f(x,y)\ \  (= E_{\tilde{P}}[f] - E_{P_{ {\cal F} \cup f}}[f]) \\
  &amp;amp;= E_{\tilde{P}}[f] - \sum_{x} \tilde{P}(x) Q_{ {\cal F} \cup f}^{\alpha} (f|x) 
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで、文献にもあるように&lt;span class="math"&gt;\(Q_{ {\cal F} \cup f}^{\alpha} (h|x) = \sum_{y} P_{ {\cal F} \cup f}^{\alpha}(y|x) h(x,y)\)&lt;/span&gt;（分布&lt;span class="math"&gt;\(P_{ {\cal F} \cup f}\)&lt;/span&gt;による、&lt;span class="math"&gt;\(h\)&lt;/span&gt;の&lt;span class="math"&gt;\(y\)&lt;/span&gt;における平均）とおいている。&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime}(0)\)&lt;/span&gt;の値は&lt;span class="math"&gt;\(P_{ {\cal F} \cup f}^{0}(y|x) = P_{\cal F}(y|x)\)&lt;/span&gt;により&lt;span class="math"&gt;\(G_{{\cal F}\cup f}^{\prime}(0) = E_{\tilde{P}}[f] - E_{P_{\cal F}}[f]\)&lt;/span&gt;となる。更に2階微分&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime \prime}(\alpha)\)&lt;/span&gt;を計算すると,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    G_{{\cal F} \cup f}^{\prime \prime}(\alpha) &amp;amp;= - \sum_{x} P(x) \frac{1}{Z_{\alpha}^{2}(x)} \left[ \left\{ \sum_{y} P_{\cal F}(y|x) \exp\left[ \alpha f(x,y) \right] f^{2}(x,y) \right\} Z_{\alpha}(x) \right. \\
    &amp;amp;  \left. - \left\{ \sum_{y} P_{\cal F}(y|x)\exp\left[ \alpha f(x,y) \right] f(x,y) \right\}^{2} \right] \\
    &amp;amp;= - \sum_{x} \tilde{P}(x) \left[ Q_{ {\cal F} \cup f}^{\alpha} (f^{2}|x) - \left\{Q_{ {\cal F} \cup f}^{\alpha}(f|x) \right\}^{2} \right] \\
    &amp;amp;= - \sum_{x} \tilde{P}(x) Q_{ {\cal F} \cup f}^{\alpha} \left( (f - Q_{ {\cal F} \cup f}^{\alpha}(f|x))^{2} | x \right) 
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで最下段の式変形には、分散と平均の関係&lt;span class="math"&gt;\(E[(X-E[X])^{2}] = E[X^{2}] - \{E[X]\}^{2}\)&lt;/span&gt;を用いている。&lt;span class="math"&gt;\((f - Q_{ {\cal F} \cup f}^{\alpha}(f|x))^{2} \geq 0\)&lt;/span&gt;より、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime \prime}(\alpha) \leq 0\)&lt;/span&gt;が成立し、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}(\alpha)\)&lt;/span&gt;は上に凸な関数であり、極大値がそのまま大域的な最大値となる事が分かる。&lt;/p&gt;
&lt;p&gt;上述の議論により、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime}(\alpha^{\ast}) = 0\)&lt;/span&gt;を満たす&lt;span class="math"&gt;\(\alpha^{\ast}\)&lt;/span&gt;を得れば良いことになるが、解くべき式が&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;について閉じた形をしていない為、数値解析的な手法を用いることになる。文献&lt;sup id="fnref5:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref3:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;によると、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime}(\alpha)\)&lt;/span&gt;は&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;に対して凸関数ではないが、&lt;span class="math"&gt;\(\exp(\alpha)\)&lt;/span&gt;に関しては下に凸の減少関数、かつ&lt;span class="math"&gt;\(\exp(-\alpha)\)&lt;/span&gt;に関しては上に凸の増加関数となる事が示されているので、&lt;span class="math"&gt;\(\exp(\alpha)、\exp(-\alpha)\)&lt;/span&gt;の数列に対してニュートン法を適用する事を考える。偏微分の連鎖律を用いることで,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \frac{\partial G_{{\cal F} \cup f}^{\prime}(\alpha)}{\partial \exp(\alpha)}  &amp;amp;= \frac{\partial G_{{\cal F} \cup f}^{\prime}(\alpha)}{\partial \alpha} \frac{\partial \alpha}{\partial \exp(\alpha)} \\
  &amp;amp;= \frac{\log t}{t} G_{{\cal F} \cup f}^{\prime \prime}(\alpha) \ \ (t = \exp(\alpha)) \\
  &amp;amp;= \frac{1}{t} G_{{\cal F} \cup f}^{\prime \prime}(\alpha) = \exp(-\alpha) G_{{\cal F} \cup f}^{\prime \prime}(\alpha)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が成り立つので、ニュートン法の更新則は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \exp(\alpha_{n+1}) &amp;amp;= \exp(\alpha_{n}) - \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{\frac{\partial G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{\partial \exp(\alpha_{n})}} = \exp(\alpha_{n}) \left[ 1 - \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{G_{{\cal F} \cup f}^{\prime\prime}(\alpha_{n})} \right] \\
  \iff \alpha_{n+1} &amp;amp;= \alpha_{n} + \log \left[ 1 - \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{G_{{\cal F} \cup f}^{\prime\prime}(\alpha_{n})} \right] \\
  \exp(-\alpha_{n+1}) &amp;amp;= \exp(-\alpha_{n}) - \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{\frac{\partial G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{\partial \exp(-\alpha_{n})}} = \exp(-\alpha_{n}) \left[ 1 + \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{G_{{\cal F} \cup f}^{\prime\prime}(\alpha_{n})} \right] \\
  \iff \alpha_{n+1} &amp;amp;= \alpha_{n} - \log \left[ 1 + \frac{G_{{\cal F} \cup f}^{\prime}(\alpha_{n})}{G_{{\cal F} \cup f}^{\prime\prime}(\alpha_{n})} \right]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となる。最適値&lt;span class="math"&gt;\(\alpha^{\ast}\)&lt;/span&gt;が&lt;span class="math"&gt;\(\alpha^{\ast} &amp;gt; 0\)&lt;/span&gt;の場合（&lt;span class="math"&gt;\(E_{\tilde{P}}[f] &amp;gt; E_{P_{\cal F}}[f]\)&lt;/span&gt;&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt;）には上の更新式を用いれば良く、&lt;span class="math"&gt;\(\alpha^{\ast} &amp;lt; 0\)&lt;/span&gt;の場合（&lt;span class="math"&gt;\(E_{\tilde{P}}[f] &amp;lt; E_{P_{\cal F}}[f]\)&lt;/span&gt;）には下の更新式を用いれば良い。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当機立断！&lt;/strong&gt; &lt;em&gt;こんな記事見てる暇があるなら今すぐ幕張に行けってことさ！&lt;/em&gt; ユ”メ”ッ！！
おう、ライブ行くんだよ、あくしろよ。フォロチケ返さねえぞこの野郎。&lt;/p&gt;
&lt;h2&gt;脚注・参考文献&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;北研二、辻井潤一、``確率的言語モデル''、東京大学出版会、1999&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref5:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;高村大也、奥村学、``言語処理のための機械学習入門''、コロナ社、2010&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;高橋治久、堀田一弘、``学習理論'' コロナ社、2009&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Berger, Adam L., Vincent J, Della Pietra, and Stephen A. Della Pietra. "A maximum entropy approach to natural language processing." Computational linguistics 22.1 (1996): 39-71.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Wu, Jun, and Sanjeev Khudanpur, "Efficient training methods for maximum entropy language modeling." INTERSPEECH. 2000.&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Zhou, Yaqian, et al. "A fast algorithm for feature selection in conditional maximum entropy modeling." Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics, 2003.&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;Pietra, Stephen Della, Vincent Della Pietra, and John Lafferty. "Inducing features of random fields." Pattern Analysis and Machine Intelligence、IEEE Transactions on 19.4 (1997): 380-393.&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;谷垣宏一, 渡邉圭輔, and 石川泰, ``最大エントロピー法による発話理解のための効率的モデル構築 (&amp;lt; 特集&amp;gt; 音声言語情報処理とその応用).'' 情報処理学会論文誌 43.7 (2002): 2138-2146.&amp;#160;&lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime}(\alpha)\)&lt;/span&gt;は&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;に関して単調減少するので、&lt;span class="math"&gt;\(G_{{\cal F} \cup f}^{\prime}(0) = E_{\tilde{P}}[f] - E_{P_{\cal F}}[f]&amp;gt;0\)&lt;/span&gt;ならば、かつその時に限り最適値&lt;span class="math"&gt;\(\alpha^{\ast}\)&lt;/span&gt;は正の値をとる。&amp;#160;&lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="機械学習"></category></entry><entry><title>SVM（サポートベクトルマシン）</title><link href="/svmsapotobekutorumashin.html" rel="alternate"></link><published>2020-04-23T12:30:00+09:00</published><updated>2020-04-23T12:30:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/svmsapotobekutorumashin.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;SVM(Support Vector Machine, サポートベクトルマシン)&lt;/strong&gt;は、深層学習の影に隠れがちではあるものの、現在使われている識別学習モデルの中でも比較的認識性能が優れ、実用に供される事はもちろん、様々な研究でも比較対象となる手法の一つである。&lt;/p&gt;
&lt;p&gt;SVMの大雑把な理論的概要を述べると、SVMは与えられた学習サンプルを最も適切に分離（識別）する境界面（&lt;strong&gt;識別面&lt;/strong&gt;）を発見する手法である。その識別面は凸計画問題に帰着して求める事ができるので、どの様なサンプルにおいても（存在するならば）最適な識別面を構成できる。 &lt;/p&gt;
&lt;p&gt;本稿では、最初に基本となる線形SVMの定式化を行い、次に汎用性をより高めた非線形SVMとソフトマージンSVMを説明し、最後にSVMを回帰問題に適用したSVR(Support Vector Regression, サポートベクトル回帰)を説明する。最後にC言語による実装例を挙げる。&lt;/p&gt;
&lt;p&gt;SVMも知り尽くされており、文献・資料は大量に存在する。ここでは、参考書籍&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;を挙げる。&lt;/p&gt;
&lt;h2&gt;線形SVM&lt;/h2&gt;
&lt;h3&gt;マージンの定式化&lt;/h3&gt;
&lt;p&gt;識別の例として、まずは図にあるような、2次元空間&lt;span class="math"&gt;\(X\times Z\)&lt;/span&gt;に存在する2クラスのサンプルデータ（以下サンプル）を仮定する。各クラスは二値のラベル付け …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;SVM(Support Vector Machine, サポートベクトルマシン)&lt;/strong&gt;は、深層学習の影に隠れがちではあるものの、現在使われている識別学習モデルの中でも比較的認識性能が優れ、実用に供される事はもちろん、様々な研究でも比較対象となる手法の一つである。&lt;/p&gt;
&lt;p&gt;SVMの大雑把な理論的概要を述べると、SVMは与えられた学習サンプルを最も適切に分離（識別）する境界面（&lt;strong&gt;識別面&lt;/strong&gt;）を発見する手法である。その識別面は凸計画問題に帰着して求める事ができるので、どの様なサンプルにおいても（存在するならば）最適な識別面を構成できる。 &lt;/p&gt;
&lt;p&gt;本稿では、最初に基本となる線形SVMの定式化を行い、次に汎用性をより高めた非線形SVMとソフトマージンSVMを説明し、最後にSVMを回帰問題に適用したSVR(Support Vector Regression, サポートベクトル回帰)を説明する。最後にC言語による実装例を挙げる。&lt;/p&gt;
&lt;p&gt;SVMも知り尽くされており、文献・資料は大量に存在する。ここでは、参考書籍&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;を挙げる。&lt;/p&gt;
&lt;h2&gt;線形SVM&lt;/h2&gt;
&lt;h3&gt;マージンの定式化&lt;/h3&gt;
&lt;p&gt;識別の例として、まずは図にあるような、2次元空間&lt;span class="math"&gt;\(X\times Z\)&lt;/span&gt;に存在する2クラスのサンプルデータ（以下サンプル）を仮定する。各クラスは二値のラベル付け&lt;span class="math"&gt;\(y=\{-1, 1\}\)&lt;/span&gt;がなされており、識別面（2次元空間では直線）&lt;span class="math"&gt;\(ax+bz+c=0\)&lt;/span&gt;の上半領域（&lt;span class="math"&gt;\(ax+bz+c&amp;gt;0\)&lt;/span&gt;）にラベル&lt;span class="math"&gt;\(y=1\)&lt;/span&gt;のサンプルが、下半領域（&lt;span class="math"&gt;\(ax+bz+c&amp;lt;0\)&lt;/span&gt;）にラベル&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;のサンプルが分布するようにする。&lt;/p&gt;
&lt;p&gt;&lt;img alt="2クラス分離の例" src="./images/2class_separation.png"&gt;&lt;/p&gt;
&lt;p&gt;更に、&lt;span class="math"&gt;\(n\)&lt;/span&gt;次元空間の元（ベクトル）&lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^{n}\)&lt;/span&gt;で表されるサンプルに対しても一般化でき、&lt;span class="math"&gt;\(n\)&lt;/span&gt;次元の係数ベクトル&lt;span class="math"&gt;\(\boldsymbol{w} \in \mathbb{R}^{n}\)&lt;/span&gt;を用いることで、 識別面は&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \boldsymbol{w}^{\mathsf{T}}\boldsymbol{x} + b = 0
 \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;と表現できる。ここで&lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;は切片（しきい値、 バイアス）である。&lt;/p&gt;
&lt;p&gt;概要でも述べたとおり、識別面は異なるラベルが付いたサンプルを互いに分離さえできていれば良いので、識別面の候補は無限に存在してしまう（上の2次元の例でも明らかである）。しかし、 その全てが適切な識別面とは限らない。 SVMでは、次の2点を最適な識別面の条件とする。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;各クラスの、最も識別面に近いサンプル（&lt;strong&gt;サポートベクトル&lt;/strong&gt;）までの距離を最大にする。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;また、 その距離を各クラスで同一にする。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この2点を満たす識別面ならば、 丁度クラス間の中心を区切ることが出来、適切な識別面といえる。 また、図に示す様に、サポートベクトル間の距離を&lt;strong&gt;マージン&lt;/strong&gt;（余白）という。 SVMは、このマージンを最大化することが目的となる。&lt;/p&gt;
&lt;p&gt;&lt;img alt="マージン" src="./images/margin.png"&gt;&lt;/p&gt;
&lt;p&gt;それでは、 マージンの定式化を考える。&lt;span class="math"&gt;\(n\)&lt;/span&gt;次元空間上に&lt;span class="math"&gt;\(N\)&lt;/span&gt;個存在するサンプルを&lt;span class="math"&gt;\(\boldsymbol{x}\_{i} \in \mathbb{R}^{n} \ (i=1, \dots, N)\)&lt;/span&gt;と書き、またそのデータに対応する二値ラベルを&lt;span class="math"&gt;\(y_{i} \in \{-1, 1\}\ (i=1, \dots, N)\)&lt;/span&gt;とかく。全てのサンプルが正しく識別されている時には、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \geq 0 \quad (i = 1, \dots, N)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;が明らかに成立する。 そして、異なる2クラスのサポートベクトル&lt;span class="math"&gt;\(\boldsymbol{x}\_{s}, \boldsymbol{x}\_{t}\)&lt;/span&gt;が&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{s} + b = l , \quad \boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{t} + b = -l\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;が成立すると仮定する&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;と（&lt;span class="math"&gt;\(l&amp;gt;0\)&lt;/span&gt;）、 &lt;span class="math"&gt;\(\boldsymbol{x}\_{s}\)&lt;/span&gt;と&lt;span class="math"&gt;\(\boldsymbol{x}\_{t}\)&lt;/span&gt;の、識別面に対して平行な距離がマージンとして計算できる。マージンを&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;と書くと、 平面の単位法ベクトルは&lt;span class="math"&gt;\(\boldsymbol{w}/||\boldsymbol{w}||\)&lt;/span&gt;（&lt;span class="math"&gt;\(||\boldsymbol{w}|| = \sqrt{\boldsymbol{w}^{\mathsf{T}}\boldsymbol{w}}\)&lt;/span&gt;）で与えられるので、マージンは &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp;  \boldsymbol{w}^{\mathsf{T}}(\boldsymbol{x}_{t} + \gamma \frac{\boldsymbol{w}}{||\boldsymbol{w}||}) + b = \boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{s} + b \nonumber \\
  &amp;amp;\iff \gamma \frac{\boldsymbol{w}^{\mathsf{T}}\boldsymbol{w}}{||\boldsymbol{w}||} = \gamma ||\boldsymbol{w}|| = (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{s}+b) - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{t}+b) = 2l \nonumber \\
  &amp;amp;\therefore \gamma = \frac{2l}{||\boldsymbol{w}||}\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;で求められる。&lt;/p&gt;
&lt;h3&gt;マージン最大化&lt;/h3&gt;
&lt;p&gt;前節でも既に述べたが、 SVMの目的はマージン&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;を最大化することである。単純には&lt;span class="math"&gt;\(\max \gamma\)&lt;/span&gt;と書けるが、 最適化を行いやすくするため、&lt;span class="math"&gt;\(1/\gamma\)&lt;/span&gt;の最小化に置き換え、 &lt;span class="math"&gt;\(l\)&lt;/span&gt;は最適化に関与しないので&lt;span class="math"&gt;\(l=1\)&lt;/span&gt;とし、更に&lt;span class="math"&gt;\(||\boldsymbol{w}||\)&lt;/span&gt;が最小化された時は&lt;span class="math"&gt;\(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{w}\)&lt;/span&gt;も最小化されるので、考えるべき最適化問題は次のように書ける: &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp; \max_{\scriptsize \boldsymbol{w}} \gamma = \frac{2l}{||\boldsymbol{w}||} \quad \text{subject to : } y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \geq l \nonumber \\
  &amp;amp;\implies \min_{\scriptsize \boldsymbol{w}} \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} \quad \text{subject to : } y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \geq 1 \quad (i=1, \dots, N)\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;この最適化問題は、 凸計画問題 &lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt; であり、不等式制約付き非線形計画問題なので、 KKT条件(Karush-Kuhn-Tucker condition)を用いる。KKT条件はラグランジェの未定乗数法（等式制約）の一般化であり、次の定理で表される:&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;KKT条件&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{v}^{\star}\)&lt;/span&gt;を&lt;span class="math"&gt;\(f(\boldsymbol{v})\)&lt;/span&gt;に関しての最適化問題の最適解とするならば、次の条件を満たす最適重みベクトル&lt;span class="math"&gt;\(\boldsymbol{\alpha}^{\star}=[\alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star}]^{\mathsf{T}}\ (\alpha_{i} \geq 0)\)&lt;/span&gt;が存在する。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
      \left\{
        \begin{array}{ll}
          \displaystyle\frac{\partial{\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star})}{\partial \boldsymbol{v}} &amp;amp;=  0 \\
          \boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}^{\star}) &amp;amp;= 0
        \end{array}
        \right.
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(\boldsymbol{g}(\boldsymbol{v})\)&lt;/span&gt;は制約条件式&lt;span class="math"&gt;\(\boldsymbol{g}(\boldsymbol{v}) = [g_{1}(\boldsymbol{v}), \cdots, g_{N}(\boldsymbol{v})]^{\mathsf{T}}\)&lt;/span&gt;, &lt;span class="math"&gt;\({\cal L}\)&lt;/span&gt;はラグランジアン（ラグランジェ関数）であり以下の様に表される。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
        g_{i}(\boldsymbol{v}) &amp;amp;\geq 0\ \ (i = 1, \dots, N) \\
        {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}) &amp;amp;= f(\boldsymbol{v}) + \boldsymbol{\alpha}^{\mathsf{T}}\boldsymbol{g}(\boldsymbol{v})
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\({\cal L}\)&lt;/span&gt;が凸関数ならば、最適点&lt;span class="math"&gt;\((\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;は鞍点にあり、&lt;span class="math"&gt;\(\displaystyle\max_{\scriptsize \boldsymbol{v}} {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;を主問題とする時、&lt;span class="math"&gt;\(\displaystyle\min_{\scriptsize \boldsymbol{\alpha}}{\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha})\)&lt;/span&gt;を主問題に対する双対問題という。&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;それでは実際にKKT条件を適用し、最適化問題を主問題(式)から双対問題へ変換する事を考える。 まず制約条件から&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  g_{i}(\boldsymbol{w}) = 1 - y_{i} (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \leq 0\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;（最小化を考えているので、 符号が逆転している事に注意）より、ラグランジアンは、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  {\cal L}(\boldsymbol{w}, \boldsymbol{\alpha}) &amp;amp;=  \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + \boldsymbol{\alpha}^{\mathsf{T}}\boldsymbol{g}(\boldsymbol{w})  \\
  &amp;amp;= \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + \sum_{i=1}^{N}\alpha_{i} \{ 1 - y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \}\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;と表現でき、 &lt;span class="math"&gt;\({\cal L}(\boldsymbol{w},  \boldsymbol{\alpha})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\boldsymbol{w}, b\)&lt;/span&gt;による偏微分は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \frac{\partial {\cal L}(\boldsymbol{w}, \boldsymbol{\alpha})}{\partial \boldsymbol{w}} = \boldsymbol{w} - \sum_{i=1}^{N} \alpha_{i}y_{i}\boldsymbol{x}_{i}, \quad \frac{\partial {\cal L}(\boldsymbol{w}, \boldsymbol{\alpha})}{\partial b} = \sum_{i=1}^{N}\alpha_{i}y_{i}\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;となる。&lt;span class="math"&gt;\(\displaystyle\frac{\partial {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\alpha})}{\partial \boldsymbol{w}} = \boldsymbol{0}\)&lt;/span&gt;とおくことで最適時の係数&lt;span class="math"&gt;\(\boldsymbol{w}^{\star}\)&lt;/span&gt;が求まる:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \boldsymbol{w}^{\star} = \sum_{i=1}^{N} \alpha_{i}y_{i}\boldsymbol{x}_{i}\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;また、&lt;span class="math"&gt;\(\displaystyle\frac{\partial {\cal L}(\boldsymbol{w}^{\star},  \boldsymbol{\alpha})}{\partial b} = 0\)&lt;/span&gt;により双対変数&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;の制約条件が得られる:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \sum_{i=1}^{N} \alpha_{i}y_{i} = 0\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;これらの関係式をラグランジアンに代入することで、 双対問題式を得る:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  {\cal L}(\boldsymbol{w}^{\star}、 \boldsymbol{\alpha}) &amp;amp;= \frac{1}{2} \boldsymbol{w}^{\star\mathsf{T}}\boldsymbol{w}^{\star} + \boldsymbol{\alpha}^{\mathsf{T}} \boldsymbol{g}(\boldsymbol{w}^{\star}) \\
  &amp;amp;= \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{j}^{\mathsf{T}}\boldsymbol{x}_{i} + \sum_{i=1}^{N} \alpha_{i} \left\{ 1 - y_{i} \left( \sum_{j=1}^{N}\alpha_{j}y_{j}\boldsymbol{x}_{j}^{\mathsf{T}} \boldsymbol{x}_{i} + b \right) \right\} \\
  &amp;amp;= \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{\mathsf{T}}\boldsymbol{x}_{j} + \sum_{i=1}^{N} \alpha_{i} - b\sum_{i=1}^{N} \alpha_{i} y_{i} - \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{\mathsf{T}} \boldsymbol{x}_{j} \\
  &amp;amp;= \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{\mathsf{T}}\boldsymbol{x}_{j}\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;（途中の式変形において、内積の対称性（&lt;span class="math"&gt;\(\boldsymbol{x}^{\mathsf{T}}\_{j}\boldsymbol{x}\_{i} = \boldsymbol{x}^{\mathsf{T}}\_{i}\boldsymbol{x}\_{j}\)&lt;/span&gt;）を用いている。）よって、 双対問題は &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp; \max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{\mathsf{T}}\boldsymbol{x}_{j} \right] \\
  &amp;amp; \text{subject to : } \alpha_{i} \geq 0, \ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1, \dots, N) \nonumber\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;と表現できる。双対問題は非負制約&lt;span class="math"&gt;\(\alpha_{i} \geq 0\)&lt;/span&gt;の中で&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;を動かし、その最大値を得れば良いので、 主問題を直接解くよりも容易に、数値最適化によって解を求める（学習する）ことができる。実際の実装については後に述べる。&lt;/p&gt;
&lt;h2&gt;非線形SVM&lt;/h2&gt;
&lt;p&gt;前節までの議論は、入力データと同じ空間（次元）で適切な識別面を発見するSVMであり、これを特に&lt;strong&gt;線形SVM&lt;/strong&gt;という。 線形SVMの場合、識別面は入力データ空間の次元&lt;span class="math"&gt;\(n\)&lt;/span&gt;に対し&lt;span class="math"&gt;\(n-1\)&lt;/span&gt;次元の平面（&lt;strong&gt;超平面&lt;/strong&gt;）であり（例:2次元空間では直線、3次元空間では平面）、図の様に、異なるクラスのサンプルが入り組んだ状態では識別面を構成できない（&lt;strong&gt;線形分離不可能&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="線形分離不可能な例" src="./images/linearly_unseparatable.png"&gt;&lt;/p&gt;
&lt;p&gt;この場合、入力データの空間&lt;span class="math"&gt;\(\mathbb{R}^{n}\)&lt;/span&gt;から高次元空間&lt;span class="math"&gt;\(\mathbb{R}^{h}\)&lt;/span&gt;(&lt;span class="math"&gt;\(h \gg n\)&lt;/span&gt;)への高次元な非線形写像（&lt;strong&gt;特徴写像&lt;/strong&gt;）&lt;span class="math"&gt;\(\boldsymbol{\phi} : \mathbb{R}^{n} \to \mathbb{R}^{h}\)&lt;/span&gt;を用いて高次元空間（特徴空間）へ写像すれば、線形分離不可能だったサンプルを一般位置&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;に写し、識別面を構成できる（線形分離可能）ようになる（図参照）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="特徴空間で線形分離可能になる例" src="./images/linearly_separatable_in_high_dimension.png"&gt;&lt;/p&gt;
&lt;p&gt;図では、入力空間は1次元（数直線）、 特徴空間は2次元（平面）である。入力空間で線形分離不可能なサンプルが、 特徴写像によって一般位置に写され、線形分離可能になっている。&lt;/p&gt;
&lt;p&gt;この様に、 入力データ次元で線形分離不可能なサンプルを、特徴写像によって写して識別面を構成し、元の次元に戻すSVMを&lt;strong&gt;非線形SVM&lt;/strong&gt;という。 この場合、識別面は曲がった形状を持つ（超曲面）。&lt;/p&gt;
&lt;p&gt;それでは非線形SVMの定式化を見ていく。特徴写像を用いてサンプルを写像することで、高次元空間内のサンプル（特徴サンプル）&lt;span class="math"&gt;\(\boldsymbol{\phi}(\boldsymbol{x}_{i})\ (i =1, \dots, N)\)&lt;/span&gt;が得られる。後は線形SVMの時と全く同様の議論を適用し、 双対問題は次の様に表現される:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{\phi}(\boldsymbol{x}_{i})^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{j}) \right] \\
  &amp;amp;\text{subject to : } \alpha_{i} \geq 0,\ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1, \dots, N) \nonumber\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;さて、 この様にして非線形SVMが実現できるが、 一般に、入力次元&lt;span class="math"&gt;\(n\)&lt;/span&gt;はもとより特徴空間の次元&lt;span class="math"&gt;\(h\)&lt;/span&gt;は非常に大きくなる（&lt;span class="math"&gt;\(\infty\)&lt;/span&gt;次元にすらなりうる）。特徴写像&lt;span class="math"&gt;\(\boldsymbol{\phi}\)&lt;/span&gt;を構成する&lt;span class="math"&gt;\(h\)&lt;/span&gt;個の非線形な基底を用意するのは、非常に困難であり、 実用上大変な不便が生じる。 そこで、特徴写像同士の内積&lt;span class="math"&gt;\(\boldsymbol{\phi}(\boldsymbol{x}\_{i})^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}\_{j})\)&lt;/span&gt;の計算結果はノルムなので、その内積を計算するのではなく、 天下り的に、最初から内積値を与えてしまうやり方がある。 即ち、 特徴写像同士の内積値を、 &lt;strong&gt;カーネル関数&lt;/strong&gt; &lt;span class="math"&gt;\(K : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}\)&lt;/span&gt;で定める:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \langle \boldsymbol{\phi}(\boldsymbol{x}_{i}), \boldsymbol{\phi}(\boldsymbol{x}_{j}) \rangle = \boldsymbol{\phi}(\boldsymbol{x}_{i})^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{j})\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(K\)&lt;/span&gt;は入力データのみで記述されるので、特徴写像はカーネル関数の中に閉じ込められてしまい、 陽に現れない。 即ち、特徴写像を構成する必要がないというのが大きなメリットである。任意の関数がカーネルになるとは限らず、 マーサーの定理 &lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;という条件をカーネル関数は満たす必要がある。代表的なカーネル関数を以下に挙げる:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;線形カーネル:   &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \langle \boldsymbol{x}_{i}, \boldsymbol{x}_{j} \rangle
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;入力次元における標準内積もカーネルとなり、 線形カーネルと呼ばれる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ガウシアン（Radial Basis Function、 RBF:放射基底関数）カーネル:   &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \exp\left(-\frac{||\boldsymbol{x}_{i}-\boldsymbol{x}_{j}||^{2}}{2\sigma^{2}}\right)
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;分散パラメタ&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;を伴ってガウス関数に従った分布を示す。実用上よく用いられる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多項式カーネル:   &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = (\langle \boldsymbol{x}_{i}, \boldsymbol{x}_{j} \rangle + c)^{k}
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;正定数&lt;span class="math"&gt;\(c\)&lt;/span&gt;と多項式の次数&lt;span class="math"&gt;\(k\)&lt;/span&gt;によって構成されるカーネルである。ガウシアンカーネルよりも性能がパラメタに依存しない特徴を持つ。&lt;/p&gt;
&lt;p&gt;カーネル関数&lt;span class="math"&gt;\(K\)&lt;/span&gt;を用いる事で、 非線形SVMの双対問題は次で表される:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}、 \boldsymbol{x}_{j}) \right] \\
  &amp;amp;\text{subject to : } \alpha_{i} \geq 0、\ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1, \dots, N) \nonumber\end{aligned}
$$&lt;/div&gt;
&lt;h2&gt;ソフトマージンSVM&lt;/h2&gt;
&lt;p&gt;前節までのSVMは、 マージンの内部にサンプルが入る事を一切許さないので、これを特に&lt;strong&gt;ハードマージンSVM&lt;/strong&gt;ということがある。カーネルを用いた非線形ハードマージンSVMは、線形分離不可能なサンプルにでも強引に曲がりくねった識別面を構成する。これは実用に供する場合に問題になることがある。 例えば、データに雑音が乗っていたり、 一部のラベルを付け間違えたりする場合であり、これらは実データを扱う場合、 往々にして起こりうる事である。この様な雑音を拾いすぎてしまうとSVMの汎化性能&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt;が悪化してしまうので、マージンの制約を緩め、一部のサンプルはマージンの内部に入っても良いようにSVMを改善する事を考える。マージンの内部にサンプルが入ることを許すSVMを&lt;strong&gt;ソフトマージンSVM&lt;/strong&gt;と呼ぶことがある。&lt;/p&gt;
&lt;p&gt;ハードマージンSVMの制約を緩める事を考える。サンプル&lt;span class="math"&gt;\(\boldsymbol{x}\_{i}\)&lt;/span&gt;に対応するスラック（緩衝）変数&lt;span class="math"&gt;\(\eta_{i} \geq 0\ (i=1, \dots, N)\)&lt;/span&gt;を用意して、 SVMの制約を &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}\_{i}) + b) \geq 1 - \eta_{i} \quad (i = 1, \dots, N)\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;とする（最初から、サンプルは特徴写像&lt;span class="math"&gt;\(\boldsymbol{\phi}\)&lt;/span&gt;によって写像されている場合を考える）。スラック変数はサンプルがマージンに食い込んでいる距離を表しており、もちろん、 &lt;span class="math"&gt;\(\eta_{i}\)&lt;/span&gt;は小さい方が良く、&lt;span class="math"&gt;\(\eta_{i} = 0\)&lt;/span&gt;の時はハードマージンに一致する。 そして、&lt;span class="math"&gt;\(\eta_{i}\)&lt;/span&gt;も同時に最適化に組み込んでしまう事で、ソフトマージンSVMが実現できる。 多くの文献では、スラック変数のノルムの取り方で異なる2種類のソフトマージンSVMの式を提示している:&lt;/p&gt;
&lt;h5&gt;1ノルムソフトマージンSVM・主問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp; \min_{\scriptsize \boldsymbol{w}} \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + C_{1}\sum_{i=1}^{N} \eta_{i} \\ &amp;amp; \text{subject to : } y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) \geq 1 - \eta_{i}, \ \eta_{i} \geq 0 \quad (i=1, \dots, N)
        \end{aligned}
$$&lt;/div&gt;
&lt;h5&gt;2ノルムソフトマージンSVM・主問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp;\min_{\scriptsize \boldsymbol{w}} \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + \frac{C_{2}}{2}\sum_{i=1}^{N} \eta_{i}^{2} \quad \\ &amp;amp;\text{subject to : } y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) \geq 1 - \eta_{i} \quad (i=1, \dots, N)
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ここで、 &lt;span class="math"&gt;\(C_{1},  C_{2}\)&lt;/span&gt;はハードマージンとソフトマージンのトレードオフを与える定数&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt;で、最適な値は実験等によって求める必要がある。 双対問題の導出は、前節までの議論と同様に、 KKT条件に当てはめる事により得られる:&lt;/p&gt;
&lt;h4&gt;1ノルムソフトマージンSVM・双対問題の導出&lt;/h4&gt;
&lt;p&gt;ラグランジアンは、 &lt;span class="math"&gt;\(\beta_{i} \geq 0\)&lt;/span&gt;なる双対変数を導入して、&lt;span class="math"&gt;\(-\beta_{i}\eta_{i} \leq 0\)&lt;/span&gt;より、 &lt;/p&gt;
&lt;div class="math"&gt;$$
    \begin{aligned}
          {\cal L}(\boldsymbol{w}, \boldsymbol{\eta}, \boldsymbol{\alpha}, \boldsymbol{\beta}) = \frac{1}{2} \boldsymbol{w}^{\mathsf{T}} \boldsymbol{w} + C_{1} \sum_{i=1}^{N}\eta_{i} + \sum_{i=1}^{N} \alpha_{i} \left\{ 1 - \eta_{i} - y_{i}(\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) \right\} + \sum_{i=1}^{N}(-\beta_{i}\eta_{i})
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;より、&lt;span class="math"&gt;\({\cal L}(\boldsymbol{w}, \boldsymbol{\eta},  \boldsymbol{\alpha},  \boldsymbol{\beta})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\boldsymbol{w}, b, \eta_{i},\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}}, \frac{\partial \cal L}{\partial b}, \frac{\partial \cal L}{\partial \eta_{i}}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \begin{aligned}
          \frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{w} - \sum_{i=1}^{N} y_{i} \alpha_{i} \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \frac{\partial \cal L}{\partial b} = \sum_{i=1}^{N}\alpha_{i}y_{i}, \quad \frac{\partial \cal L}{\partial \eta_{i}} = C_{1} - \alpha_{i} - \beta_{i}
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{0}, \frac{\partial \cal L}{\partial \eta_{i}} = 0\)&lt;/span&gt;とおくことで、最適時パラメタは、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \boldsymbol{w}^{\star} = \sum_{i=1}^{N} y_{i} \alpha_{i} \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad C_{1} = \alpha_{i} + \beta_{i}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w}^{\star}\)&lt;/span&gt;をラグランジアンに代入すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{\star}、 \boldsymbol{\alpha}、 \boldsymbol{\beta}) &amp;amp;= \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \sum_{i=1}^{N} (C_{1} - \alpha_{i} - \beta_{i}) \eta_{i} \\
          &amp;amp;= \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j})
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;制約条件&lt;span class="math"&gt;\(\alpha_{i}, \beta_{i} \geq 0\)&lt;/span&gt;を含めて考えると、&lt;span class="math"&gt;\(\beta_{i} = C_{1} - \alpha_{i} \geq 0\)&lt;/span&gt;より、&lt;span class="math"&gt;\(\alpha_{i}\)&lt;/span&gt;についての制約&lt;span class="math"&gt;\(0 \leq \alpha_{i} \leq C_{1}\)&lt;/span&gt;が得られ、結局、普通のSVMの双対問題に&lt;span class="math"&gt;\(\alpha_{i}\)&lt;/span&gt;についての制約を加えるだけで、1ノルムソフトマージンSVMが実現できる。&lt;/p&gt;
&lt;h5&gt;1ノルムソフトマージンSVM・双対問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
      &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \right] \\
      &amp;amp;\text{subject to : } 0 \leq \alpha_{i} \leq C_{1}, \ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1, \dots, N) \nonumber\end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;2ノルムソフトマージンSVM・双対問題の導出&lt;/h4&gt;
&lt;p&gt;ラグランジアンは、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          {\cal L}(\boldsymbol{w}, \boldsymbol{\eta}, \boldsymbol{\alpha}) = \frac{1}{2} \boldsymbol{w}^{\mathsf{T}} \boldsymbol{w} + \frac{C_{2}}{2} \sum_{i=1}^{N}\eta_{i}^{2} + \sum_{i=1}^{N} \alpha_{i} \left\{ 1 - \eta_{i} - y_{i}(\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) \right\}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;より、&lt;span class="math"&gt;\({\cal L}(\boldsymbol{w}, \boldsymbol{\eta}, \boldsymbol{\alpha})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\boldsymbol{w}, b, \eta_{i}\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}}, \frac{\partial \cal L}{\partial b}, \frac{\partial \cal L}{\partial \eta_{i}}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{w} - \sum_{i=1}^{N} y_{i} \alpha_{i} \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \frac{\partial \cal L}{\partial b} = \sum_{i=1}^{N}\alpha_{i}y_{i}, \quad \frac{\partial \cal L}{\partial \eta_{i}} = C_{2}\eta_{i} - \alpha_{i}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{0}, \frac{\partial \cal L}{\partial \eta_{i}} = 0\)&lt;/span&gt;とおくことで、最適時パラメタは、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \boldsymbol{w}^{\star} = \sum_{i=1}^{N} y_{i} \alpha_{i} \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \eta_{i}^{\star} = \frac{\alpha_{i}}{C_{2}}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
これをラグランジアンに代入すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{\star}, \boldsymbol{\alpha}) &amp;amp;= \sum_{i=1}^{N} \alpha_{i} + \frac{1}{2C_{2}} \sum_{i=1}^{N} \alpha_{i}^{2} + \sum_{i=1}^{N} \alpha_{i} \left[ - \frac{\alpha_{i}}{C_{2}} - y_{i} \sum_{j=1}^{N} y_{j}\alpha_{j} K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \right] + \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \\
          &amp;amp;= \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2C_{2}} \sum_{i=1}^{N} \alpha_{i}^{2} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j})
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(y_{i}y_{j} \in \{-1, 1\}\)&lt;/span&gt;に注目すれば、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{\star}, \boldsymbol{\alpha}) = \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\left( K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \frac{1}{C_{2}}\delta_{ij} \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;と整理できる。ここで&lt;span class="math"&gt;\(\delta_{ij}\)&lt;/span&gt;はディラックのデルタであり、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \delta_{ij} = 
          \left\{
            \begin{array}{ll}
              1 &amp;amp; i = j \\
              0 &amp;amp; otherwise
            \end{array}
            \right.
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;を満たす。 2ノルムソフトマージンSVMも、 結局、カーネル関数を簡単に書き換える事で実現できる。&lt;/p&gt;
&lt;h5&gt;2ノルムソフトマージンSVM・双対問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
            &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\left(K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \frac{1}{C_{2}}\delta_{ij} \right) \right] \\
            &amp;amp;\text{subject to : } \alpha_{i} \geq 0, \ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1,\dots,N) \nonumber
\end{aligned}
$$&lt;/div&gt;
&lt;h2&gt;SVR&lt;/h2&gt;
&lt;p&gt;一般にSVMは識別器として用いられる事がほとんどだが、ラベルを実数とした回帰問題&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt;にも適用することができる。SVMによる回帰モデルのことを、 &lt;strong&gt;SVR&lt;/strong&gt;（Support Vector Regression, サポートベクトル回帰）という。 基本的な考え方としては、図の様に、識別面（回帰面）を中心に幅&lt;span class="math"&gt;\(2\varepsilon\)&lt;/span&gt;の"帯"に多くのサンプルが入るようにすれば良い。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVR" src="./images/svr.png"&gt;&lt;/p&gt;
&lt;p&gt;帯を考慮して制約を表現すると、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  | y_{i} - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) | \leq \varepsilon \quad (i = 1, \dots, N)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;となる。 これはハードマージン的な制約であり、幅&lt;span class="math"&gt;\(2\varepsilon\)&lt;/span&gt;の帯に全てのサンプルが入る事を要求している。もちろん&lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;を十分に大きくとれば全てのサンプルは帯に入るが、帯が広すぎるために自由度が大きく、 結果汎化性能の悪化に繋がってしまう。ラベルが実数となり、 雑音の影響をより受けやすくなることから、SVRにおいては、 最初からスラック変数を用いて、ソフトマージン的に定式化することが多い。&lt;/p&gt;
&lt;p&gt;スラック変数を用いて、帯から飛び出た距離&lt;span class="math"&gt;\(\eta_{i}^{+}, \eta_{i}^{-} \geq 0\)&lt;/span&gt;を次で定義する（図参照）: &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \eta_{i}^{+} =
  \left\{ \begin{array}{ll}
    y_{i} - \varepsilon - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) &amp;amp; y_{i} \geq (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) + \varepsilon \\
    0 &amp;amp; otherwise
    \end{array} \right.
  \\
  \eta_{i}^{-} =
  \left\{ \begin{array}{ll}
    (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) - y_{i} - \varepsilon &amp;amp; y_{i} \leq (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) - \varepsilon \\
    0 &amp;amp; otherwise
    \end{array} \right. 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;なお、サンプルは帯からどちらか一方にしか飛び出ないので、&lt;span class="math"&gt;\(\eta_{i}^{+}, \eta_{i}^{-}\)&lt;/span&gt;のいずれか一方は必ず&lt;span class="math"&gt;\(0\)&lt;/span&gt;となり、サンプルが帯に収まっている時は両方共&lt;span class="math"&gt;\(0\)&lt;/span&gt;となる。スラック変数を用いる事で、 制約は次のように表現できる: &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  \left\{ \begin{array}{l}
    (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) - y_{i} \leq \varepsilon + \eta_{i}^{-} \\
    y_{i} - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{i})+b) \leq \varepsilon + \eta_{i}^{+}
  \end{array} \right.
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ソフトマージンの時と同様に考える事で、 最適化問題が定式化できる:&lt;/p&gt;
&lt;h5&gt;1ノルムSVR・主問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp;\min_{\scriptsize \boldsymbol{w}} \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + C_{1}\sum_{i=1}^{N} (\eta_{i}^{+} + \eta_{i}^{-}) \\
          &amp;amp;\text{subject to : } (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) - y_{i} \leq \varepsilon + \eta_{i}^{-}, \ y_{i} - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \leq \varepsilon + \eta_{i}^{+} \quad (i=1,\dots,N)
\end{aligned}
$$&lt;/div&gt;
&lt;h5&gt;2ノルムSVR・主問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp;\min_{\scriptsize \boldsymbol{w}} \frac{1}{2} \boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} + \frac{C_{2}}{2}\sum_{i=1}^{N} \left\{ (\eta_{i}^{+})^{2} + (\eta_{i}^{-})^{2} \right\} \\
          &amp;amp;\text{subject to : } (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) - y_{i} \leq \varepsilon + \eta_{i}^{-}, \ y_{i} - (\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}_{i} + b) \leq \varepsilon + \eta_{i}^{+} \quad (i=1, \dots, N)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;後はKKT条件にぶち込むだけの流れ作業である。よし、じゃあぶち込んでやるぜ！&lt;/p&gt;
&lt;h4&gt;1ノルムSVR・双対問題の導出&lt;/h4&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \begin{split}
            &amp;amp;{\cal L}(\boldsymbol{w}, \boldsymbol{\eta}^{+}, \boldsymbol{\eta}^{-}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-}, \boldsymbol{\beta}^{+}, \boldsymbol{\beta}^{-}) = \\
          &amp;amp;\frac{1}{2} \boldsymbol{w}^{\mathsf{T}} \boldsymbol{w} + C_{1} \sum_{i=1}^{N}(\eta_{i}^{+}+\eta_{i}^{-}) + \sum_{i=1}^{N}(-\beta_{i}^{+}\eta_{i}^{+} -\beta_{i}^{-}\eta_{i}^{-} ) \\ 
          &amp;amp;+ \sum_{i=1}^{N} \left[ \alpha_{i}^{-} \left\{ (\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) - y_{i} - \varepsilon - \eta_{i}^{-} \right\} + \alpha_{i}^{+} \left\{ y_{i} - (\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) - \varepsilon - \eta_{i}^{+} \right\} \right] 
          \end{split}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;より、&lt;span class="math"&gt;\({\cal L}(\boldsymbol{w},  \boldsymbol{\eta}^{+}, \boldsymbol{\eta}^{-}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-}, \boldsymbol{\beta}^{+}, \boldsymbol{\beta}^{-})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\boldsymbol{w}, b, \eta_{i}^{+}, \eta_{i}^{-}\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}}, \frac{\partial \cal L}{\partial b}, \frac{\partial \cal L}{\partial \eta_{i}^{+}}, \frac{\partial \cal L}{\partial \eta_{i}^{-}}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{w} + \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \frac{\partial \cal L}{\partial b} = \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) \\
          \frac{\partial \cal L}{\partial \eta_{i}^{+}} = C_{1} - \alpha_{i}^{+} - \beta_{i}^{+}, \quad \frac{\partial \cal L}{\partial \eta_{i}^{-}} = C_{1} - \alpha_{i}^{-} - \beta_{i}^{-}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;それぞれ&lt;span class="math"&gt;\(0\)&lt;/span&gt;とおくと、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \boldsymbol{w}^{\star} = \sum_{i=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-}) \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) = 0, \quad C_{1} = \alpha_{i}^{+} + \beta_{i}^{+} = \alpha_{i}^{-} + \beta_{i}^{-}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;これをラグランジアンに代入すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \begin{split}
            &amp;amp;{\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{+\star}, \boldsymbol{\eta}^{-\star}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-}, \boldsymbol{\beta}^{+}, \boldsymbol{\beta}^{-}) = \\
            &amp;amp;\frac{1}{2} \sum_{i、j=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-})(\alpha_{j}^{+} - \alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + C_{1} \sum_{i=1}^{N}(\eta_{i}^{+}+\eta_{i}^{-}) - \sum_{i=1}^{N}(\beta_{i}^{+}\eta_{i}^{+} + \beta_{i}^{-}\eta_{i}^{-}) \\
            &amp;amp;+\sum_{i=1}^{N} \left[ \alpha_{i}^{-}\sum_{j=1}^{N}(\alpha_{j}^{+}-\alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) - \alpha_{i}^{+}\sum_{j=1}^{N}(\alpha_{j}^{+}-\alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \right] \\
            &amp;amp;+\sum_{i=1}^{N}\left[ \alpha_{i}^{-} (-\varepsilon-\eta_{i}^{-}-y_{i}) + \alpha_{i}^{+} (-\varepsilon-\eta_{i}^{+}+y_{i}) \right]
          \end{split}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(C_{1} = \alpha_{i}^{+} + \beta_{i}^{+} = \alpha_{i}^{-} + \beta_{i}^{-}\)&lt;/span&gt;を用いて整理すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \begin{split}
            &amp;amp;{\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{+\star}, \boldsymbol{\eta}^{-\star}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-}, \boldsymbol{\beta}^{+}, \boldsymbol{\beta}^{-}) = \\
            &amp;amp;\sum_{i=1}^{N}(\alpha_{i}^{+}-\alpha_{i}^{-}) - \varepsilon\sum_{i=1}^{N}(\alpha_{i}^{-}+\alpha_{i}^{+}) - \frac{1}{2} \sum_{i、j=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-})(\alpha_{j}^{+} - \alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) 
          \end{split}
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;ところで、上でも既に述べたが&lt;span class="math"&gt;\(\eta_{i}^{+}, \eta_{i}^{-}\)&lt;/span&gt;のどちらか一方は必ず&lt;span class="math"&gt;\(0\)&lt;/span&gt;となるので、その場合は対応する&lt;span class="math"&gt;\(\alpha_{i}^{+}, \alpha_{i}^{-}\)&lt;/span&gt;の制約条件はなくなり、従って、 &lt;span class="math"&gt;\(\alpha_{i}^{+}, \alpha_{i}^{-}\)&lt;/span&gt;のどちらか一方も&lt;span class="math"&gt;\(0\)&lt;/span&gt;となる。この事から&lt;span class="math"&gt;\(\alpha_{i} = \alpha_{i}^{+} - \alpha_{i}^{-}\)&lt;/span&gt;とおけば、&lt;span class="math"&gt;\(\alpha_{i}^{-} + \alpha_{i}^{+} = |\alpha_{i}|\)&lt;/span&gt;と表現できるので、双対問題は以下の様に表現できる。&lt;/p&gt;
&lt;h5&gt;1ノルムSVR・双対問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N}y_{i}\alpha_{i} - \varepsilon\sum_{i=1}^{N}|\alpha_{i}| - \frac{1}{2} \sum_{i、j=1}^{N}\alpha_{i}\alpha_{j}K(\boldsymbol{x}_{i}、 \boldsymbol{x}_{j}) \right] \\
          &amp;amp;\text{subject to : } \sum_{i=1}^{N}\alpha_{i} = 0, \ -C_{1} \leq \alpha_{i} \leq C_{1} \quad (i = 1、\dots、N) \nonumber
\end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;2ノルムSVR・双対問題の導出&lt;/h4&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
            &amp;amp;{\cal L}(\boldsymbol{w}, \boldsymbol{\eta}^{+}, \boldsymbol{\eta}^{-}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-}) = \\
            &amp;amp;\frac{1}{2} \boldsymbol{w}^{\mathsf{T}} \boldsymbol{w} + \frac{C_{2}}{2} \sum_{i=1}^{N} \{ (\eta_{i}^{+})^{2} + (\eta_{i}^{-})^{2} \} \\
            &amp;amp;+\sum_{i=1}^{N} \left[ \alpha_{i}^{-} \left\{ (\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) - y_{i} - \varepsilon - \eta_{i}^{-} \right\} + \alpha_{i}^{+} \left\{ y_{i} - (\boldsymbol{w}^{\mathsf{T}} \boldsymbol{\phi}(\boldsymbol{x}_{i}) + b) - \varepsilon - \eta_{i}^{+} \right\} \right] 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;より、&lt;span class="math"&gt;\({\cal L}(\boldsymbol{w}, \boldsymbol{\eta}^{+}, \boldsymbol{\eta}^{-}, \boldsymbol{\alpha}^{+}, \boldsymbol{\alpha}^{-})\)&lt;/span&gt;の&lt;span class="math"&gt;\(\boldsymbol{w}, b, \eta_{i}^{+}, \eta_{i}^{-}\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\displaystyle\frac{\partial \cal L}{\partial \boldsymbol{w}}, \frac{\partial \cal L}{\partial b}, \frac{\partial \cal L}{\partial \eta_{i}^{+}}, \frac{\partial \cal L}{\partial \eta_{i}^{-}}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \frac{\partial \cal L}{\partial \boldsymbol{w}} = \boldsymbol{w} + \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \frac{\partial \cal L}{\partial b} = \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) \\
          \frac{\partial \cal L}{\partial \eta_{i}^{+}} = C_{2}\eta_{i}^{+} - \alpha_{i}^{+}, \quad \frac{\partial \cal L}{\partial \eta_{i}^{-}} = C_{2}\eta_{i}^{-} - \alpha_{i}^{-}
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;それぞれ&lt;span class="math"&gt;\(0\)&lt;/span&gt;とおくと、 &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \boldsymbol{w}^{\star} = \sum_{i=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-}) \boldsymbol{\phi}(\boldsymbol{x}_{i}), \quad \sum_{i=1}^{N}(\alpha_{i}^{-} - \alpha_{i}^{+}) = 0, \quad \eta_{i}^{+\star} = \frac{\alpha_{i}^{+}}{C_{2}}, \quad \eta_{i}^{-\star} = \frac{\alpha_{i}^{-}}{C_{2}}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w}^{\star}\)&lt;/span&gt;をラグランジアンに代入すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          \begin{split}
            &amp;amp;{\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\eta}^{+\star}, \boldsymbol{\eta}^{-\star}, \boldsymbol{\alpha}^{+},\boldsymbol{\alpha}^{-}) = \\
            &amp;amp;\frac{1}{2} \sum_{i、j=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-})(\alpha_{j}^{+} - \alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \frac{1}{C_{2}} \sum_{i=1}^{N}\left\{ (\alpha_{i}^{+})^{2}+(\alpha_{i}^{-})^{2} \right\} \\
            &amp;amp;+\sum_{i=1}^{N} \left[ \alpha_{i}^{-}\sum_{j=1}^{N}(\alpha_{j}^{+}-\alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) - \alpha_{i}^{+}\sum_{j=1}^{N}(\alpha_{j}^{+}-\alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \right] \\
            &amp;amp;+\sum_{i=1}^{N}\left[ \alpha_{i}^{-} (-\varepsilon - \frac{\alpha_{i}^{-}}{C_{2}} - y_{i}) + \alpha_{i}^{+} (- \varepsilon - \frac{\alpha_{i}^{+}}{C_{2}} +y_{i}) \right] \\
            &amp;amp;=\sum_{i=1}^{N} (\alpha_{i}^{+} - \alpha_{i}^{-})y_{i} - \frac{1}{2} \sum_{i、j=1}^{N}(\alpha_{i}^{+}-\alpha_{i}^{-})(\alpha_{j}^{+}-\alpha_{j}^{-})K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) \\
            &amp;amp;-\varepsilon\sum_{i=1}^{N}(\alpha_{i}^{-} + \alpha_{i}^{+}) - \frac{1}{2C_{2}}\sum_{i=1}^{N}\left\{ (\alpha_{i}^{-})^{2} + (\alpha_{i}^{+})^{2} \right\}
          \end{split}
        \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;1ノルムSVRの時と同様に、&lt;span class="math"&gt;\(\alpha_{i} = \alpha_{i}^{+} - \alpha_{i}^{-}\)&lt;/span&gt;とおくと、&lt;span class="math"&gt;\((\alpha_{i}^{+})^{2} + (\alpha_{i}^{-})^{2} = \alpha_{i}^{2}\)&lt;/span&gt;が成り立つので、双対問題は以下の様に表現できる。&lt;/p&gt;
&lt;h5&gt;2ノルムSVR・双対問題&lt;/h5&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
          &amp;amp; \max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N}y_{i}\alpha_{i} - \varepsilon\sum_{i=1}^{N}|\alpha_{i}| - \frac{1}{2} \sum_{i, j=1}^{N}\alpha_{i}\alpha_{j}\left( K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \frac{1}{C_{2}} \right) \right] \\
          &amp;amp; \text{subject to : } \sum_{i=1}^{N}\alpha_{i} = 0 \quad (i = 1, \dots, N) \nonumber
\end{aligned}
$$&lt;/div&gt;
&lt;h1&gt;実装の例&lt;/h1&gt;
&lt;p&gt;実装例は &lt;a href="https://github.com/MrAiki/SimpleSVM"&gt;ここ&lt;/a&gt; にある。本稿では要点を絞って見ていく。&lt;/p&gt;
&lt;h2&gt;学習&lt;/h2&gt;
&lt;h3&gt;学習則の導出&lt;/h3&gt;
&lt;p&gt;SVMの学習は、双対問題&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp;\max_{\scriptsize \boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2} \sum_{i、j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{\phi}(\boldsymbol{x}_{i})^{\mathsf{T}}\boldsymbol{\phi}(\boldsymbol{x}_{j}) \right] = \max_{\scriptsize \boldsymbol{\alpha}} {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\alpha}) \\
  &amp;amp;\text{subject to : } \alpha_{i} \geq 0,\ \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \quad (i = 1, \dots, N)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;を解けば良いことになる。 脚注&lt;sup id="fnref2:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;で既に触れたが、SVMのマージン最大化は凸計画問題である。従って局所最適解が存在せず、極大値が大域的な最大値に一致する。
ソフトマージンに対応する時は、1ノルムソフトマージンの際には係数に値域&lt;span class="math"&gt;\(0 \geq \alpha_{i} \geq C_{1}\ (i=1,...,N)\)&lt;/span&gt;を設け、2ノルムの際にはカーネル関数&lt;span class="math"&gt;\(K\)&lt;/span&gt;を次のように書き換えれば良い：&lt;/p&gt;
&lt;div class="math"&gt;$$
K'(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) + \frac{\delta_{ij}}{C_{2}}
$$&lt;/div&gt;
&lt;p&gt;ここでは簡単な&lt;strong&gt;最急勾配法&lt;/strong&gt;によって解を求めることを考える。 最急勾配法の原理は単純である。&lt;span class="math"&gt;\(F(\boldsymbol{\alpha}) = {\cal L}(\boldsymbol{w}^{\star}, \boldsymbol{\alpha})\)&lt;/span&gt; とおくと、その&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;による偏微分&lt;span class="math"&gt;\(\frac{\partial F(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}}\)&lt;/span&gt;は&lt;strong&gt;勾配&lt;/strong&gt;、即ち&lt;span class="math"&gt;\(F(\boldsymbol{\alpha})\)&lt;/span&gt;の最も上昇する方向を指すベクトルとなるので、係数の更新量&lt;span class="math"&gt;\(\Delta\boldsymbol{\alpha}\)&lt;/span&gt;は学習率&lt;span class="math"&gt;\(\eta &amp;gt; 0\)&lt;/span&gt;を用いて&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta \boldsymbol{\alpha} = \eta \frac{\partial F(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}}
$$&lt;/div&gt;
&lt;p&gt;とすれば良い&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;。学習の収束判定は、例えば&lt;span class="math"&gt;\(||\Delta \boldsymbol{\alpha}||\)&lt;/span&gt;が十分小さくなった時とすれば良く、その時は極大値が得られている。
実際に&lt;span class="math"&gt;\(\frac{\partial F(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}}\)&lt;/span&gt;を計算することを考える。&lt;span class="math"&gt;\(\frac{\partial F(\boldsymbol{\alpha})}{\partial \alpha_{i}}\ (i=1,...,N)\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\frac{\partial F(\boldsymbol{\alpha})}{\partial \alpha_{i}} &amp;amp;= 1 - \frac{1}{2} \frac{\partial}{\partial \alpha_{i}} \left( \alpha_{1} \alpha_{1} y_{1} y_{1} \boldsymbol{x}_{1}^{\mathsf{T}} \boldsymbol{x}_{1} + ... + \alpha_{i} \alpha_{1} y_{i} y_{1} \boldsymbol{x}_{i}^{\mathsf{T}} \boldsymbol{x}_{1} + ... + \alpha_{i} \alpha_{N} y_{i} y_{N} \boldsymbol{x}_{i}^{\mathsf{T}} \boldsymbol{x}_{N} + ... + \alpha_{1} \alpha_{i} y_{1} y_{i} \boldsymbol{x}_{1}^{\mathsf{T}} \boldsymbol{x}_{i} + ... + \alpha_{N} \alpha_{i} y_{N} y_{i} \boldsymbol{x}_{N}^{\mathsf{T}} \boldsymbol{x}_{i} + ... + \alpha_{N} \alpha_{N} y_{N} y_{N} \boldsymbol{x}_{N}^{\mathsf{T}} \boldsymbol{x}_{N} \right) \\
    &amp;amp;= 1 - \sum_{j=1}^{N} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathsf{T}} \boldsymbol{x}_{j}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;よって、ステップ&lt;span class="math"&gt;\(t\)&lt;/span&gt;時の係数&lt;span class="math"&gt;\(\alpha_{i}(t)\ (i=1,...,N)\)&lt;/span&gt;について以下の更新規則に従って学習を行えば良い：&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_{i}(t+1) = \alpha_{i}(t) + \eta \left( 1 - \sum_{j=1}^{N} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathsf{T}} \boldsymbol{x}_{j} \right)
$$&lt;/div&gt;
&lt;h4&gt;実装&lt;/h4&gt;
&lt;p&gt;学習を行っている箇所を抜粋すると次の様になる：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="cm"&gt;/* 勾配値の計算 */&lt;/span&gt;
    &lt;span class="n"&gt;diff_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;diff_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="cm"&gt;/* C2を踏まえたカーネル関数値を計算 */&lt;/span&gt;
        &lt;span class="n"&gt;kernel_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GRAM_MATRIX_AT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;gram_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="n"&gt;kernel_val&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;soft_margin_C2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;diff_sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;kernel_val&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="n"&gt;diff_sum&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
      &lt;span class="n"&gt;diff_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;diff_sum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="n"&gt;diff_dist&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0f&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;diff_sum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0f&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;diff_sum&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="cm"&gt;/* 双対係数の更新 */&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="cm"&gt;/* printf(&amp;quot;dual_coef[%d]:%f -&amp;gt; &amp;quot;, i_sample, handle-&amp;gt;dual_coef[i_sample]); */&lt;/span&gt;
      &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
        &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SMPSVM_MOMENT_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;pre_diff_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt; 
      &lt;span class="cm"&gt;/* printf(&amp;quot;%f \n&amp;quot;, handle-&amp;gt;dual_coef[i_sample]); */&lt;/span&gt;

      &lt;span class="cm"&gt;/* 非数,無限チェック */&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;isinf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Detected NaN or Inf Dual-Coffience. &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;既にコメントが付いているが、特筆すべき点について補足する。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="cm"&gt;/* C2を踏まえたカーネル関数値を計算 */&lt;/span&gt;
        &lt;span class="n"&gt;kernel_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GRAM_MATRIX_AT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;gram_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;予め計算しておいたカーネル関数値をグラム行列から取り出している。学習中は何度もカーネル関数値を計算するため、グラム行列を用意しておくことで若干高速化できる。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="n"&gt;kernel_val&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;soft_margin_C2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;2ノルムソフトマージンのカーネル関数値を加味している。2ノルムソフトマージンを使用しない場合は&lt;code&gt;soft_margin_C2 == FLT_MAX&lt;/code&gt;となっているため、無視できる。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;      &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
        &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SMPSVM_MOMENT_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;pre_diff_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;係数更新を行っている。ここでは、単純な最急勾配分のみだけではなく、前回の勾配値に定数を乗じて加えた&lt;strong&gt;モーメント法&lt;/strong&gt;を使用している。一般にモーメント法を使用したほうが学習が早くなることが知られている。&lt;/p&gt;
&lt;h3&gt;制約条件の考慮&lt;/h3&gt;
&lt;p&gt;学習則は単純に見えても実装時に落とし穴になるのが制約条件である。&lt;/p&gt;
&lt;h4&gt;正例と負例の双対係数の和を等しくする&lt;/h4&gt;
&lt;p&gt;KKT条件から導かれる&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;についての制約&lt;/p&gt;
&lt;div class="math"&gt;$$
  \sum_{i=1}^{N} \alpha_{i}y_{i} = 0
$$&lt;/div&gt;
&lt;p&gt;を実現するのが案外難しい。上の制約から、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
  &amp;amp;\sum_{y_{i}=1} \alpha_{i} - \sum_{y_{i}=-1} \alpha_{i} = 0  \\
  &amp;amp;\iff \sum_{y_{i}=1} \alpha_{i} = \sum_{y_{i}=-1} \alpha_{i} 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;が導かれるため、正例と負例の双対係数の和は等しくなる事が分かる。
本実装では、&lt;span class="math"&gt;\(\alpha_{i}y_{i}\)&lt;/span&gt;の平均を取り、全係数&lt;span class="math"&gt;\(\alpha_{i}\ (i=1,...,N)\)&lt;/span&gt;をその平均に寄せることで上記の制約を満たすように係数を修正している。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="cm"&gt;/* 制約1: 正例と負例の双対係数和を等しくする. */&lt;/span&gt;
    &lt;span class="n"&gt;dual_coef_average&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;dual_coef_average&lt;/span&gt; 
        &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;dual_coef_average&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
        &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dual_coef_average&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;この制約を満たすための実装はこの限りではない。&lt;/p&gt;
&lt;h4&gt;双対係数は非負&lt;/h4&gt;
&lt;p&gt;双対係数は非負でなければならないため、負になった係数は全て0に修正してしまう。 学習が進むに連れて0の係数が増えていくが、それはSVMの持つスパース学習の効果が現れている状態である。学習が収束した時、0に潰れず非負値となった係数に対応するサンプルが&lt;strong&gt;サポートベクトル&lt;/strong&gt;である。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="cm"&gt;/* 制約2: 双対係数は非負 */&lt;/span&gt;
    &lt;span class="n"&gt;coef_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;soft_margin_C1&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="cm"&gt;/* C1ノルムの制約を適用 */&lt;/span&gt;
        &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soft_margin_C1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="cm"&gt;/* ここで最終結果が出る. 前回との変化を計算 */&lt;/span&gt;
      &lt;span class="n"&gt;coef_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pre_dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
      &lt;span class="n"&gt;coef_dist&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coef_diff&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;coef_diff&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;本実装では、非負条件に咥えて1ノルムソフトマージンの制約も追加で判定している。1ノルムソフトマージンを使用しない時は&lt;code&gt;soft_margin_C1 == FLT_MAX&lt;/code&gt;となっているため、無視できる。&lt;/p&gt;
&lt;h2&gt;識別&lt;/h2&gt;
&lt;p&gt;&lt;a href="#マージンの定式化"&gt;マージンの定式化&lt;/a&gt;で述べたが、SVMのクラス識別は出力値&lt;span class="math"&gt;\(y\)&lt;/span&gt;の正負によって判断する&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;。SVMの出力式&lt;/p&gt;
&lt;div class="math"&gt;$$
  g(\boldsymbol{x}, \boldsymbol{w}) = \boldsymbol{w}^{\mathsf{T}} \boldsymbol{x} + b
$$&lt;/div&gt;
&lt;p&gt;に、KKT条件における最適条件&lt;span class="math"&gt;\(\boldsymbol{w}^{\star} = \sum_{i=1}^{N} \alpha_{i}y_{i}\boldsymbol{x}_{i}\)&lt;/span&gt;を代入すれば、次の&lt;strong&gt;双対表現&lt;/strong&gt;が得られる：&lt;/p&gt;
&lt;div class="math"&gt;$$
  g(\boldsymbol{x}, \boldsymbol{w}^{\star}) = \sum_{i=1}^{N} \alpha_{i} y_{i} K(\boldsymbol{x}_{i}, \boldsymbol{x}) + b
$$&lt;/div&gt;
&lt;p&gt;識別の際には、学習済みの係数&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;を使用して上式を計算し、その正負を判定すれば良い。実装としては次の様になる：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="cm"&gt;/* ネットワーク出力計算 */&lt;/span&gt;
  &lt;span class="n"&gt;network_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="cm"&gt;/* 係数が正に相当するサンプル（サポートベクトル）&lt;/span&gt;
&lt;span class="cm"&gt;     * のみを計算する */&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;network_output&lt;/span&gt; 
        &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;dual_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;kernel_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sample_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;normalized_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;kernel_parameter&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 識別 */&lt;/span&gt;
  &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network_output&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;脚注&lt;/h1&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;高村大也、 奥村学、 "言語処理のための機械学習入門"、 コロナ社、 2010&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;高橋治久、 堀田一弘、 "学習理論" コロナ社、 2009&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;くどいかもしれないが、 サポートベクトルは最も識別面に近いサンプルなので、この仮定により&lt;span class="math"&gt;\(y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}+b) \geq l \quad (i=1, \dots, N)\)&lt;/span&gt;が成り立つ。&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;（証明） &lt;br&gt; -   最適化対象について、&lt;span class="math"&gt;\(\displaystyle \frac{1}{2}\boldsymbol{w}^{\mathsf{T}}\boldsymbol{w} = \frac{1}{2} \sum_{i=1}^{n} w_{i}^{2}\)&lt;/span&gt;より（&lt;span class="math"&gt;\(\boldsymbol{w}=[w_{1}\dots w_{n}]^\mathsf{T}\)&lt;/span&gt;）、 明らかに下に凸である。&lt;br&gt;
 -   制約条件について、&lt;span class="math"&gt;\(W_{i} = \{ \boldsymbol{w} | y_{i}(\boldsymbol{w}^{\mathsf{T}}\boldsymbol{x}\_{i}+b) \geq 1 \}\)&lt;/span&gt;とおくと、&lt;span class="math"&gt;\(\forall \boldsymbol{w}^{\prime}, \boldsymbol{w}^{\prime\prime} \in W_{i}, \forall t \in [0, 1]\)&lt;/span&gt;に対して、&lt;div class="math"&gt;$$\begin{aligned} y_{i} \left[ \left( t\boldsymbol{w}^{\prime\mathsf{T}} + (1-t) \boldsymbol{w}^{\prime\prime\mathsf{T}} \right) \boldsymbol{x}\_{i} + b \right] = y_{i} \left[ t(\boldsymbol{w}^{\prime\mathsf{T}}\boldsymbol{x}\_{i} - \boldsymbol{w}^{\prime\prime\mathsf{T}}\boldsymbol{x}\_{i}) + \boldsymbol{w}^{\prime\prime\mathsf{T}}\boldsymbol{x}\_{i} + b \right] \\
            = y_{i} \left[ t\left( (\boldsymbol{w}^{\prime\mathsf{T}}\boldsymbol{x}\_{i} + b) - (\boldsymbol{w}^{\prime\prime\mathsf{T}}\boldsymbol{x}\_{i} + b) \right) + \boldsymbol{w}^{\prime\prime\mathsf{T}}\boldsymbol{x}\_{i} + b \right] \\
            = t y_{i} (\boldsymbol{w}^{\prime\mathsf{T}}\boldsymbol{x}\_{i} + b) + (1-t) y_{i}(\boldsymbol{w}^{\prime\prime\mathsf{T}}\boldsymbol{x}\_{i} + b) \\
            \geq t + (1-t) = 1\end{aligned}$$&lt;/div&gt; よって、&lt;span class="math"&gt;\(t\boldsymbol{w}^{\prime} + (1-t) \boldsymbol{w}^{\prime\prime} \in W_{i}\)&lt;/span&gt;より&lt;span class="math"&gt;\(W_{i}\)&lt;/span&gt;は凸集合。 最適化問題においては、 &lt;span class="math"&gt;\(W_{i}\)&lt;/span&gt;の共通部分&lt;span class="math"&gt;\(\bigcap_{i=1}^{N} W_{i}\)&lt;/span&gt;を考えれば良く、 &lt;strong&gt;凸集合の積集合もまた凸集合&lt;/strong&gt; なので、 制約条件も凸集合となる。以上の2点より、 マージン最大化は凸計画問題。&lt;br&gt; （凸集合の積集合もまた凸集合であることの証明）2つの凸集合を&lt;span class="math"&gt;\(A_{1},A_{2}\)&lt;/span&gt;とする。 両者の集合の積&lt;span class="math"&gt;\(A_{1}\cap A_{2}\)&lt;/span&gt;が空集合ならば、 空集合は凸集合と定義されるので命題は成立する。 一般に&lt;span class="math"&gt;\(A_{1}\cap A_{2}\)&lt;/span&gt;から2点&lt;span class="math"&gt;\(x,y\)&lt;/span&gt;をとると, &lt;span class="math"&gt;\(x, y\)&lt;/span&gt;を結ぶ線分は、 &lt;span class="math"&gt;\(A_{1}, A_{2}\)&lt;/span&gt;は共に凸集合なので、 &lt;span class="math"&gt;\(A_{1}\)&lt;/span&gt;にも&lt;span class="math"&gt;\(A_{2}\)&lt;/span&gt;にも属していて飛び出ることはない。 これは集合の積&lt;span class="math"&gt;\(A_{1}\cap A_{2}\)&lt;/span&gt;が凸集合であることを示している。&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;（鞍点&lt;span class="math"&gt;\((\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;が最適点となる事の証明） &lt;br&gt; &lt;span class="math"&gt;\((\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;は鞍点なので、&lt;div class="math"&gt;$$\begin{aligned} {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) \leq {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star}) \leq {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha}) \end{aligned}$$&lt;/div&gt; を満たす。 従って右側の不等式から&lt;span class="math"&gt;\(\boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}^{\star}) \leq \boldsymbol{\alpha}^{\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}^{\star})\)&lt;/span&gt;が任意の&lt;span class="math"&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;で成立する。 即ち&lt;span class="math"&gt;\(\boldsymbol{\alpha} = \boldsymbol{0}\)&lt;/span&gt;の時、 &lt;span class="math"&gt;\(g_{i}(\boldsymbol{v}^{\star}) \geq 0\)&lt;/span&gt;と併せて&lt;span class="math"&gt;\(\boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}^{\star}) = 0\)&lt;/span&gt;が成立する。 更に、 ここで関係式 &lt;div class="math"&gt;$$\begin{aligned} {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) - {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star}) \leq \left( \frac{\partial {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha})}{\partial \boldsymbol{v}} \right)^{\mathsf{T}} (\boldsymbol{v} - \boldsymbol{v}^{\star}) \end{aligned}$$&lt;/div&gt; を用いる（証明は後術）と、 鞍点であることから&lt;span class="math"&gt;\(\displaystyle\frac{\partial {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha})}{\partial \boldsymbol{v}} = \boldsymbol{0}\)&lt;/span&gt;であり、 また、 &lt;span class="math"&gt;\(\boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}^{\star}) = 0\)&lt;/span&gt;より、 &lt;div class="math"&gt;$$\begin{aligned} {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) - {\cal L}(\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star}) &amp;amp;= {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) - f(\boldsymbol{v}^{\star}) \leq 0 \iff f(\boldsymbol{v}^{\star}) \geq {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) \end{aligned}$$&lt;/div&gt; が成り立つ。 更に、 もとより&lt;span class="math"&gt;\(\boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}) \geq 0\)&lt;/span&gt;なので、 &lt;div class="math"&gt;$$\begin{aligned} f(\boldsymbol{v}) \leq f(\boldsymbol{v}) + \boldsymbol{\alpha}^{\star\mathsf{T}}\boldsymbol{g}(\boldsymbol{v}) = {\cal L}(\boldsymbol{v}, \boldsymbol{\alpha}^{\star}) \end{aligned}$$&lt;/div&gt; 従って&lt;span class="math"&gt;\(f(\boldsymbol{v}^{\star}) \geq f(\boldsymbol{v})\)&lt;/span&gt;が任意の&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;で成立し、&lt;span class="math"&gt;\((\boldsymbol{v}^{\star}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;が最適点となる事が示された。&lt;br&gt; 次いで(＊)を証明する。&lt;span class="math"&gt;\({\cal L}(\boldsymbol{v}, \boldsymbol{\alpha})\)&lt;/span&gt;が凸関数ならば、 &lt;span class="math"&gt;\({\cal L}(t\boldsymbol{v}^{\prime}+(1-t)\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) \geq t {\cal L}(\boldsymbol{v}^{\prime}, \boldsymbol{\alpha}^{\star}) + (1-t) {\cal L}(\boldsymbol{v}^{\prime}, \boldsymbol{\alpha}^{\star})\)&lt;/span&gt;が&lt;span class="math"&gt;\(t \in [0,1]\)&lt;/span&gt;で成立する。 よって、&lt;div class="math"&gt;$$t{\cal L}(\boldsymbol{v}^{\prime}, \boldsymbol{\alpha}^{\star}) \leq t{\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) - {\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) + {\cal L}(t\boldsymbol{v}^{\prime}+(1-t)\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) \iff {\cal L}(\boldsymbol{v}^{\prime}, \boldsymbol{\alpha}^{\star}) \leq {\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) + \frac{{\cal L}(\boldsymbol{v}^{\prime\prime} + t(\boldsymbol{v}^{\prime}-\boldsymbol{v}^{\prime\prime}), \boldsymbol{\alpha}^{\star}) - {\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star})}{t} $$&lt;/div&gt;ここで&lt;span class="math"&gt;\(t \to 0\)&lt;/span&gt;ならしめれば、 方向微分と勾配の関係式より、&lt;div class="math"&gt;$${\cal L}(\boldsymbol{v}^{\prime}, \boldsymbol{\alpha}^{\star}) - {\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star}) \leq \left( \frac{\partial {\cal L}(\boldsymbol{v}^{\prime\prime}, \boldsymbol{\alpha}^{\star})}{\partial \boldsymbol{v}} \right)^{\mathsf{T}} (\boldsymbol{v}^{\prime} - \boldsymbol{v}^{\prime\prime})$$&lt;/div&gt;を得る。&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;互いに同一平面上&lt;strong&gt;以外&lt;/strong&gt;の位置にある事。 例えば、2次元空間では同一直線上以外の位置であり、3次元空間では同一平面上以外の位置である。異なるクラスのサンプルが一般位置にあれば、もとより線形分離可能である。&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;有限個数&lt;span class="math"&gt;\(N&amp;lt;\infty\)&lt;/span&gt;のサンプルに対し、&lt;span class="math"&gt;\((\boldsymbol{G})\_{ij} = K(\boldsymbol{x}\_{i}, \boldsymbol{x}\_{j})\)&lt;/span&gt;、即ち&lt;span class="math"&gt;\((i,j)\)&lt;/span&gt;成分の値が&lt;span class="math"&gt;\(K(\boldsymbol{x}\_{i}, \boldsymbol{x}\_{j})\)&lt;/span&gt;となっている行列&lt;span class="math"&gt;\(\boldsymbol{G}\)&lt;/span&gt;をグラム（カーネル）行列という。特徴写像が有限次元ならば、グラム行列が（有限）正定値行列ならば&lt;span class="math"&gt;\(K\)&lt;/span&gt;はカーネル関数となる。特徴写像が無限次元の場合のカーネル関数の条件がマーサーの定理である。&lt;br&gt; その内容は、入力空間&lt;span class="math"&gt;\(X\subset \mathbb{R}^{n}\)&lt;/span&gt;が有界閉集合（&lt;span class="math"&gt;\(\iff\)&lt;/span&gt;コンパクト）であるとし、対象な連続関数&lt;span class="math"&gt;\(K\)&lt;/span&gt;が正定値、即ち任意の二乗可積分（二乗積分可能）な関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;に対し &lt;div class="math"&gt;$$\begin{aligned}
  \int_{X\times X}K(x, z)f(x)f(z)dxdz \geq 0\end{aligned}$$&lt;/div&gt; ならば、ヒルベルト空間の正規直交基底&lt;span class="math"&gt;\(\phi_{j}\ (j=1, 2, \dots)\)&lt;/span&gt;で次式が一様収束するものが存在する場合、&lt;span class="math"&gt;\(K\)&lt;/span&gt;はカーネル関数である。 &lt;div class="math"&gt;$$ \begin{aligned} K(x, z) = \sum_{j=1}^{\infty} \phi_{j}(x)\phi_{j}(z) \end{aligned} $$&lt;/div&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;サンプルに現れない未知のデータでももれなく識別できる能力&amp;#160;&lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;双対問題において、 &lt;span class="math"&gt;\(C_{1}, C_{2} \to \infty\)&lt;/span&gt;とすると、ハードマージンSVMに一致することが分かる&amp;#160;&lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;サンプルに最も当てはまる曲線（面）を探す問題。もう少し形式的に言うと、各サンプル&lt;span class="math"&gt;\(\boldsymbol{x}\_{i}\)&lt;/span&gt;でのラベル&lt;span class="math"&gt;\(y_{i}\)&lt;/span&gt;の平均値を表す関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;を学習する問題。&amp;#160;&lt;a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;ただし学習率&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;の決め方は問題依存である。一般に、&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;が小さすぎると学習が進行せず、大きすぎると極値を飛び越えてしまい学習が収束しない。&amp;#160;&lt;a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y = 0\)&lt;/span&gt;の場合の判断を明確にしている書類がない。ここでは正と判定する。&amp;#160;&lt;a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="機械学習"></category></entry><entry><title>MCMC（マルコフ連鎖モンテカルロ）法</title><link href="/mcmcmarukohulian-suo-montekarurofa.html" rel="alternate"></link><published>2020-04-23T12:20:00+09:00</published><updated>2020-04-23T12:20:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/mcmcmarukohulian-suo-montekarurofa.html</id><summary type="html">&lt;p&gt;本稿ではMCMC法の解説のため、MC法による積分の計算方法（モンテカルロ積分）から、MCMCによる手法の概要を見ていく。MCMC法は有名かつ知り尽くされた手法で、多くの良質な説明資料 &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt; が存在している。従ってここの説明は読まずに、資料を見てもらった方が理解が早いかもしれない。&lt;/p&gt;
&lt;p&gt;一般に &lt;strong&gt;MC（Monte-Calro, モンテカルロ）法&lt;/strong&gt; は、サンプリング（サンプルを乱数から生成すること）によってシミュレーションや数値計算を行う手法である。特に確率分布が関わる積分値&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; を近似的に求めるMC法はモンテカルロ積分と呼ばれる。モンテカルロ積分は確率的な推論の一種であり、大数の法則&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt; によって、十分なサンプル数をとれば近似精度をいくらでも良くする事ができる。サンプリングの手間がある為、近似分布をあらかじめ仮定する様な決定論的な推論よりも遥かに推論が遅い。しかし、MCは近似分布が求められないような場合にも適用可能であり、汎用性が高いと言える。&lt;/p&gt;
&lt;p&gt;MC法によって原理的には任意の解を求められるが、十分なサンプル数の要求というのが大きな問題を孕んでいる。サンプリングの自由度（範囲及び次元）が大きくなると、解の計算にあまり寄与しない（無駄な）サンプルが増えてしまう。計算を現実的かつ効率的に行うためには、サンプルの選択が重要になる。&lt;/p&gt;
&lt;p&gt;そして &lt;strong&gt;MCMC（Markov …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;本稿ではMCMC法の解説のため、MC法による積分の計算方法（モンテカルロ積分）から、MCMCによる手法の概要を見ていく。MCMC法は有名かつ知り尽くされた手法で、多くの良質な説明資料 &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt; が存在している。従ってここの説明は読まずに、資料を見てもらった方が理解が早いかもしれない。&lt;/p&gt;
&lt;p&gt;一般に &lt;strong&gt;MC（Monte-Calro, モンテカルロ）法&lt;/strong&gt; は、サンプリング（サンプルを乱数から生成すること）によってシミュレーションや数値計算を行う手法である。特に確率分布が関わる積分値&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; を近似的に求めるMC法はモンテカルロ積分と呼ばれる。モンテカルロ積分は確率的な推論の一種であり、大数の法則&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt; によって、十分なサンプル数をとれば近似精度をいくらでも良くする事ができる。サンプリングの手間がある為、近似分布をあらかじめ仮定する様な決定論的な推論よりも遥かに推論が遅い。しかし、MCは近似分布が求められないような場合にも適用可能であり、汎用性が高いと言える。&lt;/p&gt;
&lt;p&gt;MC法によって原理的には任意の解を求められるが、十分なサンプル数の要求というのが大きな問題を孕んでいる。サンプリングの自由度（範囲及び次元）が大きくなると、解の計算にあまり寄与しない（無駄な）サンプルが増えてしまう。計算を現実的かつ効率的に行うためには、サンプルの選択が重要になる。&lt;/p&gt;
&lt;p&gt;そして &lt;strong&gt;MCMC（Markov Chain Monte-Calro, マルコフ連鎖モンテカルロ）法&lt;/strong&gt; は、新しいサンプルを以前に生成したサンプルに確率的に依存して（サンプルの列がマルコフ連鎖となる様に）生成するMC法である。MCMCでは、新しく生成したサンプルを採択（採用）するか棄却（捨てる）するかも確率的に判断する。この手続きによって、無駄なサンプルを極力減らすようにサンプリングを実行することができる。&lt;/p&gt;
&lt;h3&gt;MC法による積分 - モンテカルロ積分&lt;/h3&gt;
&lt;p&gt;確率変数を&lt;span class="math"&gt;\(d\)&lt;/span&gt;次元の実数値ベクトル&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt; &lt;span class="math"&gt;\(\boldsymbol{x} = [x_{1},\dots,x_{d}]^{\mathsf{T}} \in X \subset \mathbb{R}^{d}\)&lt;/span&gt;とする。ここで&lt;span class="math"&gt;\(X\)&lt;/span&gt;は全事象&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt; の集合である。&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;の確率分布を&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;とし、関数&lt;span class="math"&gt;\(h\)&lt;/span&gt;の確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;による平均（期待値）&lt;/p&gt;
&lt;div class="math"&gt;$$
  I = \int_{X} h(\boldsymbol{x})r(\boldsymbol{x}) d\boldsymbol{x} = \mathrm{E}_{r}[h(\boldsymbol{x})] \tag{1}
$$&lt;/div&gt;
&lt;p&gt;を求めることを考える。ここで、&lt;span class="math"&gt;\(\mathrm{E}_{p}[\cdot]\)&lt;/span&gt;は確率分布&lt;span class="math"&gt;\(p\)&lt;/span&gt;による平均を表す。&lt;span class="math"&gt;\(I\)&lt;/span&gt;において、関数&lt;span class="math"&gt;\(h\)&lt;/span&gt;の形に制約を与えておらず積分として様々な値が計算できる。例を挙げると:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(\boldsymbol{x}) = \boldsymbol{x}\)&lt;/span&gt; : この場合は&lt;span class="math"&gt;\(\mathrm{E}_{r}[\boldsymbol{x}]\)&lt;/span&gt;、即ち&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;の平均を求める&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(\boldsymbol{x}) = (\boldsymbol{x} - \mathrm{E_{r}}[\boldsymbol{x}])(\boldsymbol{x} - \mathrm{E_{r}}[\boldsymbol{x}])^{\mathsf{T}}\)&lt;/span&gt; : &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;の分散を求める&lt;/li&gt;
&lt;li&gt;... その他 &lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;もし&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;が既知で、分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;から簡単に独立にサンプリングできる&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;ならば、&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;からの独立な（他のサンプルに依存して生成しない）&lt;span class="math"&gt;\(n\)&lt;/span&gt;個のサンプルを&lt;span class="math"&gt;\(\boldsymbol{x_{1}}, \boldsymbol{x_{2}}, \dots, \boldsymbol{x_{n}}\)&lt;/span&gt;と書くと、&lt;span class="math"&gt;\(I\)&lt;/span&gt;の標本平均による近似値&lt;span class="math"&gt;\(\hat{I}\)&lt;/span&gt;は&lt;/p&gt;
&lt;div class="math"&gt;$$
  \hat{I} = \frac{1}{n} \sum_{i=1}^{n} h(\boldsymbol{x_{i}}) \tag{2}
$$&lt;/div&gt;
&lt;p&gt;で計算できる。大数の法則により、サンプル数の極限を取れば標本平均は真の平均に一致する:&lt;/p&gt;
&lt;div class="math"&gt;$$
  \lim_{n \to \infty} \hat{I} = I
$$&lt;/div&gt;
&lt;p&gt;この様にして平均を求める方法を &lt;strong&gt;モンテカルロ積分(Monte-Carlo Integration)&lt;/strong&gt; という。一般にモンテカルロ法(Monte-Carlo Method)はサンプリングによってシミュレーションや数値計算を行う事を指す。&lt;/p&gt;
&lt;h3&gt;重点サンプリング&lt;/h3&gt;
&lt;p&gt;モンテカルロ積分によって、原理的には&lt;span class="math"&gt;\(\hat{I}\)&lt;/span&gt;を多くのサンプルで計算する事で&lt;span class="math"&gt;\(I\)&lt;/span&gt;を精度良く計算できる。しかし実際確率分布&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;は複雑であることが多く、その場合&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;から直接サンプリングするのは困難となる。そこで、より簡単でサンプリング可能な確率分布（&lt;strong&gt;提案分布&lt;/strong&gt; という）&lt;span class="math"&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;を用意して、そこからサンプリングする事を考える。&lt;span class="math"&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;を使えば、&lt;span class="math"&gt;\(I\)&lt;/span&gt;は次の様に変形できる:&lt;/p&gt;
&lt;div class="math"&gt;$$
  I = \int_{X} h(\boldsymbol{x})\frac{r(\boldsymbol{x})}{q(\boldsymbol{x})} q(\boldsymbol{x}) d\boldsymbol{x} = \mathrm{E}_{q}\left[ h(\boldsymbol{x})\frac{r(\boldsymbol{x})}{q(\boldsymbol{x})} \right]
$$&lt;/div&gt;
&lt;p&gt;モンテカルロ積分の時と同じ様にに考え、次は&lt;span class="math"&gt;\(\boldsymbol{x_{1}},\dots,\boldsymbol{x_{n}}\)&lt;/span&gt;を&lt;span class="math"&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;からの独立な&lt;span class="math"&gt;\(n\)&lt;/span&gt;個のサンプルにすれば、&lt;span class="math"&gt;\(I\)&lt;/span&gt;の近似値&lt;span class="math"&gt;\(\hat{I}_{IS}\)&lt;/span&gt;として&lt;/p&gt;
&lt;div class="math"&gt;$$
  \hat{I}_{IS} = \frac{1}{n} \sum_{i=1}^{n} h(\boldsymbol{x_{i}}) \frac{r(\boldsymbol{x_{i}})}{q(\boldsymbol{x_{i}})} = \frac{1}{n} \sum_{i=1}^{n} h(\boldsymbol{x_{i}}) w(\boldsymbol{x_{i}}) \tag{3}
$$&lt;/div&gt;
&lt;p&gt;が得られる。ここで&lt;span class="math"&gt;\(w(\boldsymbol{x_{i}}) = r(\boldsymbol{x_{i}})/q(\boldsymbol{x_{i}})\)&lt;/span&gt;はサンプル&lt;span class="math"&gt;\(\boldsymbol{x_{i}}\)&lt;/span&gt;に対する重みと見ることができる。この様に、重みが付いたサンプルで平均を求める手法を &lt;strong&gt;重点サンプリング(Importance Sampling)&lt;/strong&gt; という。重点サンプリングにおいても、&lt;span class="math"&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;がある条件を満たしていれば、大数の法則によって&lt;span class="math"&gt;\(\displaystyle\lim_{n \to \infty} \hat{I}_{IS} = I\)&lt;/span&gt;となることが保証されている。&lt;/p&gt;
&lt;h2&gt;MCMC&lt;/h2&gt;
&lt;p&gt;重点サンプリングの考え方によって、確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;が複雑でも替わりに提案分布&lt;span class="math"&gt;\(q\)&lt;/span&gt;を用いてサンプリングを行えばモンテカルロ積分が計算できる事が確かめられた。しかし、" &lt;span class="math"&gt;\(r\)&lt;/span&gt;より簡単でサンプリング可能な&lt;span class="math"&gt;\(q\)&lt;/span&gt;" を構成する事自体が一般に困難である。特に次元&lt;span class="math"&gt;\(d\)&lt;/span&gt;が増加すれば&lt;span class="math"&gt;\(r\)&lt;/span&gt;が複雑になるのはもちろん、全事象&lt;span class="math"&gt;\(X\)&lt;/span&gt;の自由度が増加し次元の呪い&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;を引き起こす。即ち、&lt;span class="math"&gt;\(r\)&lt;/span&gt;を&lt;span class="math"&gt;\(q\)&lt;/span&gt;で良く近似出来てない時に毎回独立にサンプリングを行っていると、空間&lt;span class="math"&gt;\(X\)&lt;/span&gt;から当てずっぽうなサンプルを取得しているのと同様な状態になる。&lt;/p&gt;
&lt;p&gt;そこで、簡単な提案分布&lt;span class="math"&gt;\(q\)&lt;/span&gt;を用いて、かつ逐次的に以前のサンプルを使用して新しくサンプルを生成する手法が90年代以降使われる様になってきた。この場合、サンプル列はマルコフ連鎖(Markov Chain)をなす。そして、マルコフ連鎖で生成したサンプルによるMC法をMCMC（Markov Chain Monte-Calro）法という。サンプル間の独立性は担保されなくなる為にMC法の基本原理が成立しなくなるが、提案分布（マルコフ連鎖の遷移確率）がある性質を満たせば、十分なサンプル数で確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;からのサンプリングが実現できる。&lt;/p&gt;
&lt;h3&gt;遷移確率の条件 - 詳細釣り合い条件&lt;/h3&gt;
&lt;p&gt;概要でも既に述べたが、MCMCは生成したサンプル列がマルコフ連鎖をなすように生成する。今、サンプル列&lt;span class="math"&gt;\(\boldsymbol{x_{0}}, \boldsymbol{x_{1}}, \dots\)&lt;/span&gt;はマルコフ連鎖をなすので、生成した時刻（ステップ）で実際に観測した状態を&lt;span class="math"&gt;\(\boldsymbol{e_{0}}, \boldsymbol{e_{1}}, \dots \ (\boldsymbol{e_{i}} \in X \ i=0,1,\dots)\)&lt;/span&gt;と書くと、任意の時刻&lt;span class="math"&gt;\(n \geq 0\)&lt;/span&gt;で、&lt;/p&gt;
&lt;div class="math"&gt;$$
  P(\boldsymbol{x_{n+1}} = \boldsymbol{e_{n+1}}|\boldsymbol{x_{0}} = \boldsymbol{e_{0}}, \boldsymbol{x_{1}} = \boldsymbol{e_{1}}, \dots, \boldsymbol{x_{n}} = \boldsymbol{e_{n}}) = P(\boldsymbol{x_{n+1}} = \boldsymbol{e_{n+1}}|\boldsymbol{x_{n}} = \boldsymbol{e_{n}})
$$&lt;/div&gt;
&lt;p&gt;が成り立つ（この性質をマルコフ性&lt;sup id="fnref:13"&gt;&lt;a class="footnote-ref" href="#fn:13"&gt;13&lt;/a&gt;&lt;/sup&gt;という）。即ち、サンプルは直前のサンプルのみに依存して生成する。この様にサンプルを生成する場合、実はマルコフ連鎖が &lt;strong&gt;エルゴード的(ergodic)&lt;/strong&gt; という性質を満たせば、大量のサンプルを用いた時にある分布（&lt;strong&gt;定常分布&lt;/strong&gt;）&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;からサンプリングしているのと同様になる。&lt;/p&gt;
&lt;p&gt;マルコフ連鎖がエルゴード的であるとは、規約性（どの状態からでも任意の状態へ遷移できる）と正再帰性（任意の状態へ何回でも遷移できる）非周期性（任意の状態は一回の遷移で元に戻れる）を全て同時に満たすことを言う&lt;sup id="fnref:14"&gt;&lt;a class="footnote-ref" href="#fn:14"&gt;14&lt;/a&gt;&lt;/sup&gt;。
エルゴード的なマルコフ連鎖と定常分布&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;の関係は、次の定理で表せる:&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;マルコフ連鎖の収束&lt;/h4&gt;
&lt;p&gt;マルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{0}}, \boldsymbol{x_{1}}, \dots\)&lt;/span&gt;がエルゴード的であり、その遷移確率行列を&lt;span class="math"&gt;\(\boldsymbol{P}\)&lt;/span&gt;とおく。&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;を&lt;span class="math"&gt;\(\boldsymbol{P}\)&lt;/span&gt;の定常（不変）分布とした時、任意の初期状態から始まるマルコフ連鎖はサンプル数の極限において定常分布&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;に収束する。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ここで遷移確率行列&lt;span class="math"&gt;\(\boldsymbol{P}\)&lt;/span&gt;とは、その&lt;span class="math"&gt;\((i,j)\)&lt;/span&gt;要素&lt;span class="math"&gt;\((\boldsymbol{P})\_{ij} = p_{ij}\ (i,j \in X)\)&lt;/span&gt;が任意の時刻&lt;span class="math"&gt;\(t \geq 0\)&lt;/span&gt;で&lt;/p&gt;
&lt;div class="math"&gt;$$
  (\boldsymbol{P})_{ij} = p_{ij} = P(\boldsymbol{x_{t+1}}=j|\boldsymbol{x_{t}}=i)
$$&lt;/div&gt;
&lt;p&gt;を満たすような行列である&lt;sup id="fnref:15"&gt;&lt;a class="footnote-ref" href="#fn:15"&gt;15&lt;/a&gt;&lt;/sup&gt;。
また、定常分布とは時刻が経過しようとも不変なマルコフ連鎖（一般に確率過程）の各状態の確率分布である&lt;sup id="fnref:16"&gt;&lt;a class="footnote-ref" href="#fn:16"&gt;16&lt;/a&gt;&lt;/sup&gt;。即ち、十分に長いマルコフ連鎖を観測すれば、どの状態にいる傾向があるのかを定常分布によって知ることができる。&lt;/p&gt;
&lt;p&gt;上記の議論により、マルコフ連鎖がエルゴード的であればサンプリングが定常分布に従う事は分かったが、次は遷移確率の設計が問題となる。遷移確率を規約性と正再帰性と非周期性とを満たすように設定するのは案外容易&lt;sup id="fnref:17"&gt;&lt;a class="footnote-ref" href="#fn:17"&gt;17&lt;/a&gt;&lt;/sup&gt;であるが、それだけでは定常分布の存在のみを保証するので、その定常分布が希望する分布に一致するとは限らない。次に問題となるのは、希望の確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;を定常分布とするように遷移確率を設計することである。その問題は次の &lt;strong&gt;詳細釣り合い条件(detailed balance condition)&lt;/strong&gt; という条件によって解決できる。&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;詳細釣り合い条件&lt;/h4&gt;
&lt;p&gt;希望する確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;と遷移確率&lt;span class="math"&gt;\(p\)&lt;/span&gt;が次の条件を満たす時、そのマルコフ連鎖の定常分布&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;は&lt;span class="math"&gt;\(r\)&lt;/span&gt;に一致する:
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    r_{i} p_{ij} = r_{j} p_{ji}
  $$&lt;/div&gt;
&lt;p&gt;
  ここで&lt;span class="math"&gt;\(r_{i} = r(\boldsymbol{x} = i)\)&lt;/span&gt;である（証明は&lt;a href="#詳細釣り合い条件の証明"&gt;補足&lt;/a&gt;に示した）。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;詳細釣り合い条件を満たす遷移確率を用いさえすれば、十分大きな&lt;span class="math"&gt;\(m&amp;gt;0\)&lt;/span&gt;を取った時に、マルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{m}}, \boldsymbol{x_{m+1}},\dots\)&lt;/span&gt;は&lt;span class="math"&gt;\(r\)&lt;/span&gt;からのサンプルとなる。 次の節で紹介するアルゴリズムの遷移確率は、いずれも詳細釣り合い条件を満たすように設計されている。&lt;/p&gt;
&lt;h3&gt;メトロポリス-ヘイスティングス法&lt;/h3&gt;
&lt;p&gt;メトロポリス-ヘイスティングス法は、サンプルは重点サンプリングの時と同じように提案分布によって生成し、そして新しく生成したサンプルを &lt;strong&gt;採択&lt;/strong&gt;（採用）するか、もしくは &lt;strong&gt;棄却&lt;/strong&gt; （捨てる）のかを &lt;strong&gt;採択確率(acceptance rate)&lt;/strong&gt; と呼ばれる確率によって決め、採択された場合は新しい状態に遷移し、棄却された場合には遷移は行わずに（状態を変えずに）もう一度サンプリングし直す、という手続きを繰り返す手法である。&lt;/p&gt;
&lt;p&gt;メトロポリス-ヘイスティングス法の更新規則を導出してみる。
まず、状態&lt;span class="math"&gt;\(i \in X\)&lt;/span&gt;から状態&lt;span class="math"&gt;\(j \in X\)&lt;/span&gt;に遷移する時の提案分布を条件付き確率&lt;span class="math"&gt;\(q(\boldsymbol{x_{n+1}}=j|\boldsymbol{x_{n}}=i) = q_{ij}\)&lt;/span&gt;と書き、また状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;にいる時に状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;を採択する確率（採択確率）を&lt;span class="math"&gt;\(\alpha(i \to j)\)&lt;/span&gt;と表す。すると、&lt;span class="math"&gt;\(i\)&lt;/span&gt;から&lt;span class="math"&gt;\(j\)&lt;/span&gt;への遷移確率&lt;span class="math"&gt;\(p_{ij}\)&lt;/span&gt;は&lt;span class="math"&gt;\(q_{ij}\)&lt;/span&gt;と&lt;span class="math"&gt;\(\alpha(i \to j)\)&lt;/span&gt;の積で表せる:&lt;/p&gt;
&lt;div class="math"&gt;$$
  p_{ij} = q_{ij} \alpha(i \to j) \tag{4}
$$&lt;/div&gt;
&lt;p&gt;そして、詳細釣り合い条件から、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \frac{p_{ij}}{p_{ji}} = \frac{r_{j}}{r_{i}} &amp;amp;\iff \frac{q_{ij}\alpha(i \to j)}{q_{ji}\alpha(j \to i)} = \frac{r_{j}}{r_{i}} \\
  &amp;amp;\iff \frac{\alpha(i \to j)}{\alpha(j \to i)} = \frac{r_{j}q_{ji}}{r_{i}q_{ij}}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となる。採択確率はこの条件を満たす様に設計する。メトロポリス-ヘイスティングス法では特に、&lt;/p&gt;
&lt;div class="math"&gt;$$
  \alpha(i \to j) = \min \left( 1, \frac{r_{j}q_{ji}}{r_{i}q_{ij}} \right) \tag{5}
$$&lt;/div&gt;
&lt;p&gt;とする&lt;sup id="fnref:18"&gt;&lt;a class="footnote-ref" href="#fn:18"&gt;18&lt;/a&gt;&lt;/sup&gt;。アルゴリズムの実行中には、この式によって採択確率を計算し、&lt;span class="math"&gt;\([0,1]\)&lt;/span&gt;の範囲の一様乱数を発生させて採択/棄却を判断する。&lt;/p&gt;
&lt;p&gt;これでメトロポリス-ヘイスティングス法が実行できるが、その利点を2つ挙げる:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(r\)&lt;/span&gt;が厳密計算出来なくても良い &lt;br&gt; &lt;span class="math"&gt;\(r\)&lt;/span&gt;は一般に複雑なので直接的な計算は難しいが、上の採択確率の式は確率の比率のみに注目している。従って分布が厳密に計算できなくてもアルゴリズムを実行できる。比率さえ一致すれば良いので、分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;の近似分布&lt;span class="math"&gt;\(\hat{r}\)&lt;/span&gt;として
    &lt;div class="math"&gt;$$
      \hat{r} = \frac{1}{Z_{r}} r
    $$&lt;/div&gt;
    としても良い事になる(&lt;span class="math"&gt;\(Z_{r}\)&lt;/span&gt;:正規化定数)。特に、近似分布をボルツマン-ギブス分布
    &lt;div class="math"&gt;$$
      \hat{r}(\boldsymbol{x}) = \frac{1}{Z_{r}} \exp(-r(\boldsymbol{x})/T)
    $$&lt;/div&gt;
    とする場合が多い。ここで、&lt;span class="math"&gt;\(T&amp;gt;0\)&lt;/span&gt;は温度パラメタ&lt;sup id="fnref:19"&gt;&lt;a class="footnote-ref" href="#fn:19"&gt;19&lt;/a&gt;&lt;/sup&gt;である。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(q_{ij} = q_{ji}\)&lt;/span&gt;が成り立つ場合には、より簡単にサンプリングできる&lt;br&gt; &lt;span class="math"&gt;\(q_{ij} = q_{ji}\)&lt;/span&gt;が成立する提案分布で有名なものに&lt;strong&gt;酔歩連鎖(random walk chain)&lt;/strong&gt;がある:
    &lt;div class="math"&gt;$$
    q_{ij} = {\cal N}(i, \sigma^{2}\boldsymbol{I})
    $$&lt;/div&gt;
    即ち平均（中心）を現在状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;、分散を&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;sup id="fnref:20"&gt;&lt;a class="footnote-ref" href="#fn:20"&gt;20&lt;/a&gt;&lt;/sup&gt;
    とした正規分布からの乱択でサンプリングを行う&lt;sup id="fnref:21"&gt;&lt;a class="footnote-ref" href="#fn:21"&gt;21&lt;/a&gt;&lt;/sup&gt;。
    正規分布以外でも、&lt;span class="math"&gt;\(i\)&lt;/span&gt;を平均とした一様分布、多変量&lt;span class="math"&gt;\(t\)&lt;/span&gt;分布でも実行できる。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;ギブスサンプリング&lt;/h3&gt;
&lt;p&gt;ギブスサンプリング(Gibbs Sampling, 熱浴法とも)は提案分布の変数を1個ずつ更新していく手法である。
主に多次元確率分布&lt;sup id="fnref:22"&gt;&lt;a class="footnote-ref" href="#fn:22"&gt;22&lt;/a&gt;&lt;/sup&gt;の推定に用いられる事が多い。説明のため、現在の状態を組&lt;span class="math"&gt;\(\boldsymbol{x} = (x_{1}, x_{2}, \dots, x_{d})\)&lt;/span&gt;と書く。状態の更新の際には、変数を1つ選び出し&lt;sup id="fnref:23"&gt;&lt;a class="footnote-ref" href="#fn:23"&gt;23&lt;/a&gt;&lt;/sup&gt;て&lt;span class="math"&gt;\(x_{i} \to x_{i}^{\prime}\)&lt;/span&gt;と遷移させる(&lt;span class="math"&gt;\(i=1,\dots,d\)&lt;/span&gt;)。更新後の状態を&lt;span class="math"&gt;\(\boldsymbol{x}^{\prime} = (x_{1}, \dots, x_{i-1}, x_{i}^{\prime}, x_{i+1},  \dots, x_{d})\)&lt;/span&gt;と書く。ここで、遷移確率&lt;span class="math"&gt;\(q(\boldsymbol{x}^{\prime}|\boldsymbol{x})\)&lt;/span&gt;は次で定義される:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  q(\boldsymbol{x}^{\prime}|\boldsymbol{x}) &amp;amp;= \frac{r(\boldsymbol{x}^{\prime})}{\sum_{x_{i}} r(\boldsymbol{x})} \\
  &amp;amp;= r(x^{\prime}_{i}|x_{1},\dots,x_{i-1},x_{i+1},\dots,x_{d}) \quad (\because ベイズの定理)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;即ち、選択した変数&lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt;以外を全て``固定''した確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;から&lt;span class="math"&gt;\(x_{i}^{\prime}\)&lt;/span&gt;を新しくサンプリングする。上記右辺が計算できる場合にのみ、ギブスサンプリングは適用可能となる。&lt;/p&gt;
&lt;p&gt;この更新規則が詳細釣り合い条件を満たすことは、再びベイズの定理を用いて、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  r(\boldsymbol{x})q(\boldsymbol{x}^{\prime}|\boldsymbol{x}) &amp;amp;= r(\boldsymbol{x}) r(x^{\prime}_{i}|x_{1},\dots,x_{i-1},x_{i+1},\dots,x_{d}) \\
  &amp;amp;= r(\boldsymbol{x})\frac{r(\boldsymbol{x}^{\prime})}{\sum_{{x}_{i}}r(\boldsymbol{x})} = r(\boldsymbol{x}^{\prime}) \frac{r(\boldsymbol{x})}{\sum_{x_{i}^{\prime}}r(\boldsymbol{x}^{\prime})} \\
  &amp;amp;= r(\boldsymbol{x}^{\prime}) q(\boldsymbol{x}|\boldsymbol{x}^{\prime})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;により確認できる。また、メトロポリス-ヘイスティングス法の採択確率の式から、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \alpha(\boldsymbol{x} \to \boldsymbol{x}^{\prime}) &amp;amp;= \min \left(1, \frac{r(\boldsymbol{x}^{\prime})q(\boldsymbol{x}|\boldsymbol{x}^{\prime})}{r(\boldsymbol{x})q(\boldsymbol{x}^{\prime}|\boldsymbol{x})} \right) \\
  &amp;amp;= \min (1, 1) = 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となり、ギブスサンプリングはメトロポリス-ヘイスティングス法で採択確率を&lt;span class="math"&gt;\(1\)&lt;/span&gt;（必ず採択）するようにした特別の場合である事が分かる。採択/棄却の手順を踏まくくても良く、しかも遷移確率&lt;span class="math"&gt;\(q\)&lt;/span&gt;は予め計算できるので、高速な推定ができるようになっている。&lt;/p&gt;
&lt;h3&gt;MCMCによる最適化&lt;/h3&gt;
&lt;p&gt;MCMCは関数最適化に用いることもできる。今、サンプリングを行う確率分布をボルツマン-ギブス分布&lt;/p&gt;
&lt;div class="math"&gt;$$
  r(\boldsymbol{x}) = \frac{1}{Z_{r}} \exp(-f(\boldsymbol{x})/T)
$$&lt;/div&gt;
&lt;p&gt;とした時、定義式により、&lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;が小さな値を与える点ではその確率&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;は同時に大きくことが即座に観察できる。従って、MCMCによって&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;からのサンプリングを行えば、&lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;が小さな値をとる点を集中してサンプリングできる事から、&lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;の最小化（最大化の場合は&lt;span class="math"&gt;\(-f(\boldsymbol{x})\)&lt;/span&gt;の最小化に置き換えれば良い）を考える事ができる。実際、関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;の最小値を与える点を&lt;span class="math"&gt;\(\boldsymbol{x}^{\ast}\)&lt;/span&gt;と表せば、サンプル数&lt;span class="math"&gt;\(N\)&lt;/span&gt;の極限において最小値&lt;span class="math"&gt;\(f(\boldsymbol{x}^{\ast})\)&lt;/span&gt;が確率1で得られる事:&lt;/p&gt;
&lt;div class="math"&gt;$$
  \lim_{N \to \infty} P(\min(f(\boldsymbol{x_{1}}), f(\boldsymbol{x_{2}}), \dots, f(\boldsymbol{x_{N}})) = f(\boldsymbol{x}^{\ast})) = 1
$$&lt;/div&gt;
&lt;p&gt;が示せる。以下、その証明を示す。&lt;/p&gt;
&lt;p&gt;（証明）
MCMCにおいて、定常分布を&lt;span class="math"&gt;\(r\)&lt;/span&gt;とする様に（詳細釣り合い条件を満たす様に）サンプリングを行う。この時マルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{1}}, \boldsymbol{x_{2}}, \dots, \boldsymbol{x_{n}},\dots\)&lt;/span&gt;は、十分大きな&lt;span class="math"&gt;\(n &amp;gt; 1\)&lt;/span&gt;においては&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;からのサンプルとみなせる。関数&lt;span class="math"&gt;\(f\)&lt;/span&gt;に最小値&lt;span class="math"&gt;\(f(\boldsymbol{x}^{\ast})\)&lt;/span&gt;が存在すれば、&lt;span class="math"&gt;\(\boldsymbol{x}^{\ast}\)&lt;/span&gt;をサンプリングする確率&lt;span class="math"&gt;\(r(\boldsymbol{x}^{\ast})\)&lt;/span&gt;も存在が保証され、分布の中で最大の確率を与えている。従って、&lt;span class="math"&gt;\(n\)&lt;/span&gt;回目以降のマルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{n}}, \boldsymbol{x_{n+1}},\dots\)&lt;/span&gt;において、&lt;span class="math"&gt;\(m \geq n\)&lt;/span&gt;回目に初めて&lt;span class="math"&gt;\(\boldsymbol{x}^{\ast}\)&lt;/span&gt;がサンプリングできる確率&lt;span class="math"&gt;\(P(\boldsymbol{x_{m}} = \boldsymbol{x}^{\ast})\)&lt;/span&gt;は、幾何分布と同じ様に、&lt;/p&gt;
&lt;div class="math"&gt;$$
  P(\boldsymbol{x_{m}} = \boldsymbol{x}^{\ast}) = r(\boldsymbol{x})\left\{ 1-r(\boldsymbol{x}^{\ast}) \right\}^{m-n}
$$&lt;/div&gt;
&lt;p&gt;によって計算できる。また、初めて&lt;span class="math"&gt;\(\boldsymbol{x^{\ast}}\)&lt;/span&gt;がサンプリングできるまでの回数が&lt;span class="math"&gt;\(N \geq n\)&lt;/span&gt;回以内となる確率は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  P(\boldsymbol{x_{n}} = \boldsymbol{x}^{\ast}) + P(\boldsymbol{x_{n+1}} = \boldsymbol{x}^{\ast}) + \dots + P(\boldsymbol{x_{N}} = \boldsymbol{x}^{\ast}) &amp;amp;=  \sum_{m=n}^{N} P(\boldsymbol{x_{N}} = \boldsymbol{x}^{\ast}) \\
  &amp;amp;= \sum_{k=0}^{N-n} r(\boldsymbol{x}^{\ast})\left\{ 1-r(\boldsymbol{x}^{\ast}) \right\}^{k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;となる。 ここでサンプル数の極限&lt;span class="math"&gt;\(N \to \infty\)&lt;/span&gt;をとると、初項&lt;span class="math"&gt;\(r(\boldsymbol{x}^{\ast})\)&lt;/span&gt;、項比&lt;span class="math"&gt;\(1-r(\boldsymbol{x}^{\ast})\)&lt;/span&gt;とした等比級数の和の公式より、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
  \lim_{N \to \infty} \sum_{k=0}^{N-n} r(\boldsymbol{x}^{\ast})\left\{ 1-r(\boldsymbol{x}^{\ast}) \right\}^{k} &amp;amp;= \frac{r(\boldsymbol{x}^{\ast})}{1-\left\{1-r(\boldsymbol{x}^{\ast})\right\}} = 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が得られる。即ち、サンプリングを無限に繰り返せば&lt;span class="math"&gt;\(\boldsymbol{x}^{\ast}\)&lt;/span&gt;が確率1で得られることが示された。この結果は、サンプルの関数列&lt;span class="math"&gt;\(f(\boldsymbol{x_{1}}), f(\boldsymbol{x_{2}}), \dots\)&lt;/span&gt;の中に少なくとも1つ&lt;span class="math"&gt;\(f(\boldsymbol{x}^{\ast})\)&lt;/span&gt;が存在する事と同値である。&lt;/p&gt;
&lt;h4&gt;焼きなまし法&lt;/h4&gt;
&lt;p&gt;以上でMCMCによる最適化が理論的に可能なことが示されたが、最適化の際に特に問題となるのは分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;の温度パラメタ&lt;span class="math"&gt;\(T\)&lt;/span&gt;である。&lt;span class="math"&gt;\(T\)&lt;/span&gt;が大きければ、&lt;span class="math"&gt;\(\exp\)&lt;/span&gt;内部の&lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;の値に影響されず&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;は一様分布に近くなり、一様乱数からのサンプリングと殆ど変わらなくなる。逆に&lt;span class="math"&gt;\(T\)&lt;/span&gt;が&lt;span class="math"&gt;\(0\)&lt;/span&gt;に近いと&lt;span class="math"&gt;\(r(\boldsymbol{x})\)&lt;/span&gt;は&lt;span class="math"&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;の値に大きく影響されるが、サンプリングが特定の場所だけに集中してしまって局所最適値しか得られない場合がある。この様に&lt;span class="math"&gt;\(T\)&lt;/span&gt;は適切に決定する必要があるが、&lt;span class="math"&gt;\(T\)&lt;/span&gt;の適切な決定法は存在せず、問題依存となる場合が多い。&lt;/p&gt;
&lt;p&gt;そこで、最初は&lt;span class="math"&gt;\(T\)&lt;/span&gt;（温度）を高い状態から初めてサンプリングの度に少しずつ&lt;span class="math"&gt;\(T\)&lt;/span&gt;を下げていくやり方があり、これを焼きなまし法（Simulated Annealing, SA）と呼ぶ。この様に&lt;span class="math"&gt;\(T\)&lt;/span&gt;を変化させると最初は空間全体の中から大雑把な&lt;span class="math"&gt;\(f\)&lt;/span&gt;の値を取得し、後に最適値の近傍を集中してサンプリングすることができるために効率的な探索が期待できる。証明は省くが、温度パラメタの系列&lt;span class="math"&gt;\(T_{1}, T_{2}, \dots\)&lt;/span&gt;が次の条件を満たせばサンプリングによって&lt;span class="math"&gt;\(\boldsymbol{x}^{\ast}\)&lt;/span&gt;が得られる事（収束定理）が示されている:&lt;/p&gt;
&lt;div class="math"&gt;$$
  \sum_{n=1}^{\infty} \exp(-D/T_{n}) = \infty
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;span class="math"&gt;\(D\)&lt;/span&gt;は問題によって決まる定数である。&lt;/p&gt;
&lt;h1&gt;補足&lt;/h1&gt;
&lt;h2&gt;エルゴード的なマルコフ連鎖の定常分布&lt;/h2&gt;
&lt;p&gt;上記の議論で、「マルコフ連鎖がエルゴード的ならば、一意な定常分布が存在する」という事に触れた。この定理についての証明を述べていくが、準備として確率過程についての用語や記法の定義、基本的な定理の証明を行う。大方の証明は&lt;a href="http://www-lsm.naist.jp/~kasahara/lecture/isp/part1.pdf"&gt;ここ&lt;/a&gt;を参照した。なお、状態空間（全事象）&lt;span class="math"&gt;\(X\)&lt;/span&gt;は有限集合であるとする。&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;離散時間マルコフ連鎖&lt;/h4&gt;
&lt;p&gt;確率過程（サンプル列） &lt;span class="math"&gt;\(\boldsymbol{x_{0}}, \boldsymbol{x_{1}}, \dots\)&lt;/span&gt; が次を満たす時、離散時間マルコフ連鎖という。
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    \forall n \geq 0, \forall i_{0}, \dots, i_{n+1} \in X.\ P(\boldsymbol{x_{n+1}} = i_{n+1} |\boldsymbol{x_{0}} = i_{0}, \boldsymbol{x_{1}} = i_{1}, \dots, \boldsymbol{x_{n}} = i_{n}) = P(\boldsymbol{x_{n+1}}=i_{n+1}|\boldsymbol{x_{n}}=i_{n})
  $$&lt;/div&gt;
&lt;p&gt;
  またこの性質をマルコフ性という。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;遷移確率の斉時性、nステップ遷移確率&lt;/h4&gt;
&lt;p&gt;任意の状態&lt;span class="math"&gt;\(i,j \in X\)&lt;/span&gt;と非負整数&lt;span class="math"&gt;\(n \geq 0\)&lt;/span&gt;に対して
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    p_{ij}(n) = P(\boldsymbol{x_{n+1}}=j|\boldsymbol{x_{n}}=i)
  $$&lt;/div&gt;
&lt;p&gt;
  を、状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;から状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;への遷移確率という。&lt;span class="math"&gt;\(p_{ij}(n)\)&lt;/span&gt;が&lt;span class="math"&gt;\(n\)&lt;/span&gt;と独立で常に&lt;span class="math"&gt;\(p_{ij}(n) = p_{ij}(0) = p_{ij}\)&lt;/span&gt;となる時、離散時間マルコフ連鎖は斉時であるという。今後、遷移確率は&lt;span class="math"&gt;\(p_{ij}\)&lt;/span&gt;を用いて表す。また、
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    p_{ij}^{(n)} = P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{0}}=i)
  $$&lt;/div&gt;
&lt;p&gt;
  は状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;から始まって&lt;span class="math"&gt;\(n\)&lt;/span&gt;ステップ後に状態が&lt;span class="math"&gt;\(j\)&lt;/span&gt;になる確率を表しており、&lt;span class="math"&gt;\(n\)&lt;/span&gt;ステップ遷移確率と呼ぶ。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;チャップマン−コルモゴロフ方程式&lt;/h4&gt;
&lt;p&gt;任意の状態&lt;span class="math"&gt;\(i,j \in X\)&lt;/span&gt;に対し、&lt;span class="math"&gt;\(n\)&lt;/span&gt;ステップ遷移確率&lt;span class="math"&gt;\(p_{ij}^{(n)}\)&lt;/span&gt;は次を満たす:
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    p_{ij}^{(n)} = \sum_{r \in X} p_{ir}^{(k)}p_{rj}^{(n-k)} \quad 0 \leq k \leq n
  $$&lt;/div&gt;
&lt;p&gt;
  （証明）&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    p_{ij}^{(n)} &amp;amp;= P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{0}}=i) =  \sum_{r \in X} P(\boldsymbol{x_{n}}=j, \boldsymbol{x_{k}}=r|\boldsymbol{x_{0}}=i) \quad (\because 確率分布の周辺化) \\
    &amp;amp;= \sum_{r\in S} P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{k}}=r, \boldsymbol{x_{0}}=i) P(\boldsymbol{x_{k}}=r|\boldsymbol{x_{0}}=i) \quad (\because ベイズの定理) \\
    &amp;amp;= \sum_{r\in S} P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{k}}=r) P(\boldsymbol{x_{k}}=r|\boldsymbol{x_{0}}=i) \quad (\because マルコフ性) \\
    &amp;amp;= \sum_{r\in S} P(\boldsymbol{x_{n-k}}=j|\boldsymbol{x_{0}}=r) P(\boldsymbol{x_{k}}=r|\boldsymbol{x_{0}}=i) \quad (\because 斉時性) \\
    &amp;amp;= \sum_{r\in S} p_{rj}^{(n-k)}p_{ir}^{(k)}
\end{align}
$$&lt;/div&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;到達可能、連結&lt;/h4&gt;
&lt;p&gt;ある状態&lt;span class="math"&gt;\(i,j \in X\)&lt;/span&gt;に対して&lt;span class="math"&gt;\(p_{ij}^{(n)} &amp;gt; 0\)&lt;/span&gt;なる非負整数&lt;span class="math"&gt;\(n \geq 0\)&lt;/span&gt;が存在する時、状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;は状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;から到達可能であると言い、&lt;span class="math"&gt;\(i\to j\)&lt;/span&gt;と表す。
  また&lt;span class="math"&gt;\(i \to j \land j \to i\)&lt;/span&gt;ならば、&lt;span class="math"&gt;\(i\)&lt;/span&gt;と&lt;span class="math"&gt;\(j\)&lt;/span&gt;は連結しているといい、&lt;span class="math"&gt;\(i \leftrightarrow j\)&lt;/span&gt;と表す。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;連結関係は、反射性(&lt;span class="math"&gt;\(i \leftrightarrow i\)&lt;/span&gt;)、対称性(&lt;span class="math"&gt;\(i \leftrightarrow j \Leftrightarrow j \leftrightarrow i\)&lt;/span&gt;)、推移性(&lt;span class="math"&gt;\(i \leftrightarrow j \land j \leftrightarrow k \Rightarrow i \leftrightarrow k\)&lt;/span&gt;)が成り立つ。&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;連結クラス（連結成分）&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;の部分集合&lt;span class="math"&gt;\(C \subseteq X\)&lt;/span&gt;において、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(i \in C \land j \in C \implies i \leftrightarrow j\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(i \in C \land i \leftrightarrow j \implies j \in C\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;が常に成立する時、&lt;span class="math"&gt;\(C\)&lt;/span&gt;を&lt;span class="math"&gt;\(X\)&lt;/span&gt;の連結クラス（連結成分）という。定義より、&lt;span class="math"&gt;\(C\)&lt;/span&gt;の要素は互いに連結している。また、連結クラス&lt;span class="math"&gt;\(C\)&lt;/span&gt;の任意の状態&lt;span class="math"&gt;\(i \in C\)&lt;/span&gt;から&lt;span class="math"&gt;\(j \notin C\)&lt;/span&gt;に到達できない時、&lt;span class="math"&gt;\(C\)&lt;/span&gt;は閉じていると言う。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;規約性&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;内の全ての状態が単一の閉じた連結クラスに属する、即ち&lt;span class="math"&gt;\(X\)&lt;/span&gt;の全ての要素が互いに連結している時、そのマルコフ連鎖は規約であるという。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;周期性&lt;/h4&gt;
&lt;p&gt;状態&lt;span class="math"&gt;\(i \in X\)&lt;/span&gt;に対して&lt;span class="math"&gt;\(p_{ii}^{(n)} &amp;gt; 0\)&lt;/span&gt;となる（&lt;span class="math"&gt;\(n\)&lt;/span&gt;ステップ後に元の状態に戻る）&lt;span class="math"&gt;\(n\)&lt;/span&gt;の最大公約数&lt;span class="math"&gt;\(d\)&lt;/span&gt;を、状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;の周期と呼ぶ。&lt;span class="math"&gt;\(d = 1\)&lt;/span&gt;の時は状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;は非周期的と呼ばれ、&lt;span class="math"&gt;\(d \geq 2\)&lt;/span&gt;の時は周期的であると呼ばれる。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;再帰的、過渡的&lt;/h4&gt;
&lt;p&gt;確率変数&lt;span class="math"&gt;\(T_{j}\)&lt;/span&gt;を次で定義する:&lt;/p&gt;
&lt;div class="math"&gt;$$
    T_{j} = \min_{n} \{ n &amp;gt; 0 | \boldsymbol{x_{n}} = j \}
$$&lt;/div&gt;
&lt;p&gt;即ち、離散時間マルコフ連鎖が初めて状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;を訪れる時刻を表す。また、&lt;span class="math"&gt;\(T_{i}\)&lt;/span&gt;を用いて次の値を定義する:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    f_{i} &amp;amp;= P(T_{i} &amp;lt; \infty | \boldsymbol{x_{0}} = i) = \sum_{n=1}^{\infty}P(T_{i} = n|\boldsymbol{x_{0}}=i) \\
    m_{i} &amp;amp;= \mathrm{E}[T_{i}|\boldsymbol{x_{0}}=i] = \sum_{k=0}^{\infty} k P(T_{i}=k|\boldsymbol{x_{0}}=i)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f_{i}\)&lt;/span&gt;は将来状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;に戻ってくる確率を表しており、&lt;span class="math"&gt;\(f_{i}=1\)&lt;/span&gt;ならば確率&lt;span class="math"&gt;\(1\)&lt;/span&gt;で状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;を訪れる（無限にしばしば訪れる）ので状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;は再帰的であるという。&lt;span class="math"&gt;\(f_{i} &amp;lt; 1\)&lt;/span&gt;ならば状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;は過渡的であるという。また、&lt;span class="math"&gt;\(m_{i}\)&lt;/span&gt;は初期状態が&lt;span class="math"&gt;\(i\)&lt;/span&gt;の時に、再び状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;に戻るまでの時間の期待値を表しており、&lt;span class="math"&gt;\(m_{i} &amp;lt; \infty\)&lt;/span&gt;ならば状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;は正再帰的（有限時間で&lt;span class="math"&gt;\(i\)&lt;/span&gt;に戻る）であるといい、&lt;span class="math"&gt;\(m_{i} = \infty\)&lt;/span&gt;ならば状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;は零再帰的であるという。&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h4&gt;エルゴード的な離散時間マルコフ連鎖&lt;/h4&gt;
&lt;p&gt;離散時間マルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{0}}, \boldsymbol{x_{1}},\dots\)&lt;/span&gt;が規約かつ正再帰かつ非周期的であるならば、この離散時間マルコフ連鎖はエルゴード的とも呼ばれる&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ここまでで用語の定義は揃ったので、それではエルゴード的なマルコフ連鎖の定常分布の存在についての定理を証明する。&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;エルゴード的な離散時間マルコフ連鎖の定常分布&lt;/h4&gt;
&lt;p&gt;離散時間マルコフ連鎖&lt;span class="math"&gt;\(\boldsymbol{x_{0}}, \boldsymbol{x_{1}}, \dots\)&lt;/span&gt;がエルゴード的ならば、任意の状態&lt;span class="math"&gt;\(i, j \in X\)&lt;/span&gt;について次が成り立つ:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\displaystyle\lim_{n \to \infty} p_{ij}^{(n)} = \lim_{n \to \infty} p_{jj}^{(n)} = \frac{1}{m_{j}} = \pi_{j}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\pi_{j}\)&lt;/span&gt;は&lt;span class="math"&gt;\(\displaystyle \pi_{j} = \sum_{i \in X} \pi_{i} p_{ij}\)&lt;/span&gt;と&lt;span class="math"&gt;\(\displaystyle\sum_{j \in X}\pi_{j} = 1\)&lt;/span&gt;を満たす解であり、唯一に定まる。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2.を満たす&lt;span class="math"&gt;\(\pi_{j}\)&lt;/span&gt;を極限分布（定常状態分布）と言う。&lt;/p&gt;
&lt;p&gt;一方、初期分布として&lt;span class="math"&gt;\(P(\boldsymbol{x_{0}} = j) = \pi_{j}\)&lt;/span&gt;を持つ離散時間マルコフ連鎖では、任意の&lt;span class="math"&gt;\(n \geq 1\)&lt;/span&gt;に対して&lt;span class="math"&gt;\(P(\boldsymbol{x_{n}}=j) = \pi_{j}\)&lt;/span&gt;が成り立ち、&lt;span class="math"&gt;\(\boldsymbol{x_{n}}\)&lt;/span&gt;は&lt;span class="math"&gt;\(n\)&lt;/span&gt;と独立した分布を持つ。この様に、時間に関して不変な分布&lt;span class="math"&gt;\(\pi_{j} = P(\boldsymbol{x_{n}} = j)\ n = 0,1,\dots\)&lt;/span&gt;を &lt;strong&gt;定常分布&lt;/strong&gt; と呼ぶ。&lt;/p&gt;
&lt;p&gt;（証明）まず1.から考える。最初に&lt;span class="math"&gt;\(i\neq j\)&lt;/span&gt;なる状態に対して
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    u_{k} = P(T_{j} = k|\boldsymbol{x_{0}} = i)
  $$&lt;/div&gt;
&lt;p&gt;
  を（初期状態が&lt;span class="math"&gt;\(i\)&lt;/span&gt;で、初めて&lt;span class="math"&gt;\(j\)&lt;/span&gt;に訪れる時刻が&lt;span class="math"&gt;\(k\)&lt;/span&gt;となる確率）おく。この時、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    p_{ij}^{(1)} &amp;amp;= u_{1} \\
    p_{ij}^{(2)} &amp;amp;= u_{2} + u_{1} p_{jj}^{(1)} \\ 
    p_{ij}^{(3)} &amp;amp;= u_{3} + u_{2}p_{jj}^{(1)} + u_{1}p_{jj}^{(2)} \\
    &amp;amp;\vdots
\end{align}
$$&lt;/div&gt;
&lt;p&gt;の観察により、&lt;span class="math"&gt;\(n \geq 1\)&lt;/span&gt;なる&lt;span class="math"&gt;\(n\)&lt;/span&gt;に対して帰納的に
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    p_{ij}^{(n)} = \sum_{k=1}^{n} u_{k} p_{jj}^{(n-k)}
  $$&lt;/div&gt;
&lt;p&gt;
  が成立する（最初の&lt;span class="math"&gt;\(k\)&lt;/span&gt;ステップで状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;に行き、その後&lt;span class="math"&gt;\(n-k\)&lt;/span&gt;ステップ後に再び&lt;span class="math"&gt;\(j\)&lt;/span&gt;に行く）ことが分かる。また、任意の&lt;span class="math"&gt;\(i\)&lt;/span&gt;と&lt;span class="math"&gt;\(j\)&lt;/span&gt;は連結している（&lt;span class="math"&gt;\(i \leftrightarrow j\)&lt;/span&gt;）ので、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \sum_{k=1}^{\infty} u_{k} = P(\exists n \geq 0.\ \boldsymbol{x_{n}} = j | \boldsymbol{x_{0}} =i) = 1
$$&lt;/div&gt;
&lt;p&gt;（状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;から始まり、&lt;span class="math"&gt;\(j\)&lt;/span&gt;へいつかは訪れる確率は&lt;span class="math"&gt;\(1\)&lt;/span&gt;）が成り立つ。一方&lt;span class="math"&gt;\(p_{jj}^{(n)}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    p_{jj}^{(n)} &amp;amp;= P(\boldsymbol{x_{n}} = j|\boldsymbol{x_{0}}=j) \\
    &amp;amp;= \sum_{k=1}^{n}P(\boldsymbol{x_{n}}=j, T_{j} = k|\boldsymbol{x_{0}}=j) \quad (\because 確率分布の周辺化) \\ 
    &amp;amp;= \sum_{k=1}^{n}P(\boldsymbol{x_{n}}=j|T_{j}=k, \boldsymbol{x_{0}}=j)P(T_{j}=k|\boldsymbol{x_{0}}=j) \quad (\because ベイズの定理) \\
    &amp;amp;= \sum_{k=1}^{n}P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{k}}=j, \boldsymbol{x_{0}}=j)P(T_{j}=k|\boldsymbol{x_{0}}=j) \quad (\because T_{j} = k \implies \boldsymbol{x_{k}} = j) \\
    &amp;amp;= \sum_{k=1}^{n}P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{k}}=j)P(T_{j}=k|\boldsymbol{x_{0}}=j) \quad (\because マルコフ性) \\
    &amp;amp;= \sum_{k=1}^{n}p_{jj}^{(n-k)} u_{k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;と展開できる。数列&lt;span class="math"&gt;\(p_{jj}^{(n)}\)&lt;/span&gt;の極限&lt;span class="math"&gt;\(\displaystyle\lim_{n \to \infty} p_{jj}^{(n)}\)&lt;/span&gt;を求める為、ここでは数列の &lt;strong&gt;母関数&lt;/strong&gt;を定義し、（片側）Z変換の最終値定理 &lt;sup id="fnref:24"&gt;&lt;a class="footnote-ref" href="#fn:24"&gt;24&lt;/a&gt;&lt;/sup&gt; を用いる。その為、今、&lt;span class="math"&gt;\(\displaystyle G(z) = \sum_{n=0}^{\infty}p_{jj}^{(n)}z^{n},\ U(z) = \sum_{n=1}^{\infty}u_{n}z^{n}\)&lt;/span&gt;なる母関数を定義し、上式の両辺に&lt;span class="math"&gt;\(z^{n}\)&lt;/span&gt;を掛けて&lt;span class="math"&gt;\(n=1,2,\dots\)&lt;/span&gt;についての和を取ると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    （左辺）\sum_{n=1}^{\infty} p_{jj}^{(n)}z^{n} &amp;amp;= \sum_{n=1}^{\infty} p_{jj}^{(n)}z^{n} = \sum_{n=0}^{\infty}p_{jj}^{(n)}z^{n} - p_{jj}^{(0)} \\
    &amp;amp;= G(z) - 1 \\
    （右辺）\sum_{n=1}^{\infty} \sum_{k=1}^{n} p_{jj}^{(n-k)}u_{k}z^{n} &amp;amp;= \sum_{n=1}^{\infty} \sum_{k=1}^{n} p_{jj}^{(n-k)}z^{n-k}u_{k}z^{k} \\
    &amp;amp;= G(z)U(z) \\
    \therefore G(z) &amp;amp;= \frac{1}{1-U(z)}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;ここで、右辺式の最後の式変形には冪級数の積の公式&lt;sup id="fnref:25"&gt;&lt;a class="footnote-ref" href="#fn:25"&gt;25&lt;/a&gt;&lt;/sup&gt;を用いている。最終値定理を適用する事を考えると、この場合は、
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    \lim_{n \to \infty} p_{jj}^{(n)} = \lim_{z \to 1}(1-z)G(z)
  $$&lt;/div&gt;
&lt;p&gt;
  が成立する&lt;sup id="fnref:26"&gt;&lt;a class="footnote-ref" href="#fn:26"&gt;26&lt;/a&gt;&lt;/sup&gt; ので、&lt;span class="math"&gt;\(\displaystyle\lim_{n \to \infty} p_{jj}^{(n)}\)&lt;/span&gt;の結果として、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \lim_{n \to \infty} p_{jj}^{(n)} &amp;amp;= \lim_{z \to 1}(1-z)G(z) = \lim_{z \to 1}\frac{1-z}{1-U(z)} \\
    &amp;amp;= \lim_{z \to 1}\frac{\frac{d(1-z)}{dz}}{\frac{d(1-U(z))}{dz}} \quad (\because ロピタルの定理) \\
    &amp;amp;= \lim_{z \to 1}\frac{1}{\frac{dU(z)}{dz}} = \frac{1}{m_{j}} = \pi_{j} \\
    \because \lim_{z \to 1} \frac{dU(z)}{dz} &amp;amp;= \lim_{z \to 1}\sum_{n=1}^{\infty}n u_{n} z^{n-1} = \lim_{z \to 1}\sum_{n=0}^{\infty} n u_{n} z^{n} = \sum_{n=0}^{\infty}n u_{n} = m_{j}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が得られる。さて、この結果より、任意の正数&lt;span class="math"&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;に対して&lt;span class="math"&gt;\(n \geq N\)&lt;/span&gt;なる全ての&lt;span class="math"&gt;\(n\)&lt;/span&gt;が
  &lt;/p&gt;
&lt;div class="math"&gt;$$
    |p_{jj}^{(n)} - \pi_{j}| \leq \frac{\epsilon}{2} \quad かつ \quad \sum_{k = N+1}^{\infty} u_{k} \leq \frac{\epsilon}{2}
  $$&lt;/div&gt;
&lt;p&gt;
  を同時に満たすような&lt;span class="math"&gt;\(N\)&lt;/span&gt;を取ることができる。今、&lt;span class="math"&gt;\(n \geq 2N\)&lt;/span&gt;に対し、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    |p_{ij}^{(n)} - \pi_{j}| &amp;amp;= | \sum_{k=1}^{n} u_{k} p_{jj}^{(n-k)} - \pi_{j}| = | \sum_{k=1}^{n} u_{k} p_{jj}^{(n-k)} - \sum_{k=1}^{\infty}u_{k}\pi_{j}| \\
    &amp;amp;= |\sum_{k=1}^{n-N}u_{k}(p_{jj}^{(n-k)}-\pi_{j}) + \sum_{k=n-N+1}^{n} u_{k}(p_{jj}^{(n-k)} - \pi_{j}) -\sum_{k=n+1}^{\infty}u_{k}\pi_{j}| \\
    &amp;amp;\leq \sum_{k=1}^{n-N}u_{k}|p_{jj}^{(n-k)}-\pi_{j}| + \sum_{k=n-N+1}^{n} u_{k}|p_{jj}^{(n-k)} - \pi_{j}| + \sum_{k=n+1}^{\infty}|u_{k}\pi_{j}| \\
    &amp;amp;\leq \sum_{k=1}^{n-N}u_{k}\frac{\epsilon}{2} + \sum_{k=n-N+1}^{n} u_{k} + \sum_{k=n+1}^{\infty}u_{k} = \frac{\epsilon}{2}\sum_{k=1}^{n-N}u_{k} + \sum_{k=n-N+1}^{\infty} u_{k} \\
    &amp;amp;\leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align}
$$&lt;/div&gt;
&lt;p&gt;よって、&lt;span class="math"&gt;\(\displaystyle \lim_{n \to \infty} p_{ij}^{(n)} = \pi_{j} = \lim_{n \to \infty} p_{jj}^{(n)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;次に2. の&lt;span class="math"&gt;\(\pi_{j}\)&lt;/span&gt;の一意性を示す。まず、&lt;span class="math"&gt;\(\displaystyle \sum_{j \in X}p_{ij}^{(n)} = 1\)&lt;/span&gt;（どこかの状態には確率1で遷移している）より、この式で&lt;span class="math"&gt;\(n \to \infty\)&lt;/span&gt;ならしめれば、1. により&lt;/p&gt;
&lt;div class="math"&gt;$$
    \sum_{j \in X} \pi_{j} = 1
  $$&lt;/div&gt;
&lt;p&gt;を得る。また、&lt;span class="math"&gt;\(a_{j}(n) = P(\boldsymbol{x_{n}} = j)\)&lt;/span&gt;（時刻&lt;span class="math"&gt;\(n\)&lt;/span&gt;で状態&lt;span class="math"&gt;\(j\)&lt;/span&gt;を訪れる確率）とおくと、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    a_{j}(n) &amp;amp;= \sum_{i \in X}P(\boldsymbol{x_{0}}=i)P(\boldsymbol{x_{n}}=j|\boldsymbol{x_{0}}=i) = \sum_{i \in X}P(\boldsymbol{x_{0}}=i)p_{ij}^{(n)} \\
    \therefore \lim_{n \to \infty} a_{j}(n) &amp;amp;= \sum_{i \in X}P(\boldsymbol{x_{0}}=i) \lim_{n \to \infty}p_{ij}^{(n)} = \pi_{j} \sum_{i \in X} P(\boldsymbol{x_{0}} = i) = \pi_{j}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;が成立し、チャップマン−コルモゴロフ方程式により、&lt;span class="math"&gt;\(n, m \geq0\)&lt;/span&gt;なる整数に対し、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    a_{j}(m+n) &amp;amp;= \sum_{r \in X}P(\boldsymbol{x_{0}}=r)P(\boldsymbol{x_{m+n}}=j|\boldsymbol{x_{0}}=r) = \sum_{r \in X} P(\boldsymbol{x_{0}} = r) p_{rj}^{(m+n)} \\
    &amp;amp;= \sum_{r \in X} P(\boldsymbol{x_{0}}=r) \sum_{i \in X} p_{ri}^{(m)}p_{ij}^{(n)} \quad (\because チャップマン−コルモゴロフ方程式を使用) \\
    &amp;amp;= \sum_{i \in X}\sum_{r \in X}P(\boldsymbol{x_{0}}=r)p_{ri}^{(m)} p_{ij}^{(n)} = \sum_{i \in X} a_{i}(m) p_{ij}^{(n)}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;この式の両辺を&lt;span class="math"&gt;\(m \to \infty\)&lt;/span&gt;ならしめれば、極限と和の交換法則より、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \pi_{j} = \sum_{i \in X}\pi_{i} p_{ij}^{(n)}
  $$&lt;/div&gt;
&lt;p&gt;を得る。特に&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;とすれば、&lt;span class="math"&gt;\(\displaystyle \pi_{j} = \sum_{i \in X} \pi_{j} p_{ij}\)&lt;/span&gt;が得られる。
  次に一意性を示す。今、&lt;span class="math"&gt;\(\pi_{i}^{\prime}\ (i \in X)\)&lt;/span&gt;が、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \pi_{j}^{\prime} = \sum_{i \in X}\pi_{i}^{\prime} p_{ij} \quad かつ \quad \sum_{i \in X} \pi_{i}^{\prime} = 1
  $$&lt;/div&gt;
&lt;p&gt;を満たすとする。上述の議論により、全ての正整数&lt;span class="math"&gt;\(n \geq 0\)&lt;/span&gt;に対し、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \pi_{j}^{\prime} = \sum_{i \in X}\pi_{i}^{\prime} p_{ij}^{(n)}
  $$&lt;/div&gt;
&lt;p&gt;を得る。&lt;span class="math"&gt;\(n \to \infty\)&lt;/span&gt;とすると、&lt;/p&gt;
&lt;div class="math"&gt;$$
    \pi_{j}^{\prime} = \left(\sum_{i \in X}\pi_{i}^{\prime}\right) \pi_{j} = \pi_{j}
  $$&lt;/div&gt;
&lt;p&gt;となって、一意性が示される。&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;詳細釣り合い条件の証明&lt;/h3&gt;
&lt;p&gt;最後に詳細釣り合い条件を示す。今、確率分布&lt;span class="math"&gt;\(r\)&lt;/span&gt;と遷移確率が&lt;/p&gt;
&lt;div class="math"&gt;$$
  r_{i} p_{ij} = r_{j} p_{ji}
$$&lt;/div&gt;
&lt;p&gt;を満たしているとする。この時両辺ともに状態&lt;span class="math"&gt;\(i\)&lt;/span&gt;について和をとると、&lt;/p&gt;
&lt;div class="math"&gt;$$
  \sum_{i \in X} r_{i} p_{ij} = r_{j} \sum_{i \in X} p_{ji} = r_{j}
$$&lt;/div&gt;
&lt;p&gt;2.により、&lt;span class="math"&gt;\(r\)&lt;/span&gt;は定常分布の解となっている事が分かる。&lt;/p&gt;
&lt;p&gt;プログラミングに関係ない記事感じるんでしたよね？&lt;/p&gt;
&lt;h2&gt;脚注&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://ebsa.ism.ac.jp/ebooks/sites/default/files/ebook/1881/pdf/vol3_ch10.pdf"&gt;古澄英雄, 「21世紀の統計科学」第Ⅲ巻 第10章 マルコフ連鎖モンテカルロ法入門&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="http://www-lsm.naist.jp/~kasahara/lecture/isp/part1.pdf"&gt;笠原正治, 確率過程論基礎&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/SML1/sampling1.pdf"&gt;中川裕志, マルコフ連鎖モンテカルロ法&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;a href="http://maildbs.c.u-tokyo.ac.jp/~fukushima/FSwiki/wiki.cgi?action=ATTACH&amp;amp;page=%BD%B8%C3%E6%B9%D6%B5%C1%A1%F7%C5%EC%B9%A9%C2%E7&amp;amp;file=TIT-2005-huku.pdf"&gt;福島孝治, マルコフ連鎖モンテカルロ法の実践&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;a href="http://www.slideshare.net/teramonagi/ss-5190440"&gt;tera monagi, マルコフ連鎖モンテカルロ法入門-1&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;主に、確率分布の平均（期待値）、分散が対象となる&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;十分な回数の独立な試行を行った経験分布は理論的（真の）分布に一致する、という法則。例えばコイン投げをひたすら繰り返せば、表及び裏が出る &lt;strong&gt;頻度の比率&lt;/strong&gt; はそれぞれ&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;に近づいていく。厳密には大数の法則は2種類（強、弱法則）あり、確率の応用において非常に非常に重要な法則であるが、ここでは説明をしない。&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;確率変数のとる値が実数値でなくとも、事象が有限個存在（&lt;span class="math"&gt;\(\iff\)&lt;/span&gt;全事象が有限集合）する場合（例。サイコロとかコインを投げる試行）は議論で用いている分布を離散確率分布で考えれば良い。&amp;#160;&lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;起こりえる全ての事象の集合。&amp;#160;&lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;他の個人的に興味深い例：強化学習において&lt;span class="math"&gt;\(X\)&lt;/span&gt;を選択した行動列の集合、&lt;span class="math"&gt;\(h:X \to \mathbb{R}\)&lt;/span&gt;を報酬関数とすれば、&lt;span class="math"&gt;\(h(\boldsymbol{x})\)&lt;/span&gt;で行動列の報酬が計算でき、&lt;span class="math"&gt;\(I\)&lt;/span&gt;の計算結果は報酬の期待値となる。報酬の期待値が計算できることはエージェントの行動決定において大変有用である。&amp;#160;&lt;a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;一様分布や正規分布等のよく知られた分布は、サンプリングアルゴリズムも確立されている。一様分布はメルセンヌ・ツイスタ、正規分布にはボックス-ミューラー法といった具合である。&amp;#160;&lt;a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;空間の次元が増加すると、その空間の自由度が直感に反して &lt;strong&gt;指数的&lt;/strong&gt; に増加すること。例えば、ユークリッド空間で一辺の長さが&lt;span class="math"&gt;\(a\)&lt;/span&gt;の&lt;span class="math"&gt;\(n\)&lt;/span&gt;次元超立方体を占める直径&lt;span class="math"&gt;\(a\)&lt;/span&gt;の超球体の割合を計算してみると&lt;span class="math"&gt;\(\frac{\sqrt{(\pi(a/2)^{2})^{n}}}{a^{n} \Gamma(\frac{n}{2}+1)}\)&lt;/span&gt;であり、&lt;span class="math"&gt;\(n\)&lt;/span&gt;を増加させると階乗オーダー（即ち、指数オーダーよりも早く）で減少する事が分かる。従って、一様乱数を用いていると、&lt;span class="math"&gt;\(n\)&lt;/span&gt;次元空間で超球体の内部にサンプルが入る確率が階乗オーダーで小さくなる。&amp;#160;&lt;a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:13"&gt;
&lt;p&gt;厳密には、直前の1つのサンプルのみに依存するので1階マルコフ性と呼ばれる。&amp;#160;&lt;a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:14"&gt;
&lt;p&gt;詳細は&lt;a href="#補足"&gt;補足&lt;/a&gt;で述べる。一般にエルゴード的とは、長時間に渡って観測した状態の平均（長時間平均）と、状態空間の平均（位相平均）が一致するという事を表す概念である。エルゴード理論がある様に、厳密な数学理論が展開されるが、ここではマルコフ連鎖以外については詳しくは説明しない（筆者がついていけてない）。&amp;#160;&lt;a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:15"&gt;
&lt;p&gt;連続な状態空間では、遷移確率行列の代わりに&lt;div class="math"&gt;$$P(\boldsymbol{x_{t+1}} \in C|\boldsymbol{x_{t}} = \boldsymbol{e_{t}}) = \int_{C} T(\boldsymbol{e_{t}}, \boldsymbol{y}) d \boldsymbol{y} \quad C \subset X, \boldsymbol{e_{t}} \in X$$&lt;/div&gt;となる様な条件付き確率分布&lt;span class="math"&gt;\(T(\boldsymbol{x}, \boldsymbol{y})\)&lt;/span&gt;（遷移核）を用いれば良い。&amp;#160;&lt;a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 15 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:16"&gt;
&lt;p&gt;形式的に書くと、状態&lt;span class="math"&gt;\(j \in X\)&lt;/span&gt;の定常分布&lt;span class="math"&gt;\(\pi_{j}\)&lt;/span&gt;は&lt;span class="math"&gt;\(\pi_{j} = P(\boldsymbol{x_{n}} = j)\ n=0,1,\dots\)&lt;/span&gt;と表される。&amp;#160;&lt;a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 16 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:17"&gt;
&lt;p&gt;例えば、現在状態を中心とした正規分布からでの乱択でも3つの性質を満たし、マルコフ連鎖はエルゴード的となる。&amp;#160;&lt;a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 17 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:18"&gt;
&lt;p&gt;これが詳細釣り合い条件を満たすことは、場合分けにより分かる: &lt;br&gt;
 - &lt;span class="math"&gt;\(\alpha(i \to j) = 1\)&lt;/span&gt;の時： &lt;span class="math"&gt;\(\alpha(j \to i) = \frac{r_{i}q_{ij}}{r_{j}q_{ji}}\)&lt;/span&gt;となるので、
  &lt;div class="math"&gt;$$
    p_{ji} = q_{ji} \alpha(j \to i) = q_{ji} \frac{r_{i}q_{ij}}{r_{j}q_{ji}} = \frac{r_{i}}{r_{j}}q_{ij} = \frac{r_{i}}{r_{j}} p_{ij}  \iff r_{i}p_{ij} = r_{j}p_{ji}
  $$&lt;/div&gt;
 - &lt;span class="math"&gt;\(\alpha(i \to j) = \frac{r_{j}q_{ji}}{r_{i}q_{ij}}\)&lt;/span&gt;の時： &lt;span class="math"&gt;\(\alpha(j \to i) = 1\)&lt;/span&gt;となるので、
  &lt;div class="math"&gt;$$
    p_{ij} = q_{ij} \alpha(i \to j) = q_{ij} \frac{r_{j}q_{ji}}{r_{i}q_{ij}} = \frac{r_{j}}{r_{i}}q_{ji} = \frac{r_{j}}{r_{i}} p_{ji}  \iff r_{i}p_{ij} = r_{j}p_{ji}
  $$&lt;/div&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 18 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:19"&gt;
&lt;p&gt;温度パラメタの調節は非常に難しい事が知られている。実験結果を見て経験的に設定される事がほとんどである。&amp;#160;&lt;a class="footnote-backref" href="#fnref:19" title="Jump back to footnote 19 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:20"&gt;
&lt;p&gt;分散パラメタの調節も非常に難しい。分散を大きくすると遷移幅（ステップサイズという）が大きくなって定常分布に落ち着くまでに時間が掛かり、分散を小さくし過ぎると遷移の動きが小さく、探索が十分に行われない危険性がある。一般に分散パラメタと温度パラメタにはトレードオフの関係がある。&amp;#160;&lt;a class="footnote-backref" href="#fnref:20" title="Jump back to footnote 20 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:21"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(q_{ij} = q_{ji}\)&lt;/span&gt;が成立する理由は、この場合&lt;span class="math"&gt;\(j\)&lt;/span&gt;は
  &lt;div class="math"&gt;$$
    j = i + \varepsilon \quad \varepsilon \sim {\cal N}(\boldsymbol{0}, \sigma^{2} \boldsymbol{I}) 
  $$&lt;/div&gt;
  と書けるので、平均が&lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;かつ正規分布の対称性により、
  &lt;div class="math"&gt;$$
    i = j - \varepsilon = j + \varepsilon
  $$&lt;/div&gt;
  よって&lt;span class="math"&gt;\(q_{ij} = {\cal N}(i, \sigma^{2}\boldsymbol{I}) = {\cal N}(j, \sigma^{2}\boldsymbol{I}) = q_{ji}\)&lt;/span&gt;を満たす&amp;#160;&lt;a class="footnote-backref" href="#fnref:21" title="Jump back to footnote 21 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:22"&gt;
&lt;p&gt;機械学習では、ベイジアンネットワークやボルツマンマシン（深層学習の一部）等のモデル学習に使われる&amp;#160;&lt;a class="footnote-backref" href="#fnref:22" title="Jump back to footnote 22 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:23"&gt;
&lt;p&gt;毎回ランダムで選んでも、順番に全変数を1個ずつ選んでも良い&amp;#160;&lt;a class="footnote-backref" href="#fnref:23" title="Jump back to footnote 23 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:24"&gt;
&lt;p&gt;数列&lt;span class="math"&gt;\(a_{n}\)&lt;/span&gt;の母関数を&lt;span class="math"&gt;\(F(z) = \displaystyle\sum_{n=0}^{\infty}a_{n}z^{n}\)&lt;/span&gt;とする。今、複素数&lt;span class="math"&gt;\(s \in \mathbb{C}\)&lt;/span&gt;を用いて&lt;span class="math"&gt;\(z = \exp(-s)\)&lt;/span&gt;とおき、&lt;span class="math"&gt;\(n\)&lt;/span&gt;の和を&lt;span class="math"&gt;\(t\)&lt;/span&gt;の積分に置き換えると、
&lt;div class="math"&gt;$$
  F(\exp(-s)) = \int_{0}^{\infty} a_{t}\exp(-st) dt
$$&lt;/div&gt;
これは数列&lt;span class="math"&gt;\(a_{t}\)&lt;/span&gt;のラプラス変換に他ならない。従ってラプラス変換の最終値定理を適用できる。離散の場合のラプラス変換を（片側）Z変換と呼ぶ。&amp;#160;&lt;a class="footnote-backref" href="#fnref:24" title="Jump back to footnote 24 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:25"&gt;
&lt;p&gt;2つの冪級数を&lt;span class="math"&gt;\(\displaystyle\sum_{n=0}^{\infty}a_{n}z^{n}, \sum_{n=0}^{\infty}b_{n}z^{n}\)&lt;/span&gt;とし、積の結果を&lt;span class="math"&gt;\(\displaystyle\sum_{n=0}^{\infty}c_{n}z^{n}\)&lt;/span&gt;とする。等号を立てると、&lt;div class="math"&gt;$$\begin{align}    \left(\sum_{n=0}^{\infty}a_{n}z^{n}\right)\left(\sum_{n=0}^{\infty}b_{n}z^{n} \right) = a_{0}b_{0}z^{0} + (a_{0}b_{1} + a_{1}b_{0})z^{1} + (a_{0}b_{2}+a_{1}b_{1}+a_{2}b_{0})z^{2} + \dots \\
  = \sum_{n=0}^{\infty}c_{n}z^{n} = c_{0}z^{0} + c_{1}z^{1} + c_{2}z^{2} + \dots \end{align}$$&lt;/div&gt;係数比較により、&lt;span class="math"&gt;\(c_{0} = a_{0}b_{0},\ c_{1} = a_{0}b_{1} + a_{1}b_{0},\ \dots\)&lt;/span&gt;が成立し、よって&lt;span class="math"&gt;\(c_{n} = \displaystyle \sum_{k=0}^{n}a_{k}b_{n-k}\)&lt;/span&gt;となる。ここの例では、&lt;span class="math"&gt;\(a_{k} = u_{k}z^{k},\ b_{k} = p_{jj}^{(k)}z^{k}\)&lt;/span&gt;とおけば良い。&amp;#160;&lt;a class="footnote-backref" href="#fnref:25" title="Jump back to footnote 25 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:26"&gt;
&lt;p&gt;（証明）母関数（Z変換）を&lt;span class="math"&gt;\(F(z) = \displaystyle \sum_{n=0}^{\infty}a_{n}z^{n}\)&lt;/span&gt;とおくと、&lt;div class="math"&gt;$$\begin{align}
  \lim_{z \to 1} (1-z) F(z) = \lim_{z \to 1}(1-z) \sum_{n=0}^{\infty} a_{n}z^{n} = \lim_{z \to 1} \sum_{n=0}^{\infty} a_{n} (z^{n} - z^{n+1}) = \lim_{z \to 1} \lim_{n \to \infty} \sum_{k=0}^{n} a_{k} (z^{k} - z^{k+1}) \\
  = \lim_{z \to 1} \lim_{n \to \infty} \sum_{k=0}^{n} (a_{k} - a_{k-1}) z^{k} \quad (\because a_{-1} = 0, また a_{0}(z^{0}-z^{1}) + a_{1}(z^{1}-z^{2}) +\dots = a_{0}z^{0} + (a_{0}-a_{1})z^{1} + \dots) \\
  = \lim_{n \to \infty} \sum_{k=0}^{n} (a_{k} - a_{k-1}) = \lim_{n \to \infty} a_{n}\end{align}$$&lt;/div&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:26" title="Jump back to footnote 26 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="統計"></category></entry><entry><title>LPC（Linear Predictive Coding, 線形予測符号化）</title><link href="/lpclinear-predictive-coding-xian-xing-yu-ce-fu-hao-hua.html" rel="alternate"></link><published>2020-04-23T12:10:00+09:00</published><updated>2020-04-23T12:10:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/lpclinear-predictive-coding-xian-xing-yu-ce-fu-hao-hua.html</id><summary type="html">&lt;p&gt;線形予測分析等とも言及される。&lt;/p&gt;
&lt;p&gt;英語版で決定的に簡単な資料は&lt;a href="http://www.emptyloop.com/technotes/A%20tutorial%20on%20linear%20prediction%20and%20Levinson-Durbin.pdf"&gt;ここ&lt;/a&gt;で見れます。ここの解説はその日本語訳以下の何かです。英語が読める人はそっちを見たほうが絶対早いです。&lt;/p&gt;
&lt;p&gt;ここよりも良い資料が有ります：（&lt;a href="http://aidiary.hatenablog.com/entry/20120415/1334458954" title="人工知能に関する断創録 - 線形予測分析（LPC）"&gt;人工知能に関する断創録&lt;/a&gt;）&lt;/p&gt;
&lt;h2&gt;アルゴリズムの導出&lt;/h2&gt;
&lt;h3&gt;問題設定&lt;/h3&gt;
&lt;p&gt;時間について離散化した信号が&lt;span class="math"&gt;\(y_{0}, y_{1}, ..., y_{n}\)&lt;/span&gt;として得られたとする。ここで、&lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt;を直前の&lt;span class="math"&gt;\(y_{i}\ (i=0,...,n-1)\)&lt;/span&gt;によって予測する事を考える。&lt;/p&gt;
&lt;p&gt;予測にあたって、線形予測では&lt;span class="math"&gt;\(k\)&lt;/span&gt;個の係数&lt;span class="math"&gt;\(a_{1},...,a_{k}\)&lt;/span&gt;を用いた単純な線形結合
&lt;/p&gt;
&lt;div class="math"&gt;$$
-a_{1}y_{n-1} - a_{2}y_{n-2} - ... - a_{k}y_{n-k} = - \sum_ …&lt;/div&gt;</summary><content type="html">&lt;p&gt;線形予測分析等とも言及される。&lt;/p&gt;
&lt;p&gt;英語版で決定的に簡単な資料は&lt;a href="http://www.emptyloop.com/technotes/A%20tutorial%20on%20linear%20prediction%20and%20Levinson-Durbin.pdf"&gt;ここ&lt;/a&gt;で見れます。ここの解説はその日本語訳以下の何かです。英語が読める人はそっちを見たほうが絶対早いです。&lt;/p&gt;
&lt;p&gt;ここよりも良い資料が有ります：（&lt;a href="http://aidiary.hatenablog.com/entry/20120415/1334458954" title="人工知能に関する断創録 - 線形予測分析（LPC）"&gt;人工知能に関する断創録&lt;/a&gt;）&lt;/p&gt;
&lt;h2&gt;アルゴリズムの導出&lt;/h2&gt;
&lt;h3&gt;問題設定&lt;/h3&gt;
&lt;p&gt;時間について離散化した信号が&lt;span class="math"&gt;\(y_{0}, y_{1}, ..., y_{n}\)&lt;/span&gt;として得られたとする。ここで、&lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt;を直前の&lt;span class="math"&gt;\(y_{i}\ (i=0,...,n-1)\)&lt;/span&gt;によって予測する事を考える。&lt;/p&gt;
&lt;p&gt;予測にあたって、線形予測では&lt;span class="math"&gt;\(k\)&lt;/span&gt;個の係数&lt;span class="math"&gt;\(a_{1},...,a_{k}\)&lt;/span&gt;を用いた単純な線形結合
&lt;/p&gt;
&lt;div class="math"&gt;$$
-a_{1}y_{n-1} - a_{2}y_{n-2} - ... - a_{k}y_{n-k} = - \sum_{i=1}^{k} a_{i} y_{n-i}
$$&lt;/div&gt;
&lt;p&gt;
によって&lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt;を近似する：
&lt;/p&gt;
&lt;div class="math"&gt;$$
y_{n} \approx - \sum_{i=1}^{k} a_{i} y_{n-i}
$$&lt;/div&gt;
&lt;p&gt;
（係数に負号&lt;span class="math"&gt;\(-\)&lt;/span&gt;が付いているのは、システムのフィードバック係数として捉えた時は負を付けるのが常識となっているからと考えられる。全ての係数の符号を反転させれば通常の和に戻るので、以下の導出にとって本質的な問題にならない。）&lt;/p&gt;
&lt;p&gt;予測の&lt;strong&gt;誤差&lt;/strong&gt;は、全ての&lt;span class="math"&gt;\(n\)&lt;/span&gt;における&lt;strong&gt;二乗誤差&lt;/strong&gt;の和&lt;span class="math"&gt;\(E\)&lt;/span&gt;によって測る：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
E &amp;amp;= \sum_{n=-\infty}^{\infty} \left[ y_{n} - \left\{ -\sum_{i=1}^{k} a_{i} y_{n-i} \right\} \right]^{2} \\
&amp;amp;= \sum_{n=-\infty}^{\infty} \left\{ y_{n} + \sum_{i=1}^{k}a_{i}y_{n-i} \right\}^{2}
\end{split}
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(a_{0} = 1\)&lt;/span&gt;と定義すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
E = \sum_{n=-\infty}^{\infty} \left\{\sum_{i=0}^{k}a_{i}y_{n-i}\right\}^{2}
$$&lt;/div&gt;
&lt;p&gt;とまとめられる。後は、この&lt;span class="math"&gt;\(E\)&lt;/span&gt;を最小化するように係数&lt;span class="math"&gt;\(a_{1},...,a_{k}\)&lt;/span&gt;を定めれば良い。&lt;/p&gt;
&lt;h3&gt;誤差の最小化&lt;/h3&gt;
&lt;h4&gt;偏微分&lt;/h4&gt;
&lt;p&gt;誤差の最小化を考える。常套手段ではあるが、&lt;span class="math"&gt;\(E\)&lt;/span&gt;を&lt;span class="math"&gt;\(a_{j} \ (j=1,...,k)\)&lt;/span&gt;によって偏微分し、その結果を&lt;span class="math"&gt;\(0\)&lt;/span&gt;とおいて解くことを考える。まず、&lt;span class="math"&gt;\(E\)&lt;/span&gt;の偏微分は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
\frac{\partial E}{\partial a_{j}} &amp;amp;= \sum_{n=-\infty}^{\infty} \frac{\partial}{\partial a_{j}} \left\{\sum_{i=0}^{k}a_{i}y_{n-i} \right\}^{2} \\
&amp;amp;= \sum_{n=-\infty}^{\infty} \frac{\partial}{\partial a_{j}} \left\{ a_{0}y_{n} + ... + a_{j}y_{n-j} + ... + a_{k}y_{n-k} \right\}^{2} \\
&amp;amp;= \sum_{n=-\infty}^{\infty} 2 y_{n-j} \sum_{i=0}^{k}a_{i}y_{n-i} \\
&amp;amp;= 2 \sum_{i=0}^{k}a_{i} \sum_{n=-\infty}^{\infty} y_{n-j} y_{n-i} \quad (\because \text{和の順序交換}) \\
&amp;amp;= 2 \sum_{i=0}^{k}a_{i} \sum_{n^{\prime}=-\infty}^{\infty} y_{n^{\prime}} y_{n^{\prime}+j-i} \quad (n^{\prime} = n-j \ \text{とおいた})
\end{split}
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;span class="math"&gt;\(R_{l}\)&lt;/span&gt;を次の式で定義する：&lt;/p&gt;
&lt;div class="math"&gt;$$
R_{l} = \sum_{n=-\infty}^{\infty} y_{n} y_{n+l}
$$&lt;/div&gt;
&lt;p&gt;（&lt;strong&gt;自己相関&lt;/strong&gt; という。）&lt;span class="math"&gt;\(R_{l}\)&lt;/span&gt;を用いることで、偏微分の結果は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial E}{\partial a_{j}} = 2 \sum_{i=0}^{k} a_{i}R_{|j-i|}
$$&lt;/div&gt;
&lt;p&gt;と表せる。&lt;/p&gt;
&lt;p&gt;次に、&lt;span class="math"&gt;\(\displaystyle\frac{\partial E}{\partial a_{j}} = 0\ (j=1,...,k)\)&lt;/span&gt;とおいて解く事を考える。和の前に付いている係数&lt;span class="math"&gt;\(2\)&lt;/span&gt;は両辺&lt;span class="math"&gt;\(2\)&lt;/span&gt;で割ることで消すことが出来る。その上で&lt;span class="math"&gt;\(j=1,...,k\)&lt;/span&gt;での式を並べてみると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
a_{0}R_{|0-1|} + a_{1}R_{|1-1|} + ... + a_{k}R_{|k-1|} &amp;amp;= 0 \\
a_{0}R_{|0-2|} + a_{1}R_{|1-2|} + ... + a_{k}R_{|k-2|} &amp;amp;= 0 \\
\vdots \\
a_{0}R_{|0-k|} + a_{1}R_{|1-k|} + ... + a_{k}R_{|k-k|} &amp;amp;= 0 \\
\end{split}
$$&lt;/div&gt;
&lt;p&gt;より、行列形式で&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
R_{1} &amp;amp; R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k-1} \\
R_{2} &amp;amp; R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-2} \\
\vdots &amp;amp;      &amp;amp;  &amp;amp; \ddots   &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; R_{k-2} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
= \vec{0}
$$&lt;/div&gt;
&lt;p&gt;と表せられる。以下、&lt;/p&gt;
&lt;div class="math"&gt;$$
M = 
\begin{bmatrix}
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
R_{2} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k-2} \\
\vdots &amp;amp;      &amp;amp;  \ddots &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\ , \  
\vec{a}_{k} = 
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;として、&lt;span class="math"&gt;\(M\vec{a}_{k} = \vec{0}\)&lt;/span&gt;を解くことを考える。&lt;/p&gt;
&lt;h4&gt;Levinson-Durbin再帰（Levinson-Durbin recursion）へ&lt;/h4&gt;
&lt;p&gt;上までで求まった連立方程式&lt;span class="math"&gt;\(M\vec{a}\_{k+1} = \vec{0}\)&lt;/span&gt;をもう少し整理していく。数値解法的には、&lt;span class="math"&gt;\(M\)&lt;/span&gt;は正方行列にしておくのが望ましい。そこで、&lt;span class="math"&gt;\(M\)&lt;/span&gt;の一番上の行に&lt;span class="math"&gt;\([R_{0} R_{1} ... R_{k}]\)&lt;/span&gt;を追加すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
M\vec{a}_{k} &amp;amp;= 
\begin{bmatrix}
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
R_{2} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k-2} \\
\vdots &amp;amp;      &amp;amp;  \ddots &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
\\&amp;amp;=
\begin{bmatrix}
R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k}   \\
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
\vdots &amp;amp;       &amp;amp; \ddots   &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
-\begin{bmatrix}
\sum_{i=0}^{k}a_{i}R_{i} \\ 0 \\ 0 \\ \vdots \\ 0 
\end{bmatrix}
= \vec{0}
\end{split}
$$&lt;/div&gt;
&lt;p&gt;と変形できる。よって、次の連立方程式を解くことに帰着できる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k}   \\
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
\vdots &amp;amp;       &amp;amp; \ddots   &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
=\begin{bmatrix}
\sum_{i=0}^{k}a_{i}R_{i} \\ 0 \\ 0 \\ \vdots \\ 0 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;この連立方程式を高速に解くアルゴリズムが、Levinson-Durbin再帰法である。以下、&lt;span class="math"&gt;\(e_{k} = \sum_{i=0}^{k} a_{i} R_{i}\)&lt;/span&gt;とし、また行列&lt;span class="math"&gt;\(N_{k}\)&lt;/span&gt;を次で定義する：&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k} =
\begin{bmatrix}
R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k}   \\
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
\vdots &amp;amp;       &amp;amp; \ddots   &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
$$&lt;/div&gt;
&lt;h3&gt;Levinson-Durbin再帰&lt;/h3&gt;
&lt;p&gt;このアルゴリズムは、数学的帰納法によく似ている：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(k=1\)&lt;/span&gt;の場合で係数を求める&lt;/li&gt;
&lt;li&gt;一般の&lt;span class="math"&gt;\(k\)&lt;/span&gt;で係数が求まったとし、その結果から&lt;span class="math"&gt;\(k+1\)&lt;/span&gt;で係数を求める&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;（&lt;a href="http://www.emptyloop.com/technotes/A%20tutorial%20on%20linear%20prediction%20and%20Levinson-Durbin.pdf"&gt;参考資料&lt;/a&gt;で筆者は、「Levinson-Durbin帰納法と言ったほうがいいんじゃないか」と書いてあった。）ここでは、1.および2.の場合の解をそれぞれ見ていく。&lt;/p&gt;
&lt;h4&gt;k=1の時&lt;/h4&gt;
&lt;div class="math"&gt;$$
\vec{a}_{1}=
\begin{bmatrix}
 1 \\ 
 a_{1}
\end{bmatrix}
,\ 
N_{1}\vec{a}_{1}=
\begin{bmatrix}
 e_{1} \\ 
 0
\end{bmatrix}
,\ 
N_{1}=
\begin{bmatrix}
 R_{0} &amp;amp; R_{1} \\ 
 R_{1} &amp;amp; R_{0}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;より、実際に&lt;span class="math"&gt;\(N_{1}\vec{a}_{1}\)&lt;/span&gt;を計算してみると、&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{1}\vec{a}_{1}=
\begin{bmatrix}
 R_{0} + R_{1}a_{1} \\ 
 R_{1} + R_{0}a_{1}
\end{bmatrix}=
\begin{bmatrix}
 e_{1} \\ 
 0
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;より、&lt;span class="math"&gt;\(e_{1} = R_{0} + R_{1}a_{1}\)&lt;/span&gt;、及び&lt;span class="math"&gt;\(R_{1} + R_{0}a_{1} = 0\)&lt;/span&gt;から&lt;span class="math"&gt;\(a_{1} = -\displaystyle\frac{R_{1}}{R_{0}}\)&lt;/span&gt;と求められる。（&lt;span class="math"&gt;\(R_{0} = \displaystyle\sum_{n=-\infty}^{\infty}y_{n}^{2} &amp;gt; 0\)&lt;/span&gt;より、至る所ゼロ除算の心配はない）&lt;/p&gt;
&lt;h4&gt;一般のkの時&lt;/h4&gt;
&lt;p&gt;仮定として、&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k}\vec{a}_{k}=
\begin{bmatrix}
R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k}   \\
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} \\
\vdots &amp;amp;       &amp;amp; \ddots   &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} 
\end{bmatrix}
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} 
\end{bmatrix}
=\begin{bmatrix}
e_{k} \\ 0 \\ 0 \\ \vdots \\ 0 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;が成立していたとする。&lt;span class="math"&gt;\(k+1\)&lt;/span&gt;の時、行列&lt;span class="math"&gt;\(N_{k+1}\)&lt;/span&gt;は&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k+1}=
\begin{bmatrix}
R_{0} &amp;amp; R_{1} &amp;amp; ... &amp;amp; R_{k} &amp;amp; R_{k+1}   \\
R_{1} &amp;amp; R_{0} &amp;amp; ... &amp;amp; R_{k-1} &amp;amp; R_{k} \\
\vdots &amp;amp;       &amp;amp; \ddots  &amp;amp; &amp;amp; \vdots  \\
R_{k} &amp;amp; R_{k-1} &amp;amp; ... &amp;amp; R_{0} &amp;amp; R_{1} \\
R_{k+1} &amp;amp; R_{k} &amp;amp; ... &amp;amp; R_{1} &amp;amp; R_{0} 
\end{bmatrix}=
\left[
\begin{array}{cccc|c}
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; R_{k+1}   \\
 &amp;amp; N_{k} &amp;amp; &amp;amp; &amp;amp; R_{k} \\
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; \vdots  \\
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; R_{1} \\\hline
R_{k+1} &amp;amp; R_{k} &amp;amp; ... &amp;amp; R_{1} &amp;amp; R_{0} 
\end{array}
\right]
$$&lt;/div&gt;
&lt;p&gt;となり、&lt;span class="math"&gt;\(N_{k}\)&lt;/span&gt;の行・列共に1つ増えた行列となる。&lt;/p&gt;
&lt;p&gt;一方の&lt;span class="math"&gt;\(\vec{a}_{k+1}\)&lt;/span&gt;は未知である。そこで、技巧的ではあるが次&lt;span class="math"&gt;\(\vec{a}\_{k}\)&lt;/span&gt;を&lt;span class="math"&gt;\(0\)&lt;/span&gt;を追加する事で拡張した次のベクトル&lt;span class="math"&gt;\(\vec{u}\_{k+1}, \vec{v}\_{k+1}\)&lt;/span&gt;を用いる事を考える：&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{u}_{k+1}=
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} \\ 0
\end{bmatrix}\ ,\ 
\vec{v}_{k+1}=
\begin{bmatrix}
0 \\ a_{k} \\ \vdots \\ a_{2} \\ a_{1} \\ 1 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\vec{u}\_{k+1}, \vec{v}\_{k+1}\)&lt;/span&gt;は互いに要素を反転したベクトルである（互いに&lt;strong&gt;一次独立&lt;/strong&gt;で有ることにも注目）。これら&lt;span class="math"&gt;\(\vec{u}\_{k+1}, \vec{v}\_{k+1}\)&lt;/span&gt;を用いて&lt;span class="math"&gt;\(N\_{k+1}\vec{u}\_{k+1}\)&lt;/span&gt;と&lt;span class="math"&gt;\(N_{k+1}\vec{v}\_{k+1}\)&lt;/span&gt;を計算すると、まず&lt;span class="math"&gt;\(N\_{k+1}\vec{u}\_{k+1}\)&lt;/span&gt;は&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
N_{k+1}\vec{u}_{k+1}&amp;amp;=
\left[
\begin{array}{cccc|c}
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; R_{k+1}   \\
 &amp;amp; N_{k} &amp;amp; &amp;amp; &amp;amp; R_{k} \\
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; \vdots  \\
 &amp;amp; &amp;amp; &amp;amp; &amp;amp; R_{1} \\\hline
R_{k+1} &amp;amp; R_{k} &amp;amp; ... &amp;amp; R_{1} &amp;amp; R_{0} 
\end{array}
\right]
\begin{bmatrix}
1 \\ a_{1} \\ a_{2} \\ \vdots \\ a_{k} \\ 0
\end{bmatrix}\\
&amp;amp;=
\begin{bmatrix}
 \\  \\ N_{k}\vec{a}_{k} \\  \\  \\ \hline [R_{k+1} R_{k} ... R_{1}] \vec{a}_{k}
\end{bmatrix}=
\begin{bmatrix}
e_{k} \\ 0 \\ \vdots \\ 0 \\  \displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j}
\end{bmatrix}
\end{split}
$$&lt;/div&gt;
&lt;p&gt;であり、もう一方の&lt;span class="math"&gt;\(N\_{k+1}\vec{v}\_{k+1}\)&lt;/span&gt;は、&lt;span class="math"&gt;\(N\_{k+1}\)&lt;/span&gt;が&lt;strong&gt;対称行列&lt;/strong&gt;なので&lt;span class="math"&gt;\(N\_{k+1}\vec{u}\_{k+1}\)&lt;/span&gt;の結果を反転したベクトルとなる：&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k+1}\vec{v}_{k+1}=
\begin{bmatrix}
\displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j} \\ 0 \\ \vdots \\ 0 \\ e_{k}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;そして、&lt;span class="math"&gt;\(\vec{a}\_{k+1}\)&lt;/span&gt;は&lt;span class="math"&gt;\(\vec{u}\_{k+1}\)&lt;/span&gt;と&lt;span class="math"&gt;\(\vec{v}\_{k+1}\)&lt;/span&gt;の線形結合で表現できる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{a}_{k+1} = \vec{u}_{k+1} + \lambda \vec{v}_{k+1} \quad (\lambda : 実数)
$$&lt;/div&gt;
&lt;p&gt;これは、実際に&lt;span class="math"&gt;\(N\_{k+1}(\vec{u}\_{k+1} + \lambda \vec{v}\_{k+1})\)&lt;/span&gt;を計算することで確かめられる：&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k+1}(\vec{u}_{k+1} + \lambda \vec{v}_{k+1}) = N_{k+1}\vec{u}_{k+1} + N_{k+1}\lambda \vec{v}_{k+1}=
\begin{bmatrix}
e_{k} + \lambda \displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j} \\ 0 \\ \vdots \\ 0 \\ \displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j} + \lambda e_{k}
\end{bmatrix} 
$$&lt;/div&gt;
&lt;p&gt;ここで&lt;span class="math"&gt;\(\lambda = - \displaystyle\frac{\sum_{j=0}^{k} a_{j} R_{k+1-j}}{e_{k}}\)&lt;/span&gt;とすれば、&lt;/p&gt;
&lt;div class="math"&gt;$$
N_{k+1}(\vec{u}_{k+1} + \lambda \vec{v}_{k+1}) =
\begin{bmatrix}
e_{k} - \lambda^{2} e_{k} \\ 0 \\ \vdots \\ 0 \\ \displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j} - \displaystyle\sum_{j=0}^{k} a_{j} R_{k+1-j}
\end{bmatrix}=
\begin{bmatrix}
(1-\lambda^{2}) e_{k} \\ 0 \\ \vdots  \\ 0
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;となって&lt;span class="math"&gt;\(e\_{k+1}\)&lt;/span&gt;が求まる。同時に右辺の結果を与える&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;は唯一つしか存在しないので、この時の&lt;span class="math"&gt;\(\vec{u}\_{k+1} + \lambda \vec{v}\_{k+1}\)&lt;/span&gt;は&lt;span class="math"&gt;\(\vec{a}\_{k+1}\)&lt;/span&gt;と一致する。&lt;/p&gt;
&lt;h2&gt;アルゴリズム&lt;/h2&gt;
&lt;p&gt;以上の導出結果をまとめると、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(k=1\)&lt;/span&gt;の時：
&lt;div class="math"&gt;$$
a_{1} = - \frac{R_{1}}{R_{0}} \ , \ e_{1} = R_{0} + R_{1}a_{1}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(k\)&lt;/span&gt;が求まった時、&lt;span class="math"&gt;\(k+1\)&lt;/span&gt;は：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
\lambda = - \displaystyle\frac{\sum_{j=0}^{k}a_{j}R_{k+1-j}}{e_{k}} \ , \ e_{k+1} = (1-\lambda^{2})e_{k}\ ,\ \vec{a}_{k+1} = \vec{u}_{k+1} + \lambda \vec{v}_{k+1}
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;div class="math"&gt;$$
R_{l} = \sum_{n=-\infty}^{\infty} y_{n}y_{n+l}\ ,\ 
\vec{u}_{k+1}=
\begin{bmatrix}
1 \\ a_{1} \\ \vdots \\ a_{k} \\ 0
\end{bmatrix}\ ,\ 
\vec{v}_{k+1}=
\begin{bmatrix}
0 \\ a_{k} \\ \vdots \\ a_{1} \\ 1 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;となる。&lt;/p&gt;
&lt;p&gt;自己相関&lt;span class="math"&gt;\(R_{l}\)&lt;/span&gt;は過去から未来までの無限の信号和になっているので現実の計算機では計算出来ない。実際には自己相関の代わりに次の&lt;strong&gt;標本自己相関&lt;/strong&gt;&lt;span class="math"&gt;\(\tilde{R}\_{l}\)&lt;/span&gt;を用いる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{R}_{l} = \sum_{i=0}^{n} y_{i}y_{i-l} \quad (l = 0, ..., k)
$$&lt;/div&gt;
&lt;h2&gt;補足&lt;/h2&gt;
&lt;h3&gt;周波数特性の導出&lt;/h3&gt;
&lt;p&gt;近似式は誤差項&lt;span class="math"&gt;\(e_{n}\)&lt;/span&gt;を用いて次の等式で書き表せる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
y_{n} = - a_{1}y_{n-1} - a_{2}y_{n-2} - \dots -a_{k}y_{n-k} + e_{n} \\
y_{n} = - \sum_{i=1}^{k} a_{i}y_{n-k} + e_{n}
\end{split}
$$&lt;/div&gt;
&lt;p&gt;この式を両辺z変換すると、次の伝達関数を得る：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
Y(z) = - \sum_{i=1}^{k} a_{i}z^{-i}Y(z) + E(z) \\
\iff \frac{Y(z)}{E(z)} = \frac{1}{1+ \sum_{i=1}^{k}a_{i}z^{-i}}
\end{split}
$$&lt;/div&gt;
&lt;p&gt;この結果は、予測誤差を入力することで出力音声が得られるシステムを表している。人間の声帯から発せられた音声を&lt;span class="math"&gt;\(E(z)\)&lt;/span&gt;とすれば、この伝達関数は声道の共鳴する特性をモデル化していると考えることができる。共鳴が発生する周波数では伝達関数のパワー（振幅、ゲイン）が高くなり、この結果からフォルマント分析を行うことができる。&lt;/p&gt;
&lt;p&gt;伝達関数の周波数特性を求めるには、z変換の結果に&lt;span class="math"&gt;\(z=\exp(j\omega), (\omega=2\pi f:角周波数)\)&lt;/span&gt;を代入する：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
\frac{Y(z)}{E(z)} = \frac{1}{1+ \sum_{i=1}^{k} a_{i} \exp(-j i \omega) }
\end{split}
$$&lt;/div&gt;
&lt;h3&gt;標本自己相関の計算&lt;/h3&gt;
&lt;p&gt;標本自己相関は自分自身との相関を計算するので&lt;span class="math"&gt;\(O(N^{2})\)&lt;/span&gt;の計算量があるが、ウィーナー・ヒンチンの定理（信号のパワースペクトラムは、その自己相関に等しい）を使って自己相関を計算すれば、実質FFTと同等の計算量&lt;span class="math"&gt;\(O(N \log N)\)&lt;/span&gt;で抑えることもできる。但し、巡回畳み込みや、パワースペクトラムの平均処理を考慮する必要がある。&lt;/p&gt;
&lt;h2&gt;参考資料リスト&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LPCについて：&lt;a href="http://ahclab.naist.jp/lecture/2014/sp/material/sp2nd-2.pdf"&gt;東京大学 音情報処理論&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ウィーナー・ヒンチンの定理：&lt;a href="http://manabukano.brilliant-future.net/lecture/appliedmathF2/slide/Slide07_PowerSpctrum.pdf"&gt;京都大学 工業数学&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;実装&lt;/h2&gt;
&lt;p&gt;実装はC言語です（リファレンスはLLで書くべきだった...）&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;math.h&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;stdlib.h&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;string.h&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;float.h&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;

&lt;span class="cm"&gt;/* （標本）自己相関の計算 */&lt;/span&gt;
&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; 
&lt;span class="nf"&gt;calc_auto_correlation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="cm"&gt;/* Levinson-Durbin再帰計算 */&lt;/span&gt;
&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; 
&lt;span class="nf"&gt;levinson_durbin_recursion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;lpc_coef&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="cm"&gt;/* サンプル数 */&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;max_delay&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  &lt;span class="cm"&gt;/* LPC係数の数 */&lt;/span&gt;

  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;auto_cor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_delay&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;coff&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_delay&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="cm"&gt;/* 波形の生成 */&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;4.0f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 自己相関・Levinson-Durbin再帰計算 */&lt;/span&gt;
  &lt;span class="n"&gt;calc_auto_correlation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auto_cor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_delay&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;levinson_durbin_recursion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;auto_cor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_delay&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="cm"&gt;/* 予測テスト */&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_delay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="cm"&gt;/* 最初のmax_delayステップ分は元信号を単純コピー */&lt;/span&gt;
      &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
      &lt;span class="cm"&gt;/* 以降は予測 */&lt;/span&gt;
      &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;max_delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coff&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 誤差計算・結果表示 */&lt;/span&gt;
  &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No:%d Data: %f Predict: %f &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Error : %f &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;

  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auto_cor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coff&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;levinson_durbin_recursion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;lpc_coef&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lpc_coef&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Data or result pointer point to NULL. &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* &lt;/span&gt;
&lt;span class="cm"&gt;   * 0次自己相関（信号の二乗和）が0に近い場合、入力信号は無音と判定&lt;/span&gt;
&lt;span class="cm"&gt;   * =&amp;gt; 予測誤差, LPC係数は全て0として無音出力システムを予測.&lt;/span&gt;
&lt;span class="cm"&gt;   */&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;FLT_EPSILON&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;lpc_coef&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 初期化 */&lt;/span&gt;
  &lt;span class="n"&gt;a_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="cm"&gt;/* a_0, a_k+1を含めるとmax_order+2 */&lt;/span&gt;
  &lt;span class="n"&gt;e_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="cm"&gt;/* e_0, e_k+1を含めるとmax_order+2 */&lt;/span&gt;
  &lt;span class="n"&gt;u_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="n"&gt;v_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 最初のステップの係数をセット */&lt;/span&gt;
  &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
  &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
  &lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
  &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
  &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 

  &lt;span class="cm"&gt;/* 再帰処理 */&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
    &lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;

    &lt;span class="cm"&gt;/* u_vec, v_vecの更新 */&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="cm"&gt;/* resultの更新 */&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
       &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_delay&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* 結果の取得 */&lt;/span&gt;
  &lt;span class="n"&gt;memcpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lpc_coef&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;

  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u_vec&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v_vec&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a_vec&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e_vec&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;calc_auto_correlation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_order&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Max order(%zu) is larger than number of samples(%zu). &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auto_corr&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Data or result pointer point to NULL. &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="cm"&gt;/* （標本）自己相関の計算 */&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delay_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;delay_time&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_order&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0f&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;num_sample&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;auto_corr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_sample&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;delay_time&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;実験&lt;/h2&gt;
&lt;p&gt;実際に走らせた結果のグラフは以下。
&lt;img alt="result" src="./images/lpc_result.png"&gt;&lt;/p&gt;
&lt;p&gt;原信号が簡単すぎたのか、係数は少なめでも十分に予測できている。しかし、適当な係数の数の取り方を決める手法がないと、実信号で使い物になりそうにない。とりあえず、自己相関を使いこなしたN. Wiener is GOD.（結言）&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="LPC"></category><category term="信号処理"></category><category term="ロスレス音声"></category></entry><entry><title>離散フーリエ変換（DFT）</title><link href="/li-san-huriebian-huan-dft.html" rel="alternate"></link><published>2020-04-23T12:00:00+09:00</published><updated>2020-04-23T12:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-23:/li-san-huriebian-huan-dft.html</id><summary type="html">&lt;p&gt;離散時間かつ離散周波数でのフーリエ変換を離散フーリエ変換という。&lt;/p&gt;
&lt;h2&gt;準備：時間領域で離散化すると、周波数領域では周期的になる&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;を離散化した信号を&lt;span class="math"&gt;\(g(t)\)&lt;/span&gt;とおく。離散化には、サンプリング周期&lt;span class="math"&gt;\(t_{s}\)&lt;/span&gt;の周期的デルタ関数&lt;span class="math"&gt;\(\delta_{t_{s}}(t)\)&lt;/span&gt;を用いて&lt;/p&gt;
&lt;div class="math"&gt;$$ g(t) = f(t) \delta_{t_s}(t) = \sum_{n=-\infty}^{\infty} f(t) \delta(t - nt_{s}) $$&lt;/div&gt;
&lt;p&gt;とする。デルタ関数&lt;span class="math"&gt;\(\delta(t)\)&lt;/span&gt;は関数&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;に対して次が成り立つ（超）関数である：&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_ …&lt;/div&gt;</summary><content type="html">&lt;p&gt;離散時間かつ離散周波数でのフーリエ変換を離散フーリエ変換という。&lt;/p&gt;
&lt;h2&gt;準備：時間領域で離散化すると、周波数領域では周期的になる&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;を離散化した信号を&lt;span class="math"&gt;\(g(t)\)&lt;/span&gt;とおく。離散化には、サンプリング周期&lt;span class="math"&gt;\(t_{s}\)&lt;/span&gt;の周期的デルタ関数&lt;span class="math"&gt;\(\delta_{t_{s}}(t)\)&lt;/span&gt;を用いて&lt;/p&gt;
&lt;div class="math"&gt;$$ g(t) = f(t) \delta_{t_s}(t) = \sum_{n=-\infty}^{\infty} f(t) \delta(t - nt_{s}) $$&lt;/div&gt;
&lt;p&gt;とする。デルタ関数&lt;span class="math"&gt;\(\delta(t)\)&lt;/span&gt;は関数&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;に対して次が成り立つ（超）関数である：&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty} f(t) \delta(t) dt = f(0)
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_{s}\)&lt;/span&gt;の逆数はサンプリングレート（&lt;span class="math"&gt;\(f_{s} = 1/t_{s}\)&lt;/span&gt;）そのものである。また、周期的デルタ関数の（複素）フーリエ級数は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\delta_{t_{s}}(t) &amp;amp;= \sum_{n=-\infty}^{\infty} c_{n} \exp(j\omega_{s}t)dt \\
c_{n} &amp;amp;= \frac{1}{t_{s}} \int^{t_{s}/2}_{-t_{s}/2} \delta_{t_{s}}(t) \exp(-jn\omega_{s}t)dt
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;と表せる。ここで&lt;span class="math"&gt;\(\omega_{s}=2\pi/t_{s}\)&lt;/span&gt;（サンプリング角周波数）である。&lt;span class="math"&gt;\(c_{n}\)&lt;/span&gt;の計算を考えると、積分範囲&lt;span class="math"&gt;\([-t_{s}/2, t_{s}/2]\)&lt;/span&gt;に唯一つのインパルスが存在する事に留意すれば、次の結果を得る：&lt;/p&gt;
&lt;div class="math"&gt;$$
c_{n} = \frac{1}{t_{s}} \int^{t_{s}/2}_{-t_{s}/2} \delta(t) \exp(-jn\omega_{s}t)dt = \frac{1}{t_{s}}\exp(0) = \frac{1}{t_{s}}
$$&lt;/div&gt;
&lt;p&gt;よって、周期的デルタ関数の複素フーリエ級数は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta_{t_s}(t) = \frac{1}{t_{s}} \sum_{n=-\infty}^{\infty} \exp(j n \omega_{s} t) = \frac{1}{t_{s}} \sum_{n=-\infty}^{\infty} \exp(j 2\pi n t)
$$&lt;/div&gt;
&lt;p&gt;であり、この結果を用いると、&lt;span class="math"&gt;\(g(t)\)&lt;/span&gt;のフーリエ変換&lt;span class="math"&gt;\({\cal F}[g(t)]\)&lt;/span&gt;は、&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
{\cal F}[g(t)] &amp;amp;= \frac{1}{t_{s}} \sum_{n=-\infty}^{\infty} {\cal F} \left[ f(t) \exp(j n \omega_{s} t) \right] \\
&amp;amp;= \frac{1}{t_{s}} \sum_{n=-\infty}^{\infty} \int_{-\infty}^{\infty} f(t) \exp[ -j (\omega - n\omega_{s}) t] dt 
\\
&amp;amp;= \frac{1}{t_{s}} \sum_{n=-\infty}^{\infty} F(\omega - n\omega_{s})
\end{split} 
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;span class="math"&gt;\(F(\omega)\)&lt;/span&gt;は&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;をフーリエ変換した結果を表している。この結果は、離散化した信号のフーリエ変換は周波数領域で&lt;strong&gt;周期&lt;span class="math"&gt;\(\omega_{s}\)&lt;/span&gt;で&lt;span class="math"&gt;\(F(\omega)\)&lt;/span&gt;を繰り返す&lt;/strong&gt;事を示している。&lt;/p&gt;
&lt;h2&gt;離散フーリエ変換・離散フーリエ逆変換&lt;/h2&gt;
&lt;h3&gt;離散化の仮定&lt;/h3&gt;
&lt;p&gt;時間領域で離散化した信号&lt;span class="math"&gt;\(f[n]\)&lt;/span&gt;を次の様に定義する：&lt;/p&gt;
&lt;div class="math"&gt;$$
f[n] = f(nt_{s}) \quad n = 0,...,N-1
$$&lt;/div&gt;
&lt;p&gt;ここで、&lt;span class="math"&gt;\(N\)&lt;/span&gt;はサンプリング個数である。重要な仮定として、&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;は&lt;span class="math"&gt;\(N\)&lt;/span&gt;このサンプリング期間で周期的であるとする。即ち、&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;の周期を&lt;span class="math"&gt;\(T\)&lt;/span&gt;とおくと、&lt;/p&gt;
&lt;div class="math"&gt;$$
T = Nt_{s}
$$&lt;/div&gt;
&lt;p&gt;が成立する。更に、&lt;strong&gt;周波数領域についても&lt;span class="math"&gt;\(\omega_{s}\)&lt;/span&gt;を&lt;span class="math"&gt;\(N\)&lt;/span&gt;分割&lt;/strong&gt; し、&lt;/p&gt;
&lt;div class="math"&gt;$$
\omega_{k} = \frac{\omega_{s}}{N} k = \frac{2\pi}{Nt_{s}}k \quad k = 0,...,N-1
$$&lt;/div&gt;
&lt;p&gt;として、周波数領域で離散化した信号&lt;span class="math"&gt;\(F[k]\)&lt;/span&gt;を次の様に定義する：&lt;/p&gt;
&lt;div class="math"&gt;$$
F[k] = F(\omega_{k}) \quad k = 0,...,N-1
$$&lt;/div&gt;
&lt;p&gt;分割の個数&lt;span class="math"&gt;\(N\)&lt;/span&gt;が時間領域と周波数領域で異なる場合、変換対が対称にならないので高速フーリエ変換の時に不都合が生じる。&lt;/p&gt;
&lt;h3&gt;離散フーリエ変換の導出&lt;/h3&gt;
&lt;p&gt;離散化の仮定のもとで、フーリエ変換は次の様に計算できる：&lt;/p&gt;
&lt;div class="math"&gt;$$
F[k] = \int_{-\infty}^{\infty} f(t) \exp(-j\omega_{k}t) dt = \int_{-\infty}^{\infty} f(t) \exp\left(-j\frac{2\pi k}{Nt_{s}} t \right) dt
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(t)\)&lt;/span&gt;は周期&lt;span class="math"&gt;\(T\)&lt;/span&gt;で繰り返すので、積分範囲は1周期分とする（なぜ一周期か：フーリエ係数の仮定から。係数は1周期の積分で良い。三角関数の完全性を見よ）：&lt;/p&gt;
&lt;div class="math"&gt;$$
F[k] = \int^{T}_{0} f(t) \exp \left(-j \frac{2\pi k}{Nt_{s}} t \right)dt
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t = nt_{s}\)&lt;/span&gt;と変数変換すると（&lt;span class="math"&gt;\(n\)&lt;/span&gt;を積分変数とする）、&lt;/p&gt;
&lt;div class="math"&gt;$$
F[k] = t_{s}\int^{N}_{0} f(nt_{s}) \exp \left(-j \frac{2\pi k}{N} n\right)dn
$$&lt;/div&gt;
&lt;p&gt;この積分は、次の和で近似できる。&lt;/p&gt;
&lt;div class="math"&gt;$$
F[k] \approx t_{s}\sum^{N-1}_{n=0} f(nt_{s}) \exp \left(-j \frac{2\pi k}{N} n\right) = t_{s} \sum^{N-1}_{n=0} f[n] \exp \left(-j \frac{2\pi nk}{N} \right)
$$&lt;/div&gt;
&lt;p&gt;この式が離散フーリエ変換の式となる。逆変換については、複素フーリエ級数&lt;/p&gt;
&lt;div class="math"&gt;$$
\left\{ \begin{array}{l}
f(nt_{s})  = \displaystyle \sum_{k=-\infty}^{\infty} c_{n} \exp(j\omega_{k}kn t) \\
c_{n} = \displaystyle \frac{1}{T} \int^{T}_{0} f(t) \exp(-j n\omega_{k} t) dt
\end{array} \right.   
$$&lt;/div&gt;
&lt;p&gt;から、&lt;span class="math"&gt;\(c_{n}\)&lt;/span&gt;を消去すると、&lt;/p&gt;
&lt;div class="math"&gt;$$
f(nt_{s}) = \sum_{k=-\infty}^{\infty} \left\{ \frac{1}{T} \int^{T}_{0} f(t) \exp\left( -j \frac{2\pi kt}{T} \right) dt \right\} \exp\left( \frac{j2\pi k}{T} nt_{s} \right) \\
f(nt_{s}) = \frac{\omega_{s}}{2 \pi N} \sum_{k=-\infty}^{\infty} F[k] \exp\left(j \frac{2\pi nk}{N} \right)
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(F[k]\)&lt;/span&gt;の周期は&lt;span class="math"&gt;\(\omega_{s}\)&lt;/span&gt;なので、1周期分は&lt;span class="math"&gt;\(k = 0,...,N-1\)&lt;/span&gt;となる。再び1周期分のみを考えると、&lt;/p&gt;
&lt;div class="math"&gt;$$
f[n] = \frac{\omega_{s}}{2 \pi N} \sum_{k=0}^{N-1} F[k] \exp\left(j \frac{2\pi nk}{N} \right)
$$&lt;/div&gt;
&lt;p&gt;この式が離散フーリエ逆変換の式となる。変換の式をまとめると、&lt;/p&gt;
&lt;div class="math"&gt;$$
\left\{ \begin{array}{l}
\displaystyle F[k] = t_{s} \sum^{N-1}_{n=0} f[n] \exp \left(-j \frac{2\pi nk}{N} \right) \\
\displaystyle f[n] = \frac{\omega_{s}}{2 \pi N} \sum_{k=0}^{N-1} F[k] \exp\left(j \frac{2\pi nk}{N} \right)
\end{array} \right.   
$$&lt;/div&gt;
&lt;p&gt;これがフーリエ変換対となり、一方に他方を代入するとちゃんと逆に戻る事が確認できる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split}
f[n] &amp;amp;= \frac{\omega_{s}}{2\pi N}\sum_{k=0}^{N-1}F[k]\exp\left(j\frac{2\pi nk}{N}\right) \\
&amp;amp;= \frac{2\pi t_{s}}{2\pi t_{s}N}\sum_{k=0}^{N-1}\left\{ \sum^{N-1}_{n^\prime=0} f[n^\prime] \exp \left(-j \frac{2\pi n^\prime k}{N} \right) \right\} \exp\left(j\frac{2\pi nk}{N}\right) \\
&amp;amp;= \frac{1}{N} \sum_{n^{\prime}=0}^{N-1} f[n^\prime] \sum_{k=0}^{N-1} \exp\left[ -j (n-n^\prime) \frac{2\pi k}{N} \right]
\end{split} 
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\sum_{k=0}^{N-1} \exp\left[ -j (n-n^\prime) \frac{2\pi k}{N} \right]\)&lt;/span&gt;の値ついては&lt;span class="math"&gt;\(k\)&lt;/span&gt;の積分&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{N}_{0} \exp\left[ -j (n-n^\prime) \frac{2\pi k}{N} \right] dk
$$&lt;/div&gt;
&lt;p&gt;と考えれば、&lt;span class="math"&gt;\(n=n^\prime\)&lt;/span&gt;の時は明らかに&lt;span class="math"&gt;\(N\)&lt;/span&gt;であり、残りの&lt;span class="math"&gt;\(n \neq n^\prime\)&lt;/span&gt;の時は&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{split} 
\int^{N}_{0} \exp\left[ -j (n-n^\prime) \frac{2\pi k}{N} \right] dk 
&amp;amp;= - \frac{1}{j(n-n^\prime)\frac{2\pi}{N}} 
\left[ \exp\left[ -j(n-n^\prime)\frac{2\pi k}{N} \right] \right]_{0}^{N} \\
&amp;amp;= - \frac{1}{j(n-n^\prime)\frac{2\pi}{N}} \left\{ \exp[-j2(n-n^\prime)\pi] - \exp(0)\right\} \\
&amp;amp;= 0
\end{split} 
$$&lt;/div&gt;
&lt;p&gt;となるので、最終的に&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{1}{N} \sum_{n^{\prime}=0}^{N-1} f[n^\prime] \sum_{k=0}^{N-1} \exp\left[ -j (n-n^\prime) \frac{2\pi k}{N} \right] = \frac{1}{N} f[n] N = f[n]
$$&lt;/div&gt;
&lt;p&gt;を得る。
また&lt;span class="math"&gt;\(t_{s}=1\)&lt;/span&gt;とおくと、&lt;span class="math"&gt;\(\omega_{s} = 2\pi\)&lt;/span&gt;となって、DFTのよく見る変換式が得られる：&lt;/p&gt;
&lt;div class="math"&gt;$$
\left\{ \begin{array}{l}
\displaystyle F[k] = \sum^{N-1}_{n=0} f[n] \exp \left(-j \frac{2\pi nk}{N} \right) \\
\displaystyle f[n] = \frac{1}{N} \sum_{k=0}^{N-1} F[k] \exp\left(j \frac{2\pi nk}{N} \right)
\end{array} \right.   
$$&lt;/div&gt;
&lt;p&gt;これらの式を実装するのは簡単である。じゃあ、実装しようか…（暗黒微笑）&lt;/p&gt;
&lt;p&gt;（デルタ関数から導く方法だと、どうしても正規化定数&lt;span class="math"&gt;\(1/N\)&lt;/span&gt;が出てこない。正規化定数は本質的では無いとかいうけど、計算上は無視できない。）&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="記事"></category><category term="DFT"></category><category term="信号処理"></category></entry><entry><title>2020-04-22</title><link href="/2020-04-22.html" rel="alternate"></link><published>2020-04-22T11:34:00+09:00</published><updated>2020-04-22T12:10:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-22:/2020-04-22.html</id><summary type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;div class="section" id="signed-lms2-2"&gt;
&lt;h2&gt;Signed-LMSの2階微分 その2&lt;/h2&gt;
&lt;p&gt;早速既存研究が無いか見ている。二乗誤差最小化のLMSでもヘッセ行列の逆行列の計算負荷が高いから使わん、という論調がほとんど。Signed-LMSについては今の所、微分してるところも見てない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.slideshare.net/mentelibre/neural-network-widrowhoff-learning-adaline-hagan-lms"&gt;NEURAL NETWORK Widrow-Hoff Learning Adaline Hagan LMS&lt;/a&gt; 観測分散行列がヘッセ行列に一致することが書いてあった。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www1.coe.neu.edu/~erdogmus/publications/J013_NEUNET_SpIssueIJCNN03_EWCLMS_Yadu.pdf"&gt;Stochastic error whitening algorithm for linear filter estimation with noisy data …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;div class="section" id="signed-lms2-2"&gt;
&lt;h2&gt;Signed-LMSの2階微分 その2&lt;/h2&gt;
&lt;p&gt;早速既存研究が無いか見ている。二乗誤差最小化のLMSでもヘッセ行列の逆行列の計算負荷が高いから使わん、という論調がほとんど。Signed-LMSについては今の所、微分してるところも見てない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.slideshare.net/mentelibre/neural-network-widrowhoff-learning-adaline-hagan-lms"&gt;NEURAL NETWORK Widrow-Hoff Learning Adaline Hagan LMS&lt;/a&gt; 観測分散行列がヘッセ行列に一致することが書いてあった。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www1.coe.neu.edu/~erdogmus/publications/J013_NEUNET_SpIssueIJCNN03_EWCLMS_Yadu.pdf"&gt;Stochastic error whitening algorithm for linear filter estimation with noisy data&lt;/a&gt; 評価関数として絶対値が入ったものを使っている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faculty.cord.edu/kamel/09S-380/Presentations/LMS.pdf"&gt;The Least Mean Squares Algorithm&lt;/a&gt; 分かりやすめな解説。そうか、ウィーナーフィルタか。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;行列 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; が正則にならない件について、これ正則化すればいいんじゃねと思い立つ。要は &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; を正則化パラメータとして &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; に対して逆行列を求めていく。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;多分、係数側に正則項を追加することになるはず。&lt;span class="math"&gt;\(\min \mathrm{E}[|\varepsilon(n)|] + \lambda ||\ve{h}||_{2}\)&lt;/span&gt; のような定式化か？&lt;/li&gt;
&lt;li&gt;それでも逆行列 &lt;span class="math"&gt;\((\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I})^{-1}\)&lt;/span&gt; を求めるのは骨が折れそう。そこで、自然勾配学習で使っていた適応的自然勾配学習法（ &lt;a class="reference external" href="https://bsi-ni.brain.riken.jp/database/file/274/280.pdf"&gt;Singularities Affect Dynamics of Learning in Neuromanifolds&lt;/a&gt; より）が使えそう。具体的には、次の式で自然勾配を適応的に求めていく。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{G}_{t+1}^{-1} = (1 + \varepsilon_{t}) \ve{G}_{t}^{-1} - \varepsilon_{t} \ve{G}_{t}^{-1} \parfrac{J(\ve{h})}{\ve{h}} \left( \ve{G}_{t}^{-1} \parfrac{J(\ve{h})}{\ve{h}} \right)^{\mathsf{T}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\varepsilon_{t}\)&lt;/span&gt; は小さな定数。『情報幾何の新展開』では、カルマンフィルタ由来らしい。うーん、もう試してみたいな。&lt;/p&gt;
&lt;div class="section" id="ve-x-ve-x-mathsf-t-lambda-ve-i"&gt;
&lt;h3&gt;（念の為） &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; が正則行列になる理由&lt;/h3&gt;
&lt;p&gt;すぐに思い出せなくてヒヤッとしたのでここで示しておく。&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; は対称行列だから、直交行列 &lt;span class="math"&gt;\(\ve{P}\)&lt;/span&gt; （&lt;span class="math"&gt;\(\ve{P}^{-1} = \ve{P}^{\mathsf{T}}\)&lt;/span&gt; ）と固有値を並べた対角行列 &lt;span class="math"&gt;\(\ve{\Lambda}\)&lt;/span&gt; を用いて、&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} = \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P}\)&lt;/span&gt; と対角化できる。よって、&lt;span class="math"&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; なる定数を用いた時、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I} &amp;amp;= \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} + \lambda \ve{P}^{\mathsf{T}} \ve{P} \\
&amp;amp;= \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} + \ve{P}^{\mathsf{T}} \lambda \ve{I} \ve{P} \\
&amp;amp;= \ve{P}^{\mathsf{T}} (\ve{\Lambda} + \lambda \ve{I}) \ve{P}
\end{align*}
&lt;/div&gt;
&lt;p&gt;また、任意のベクトル &lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt; を使った時、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{v}^{\mathsf{T}} \ve{X} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= (\ve{X}^{\mathsf{T}} \ve{v})^{\mathsf{T}} \ve{X}^{\mathsf{T}} \ve{v} = ||\ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} \\
\ve{v}^{\mathsf{T}} \ve{X} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= \ve{v}^{\mathsf{T}} \ve{P}^{\mathsf{T}} \ve{\Lambda} \ve{P} \ve{v} = \sum_{i}^{N} \ve{\Lambda}_{ii} (\ve{Pv})_{i}^{2} \\
\Rightarrow ||\ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} &amp;amp;= \sum_{i}^{N} \ve{\Lambda}_{ii} (\ve{Pv})_{i}^{2} \geq 0
\end{align*}
&lt;/div&gt;
&lt;p&gt;の関係式が成り立つ。最後の不等式が成り立つには、全ての &lt;span class="math"&gt;\(i\)&lt;/span&gt; に対して &lt;span class="math"&gt;\(\ve{\Lambda}_{ii} \geq 0\)&lt;/span&gt; でなければならない。よって &lt;span class="math"&gt;\(\ve{XX}^{\mathsf{T}}\)&lt;/span&gt; の固有値は全て非負。&lt;/p&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\ve{P}^{\mathsf{T}} (\ve{\Lambda} + \lambda \ve{I}) \ve{P}\)&lt;/span&gt; に注目すると、全ての固有値に &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; が足されていることが分かる。&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; は正だから、 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; の固有値は全て正になり正定値行列となる。正定値行列は正則だから、 &lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}} + \lambda \ve{I}\)&lt;/span&gt; は正則行列。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt;フィッシャー情報行列とヘッセ行列と分散行列の絡みについて&lt;/h3&gt;
&lt;p&gt;以下の記事が非常にわかりやすい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/"&gt;Fisher Information Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/"&gt;Natural Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;結論、ラプラス分布に従う残差を仮定した最尤推定において、観測分散行列はフィッシャー情報行列に一致し、その逆行列は自然勾配に該当するはず。つうかニュートン法の特殊ケースに見えるがどうなんでしょ。フィッシャー情報行列がヘッセ行列に見えるんだが、定義通り（対数尤度のヘッセ行列）そうだよな。指数族の最尤推定をニュートン法で解こうとしたら全部自然勾配学習法にならね？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="todo"&gt;
&lt;h2&gt;TODO&lt;/h2&gt;
&lt;p&gt;評価のことを考えて行きたい。固定した信号（答えが分かっている信号。乱数固定。）を使ったときに、誤差平面と勾配はどうなっている？フィルタの次元は2ぐらいにして、フィルタを固定して各統計量がどうなっているかプロットする。まずは絶対値残差と勾配の観察が重要に思える（もちろん、2次の最小化ケースも重要）。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;古いQiita記事を移行せねば…&lt;/li&gt;
&lt;li&gt;Jupiterええな。使ってみるか&lt;/li&gt;
&lt;li&gt;評価がまとまったら結果共有に入りたい。&lt;/li&gt;
&lt;li&gt;OMPを使う。&lt;/li&gt;
&lt;li&gt;メッセージパッシング使えない？&lt;ul&gt;
&lt;li&gt;何らかの確率モデル化をせよ、というふうに受け取った。&lt;/li&gt;
&lt;li&gt;AMP, Survay-Propagation（三村さん、樺島さん）がありえる。&lt;/li&gt;
&lt;li&gt;→ AMP, Survay-Propagationについて調査すべし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;いろんな論文で自然勾配をどうやって定義しているか要観察。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category><category term="IRLS"></category><category term="LMS"></category><category term="Fisher Information Matrix"></category><category term="Hessian"></category><category term="Natural Gradient"></category></entry><entry><title>2020-04-21</title><link href="/2020-04-21.html" rel="alternate"></link><published>2020-04-21T12:10:00+09:00</published><updated>2020-04-21T12:10:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-21:/2020-04-21.html</id><summary type="html">&lt;div class="section" id="mathrm-e-varepsilon-n-x-n-m"&gt;
&lt;h2&gt;残差勾配 &lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; の挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が大きいときは無視できるのでは？ なお、長時間平均値は0に収束していることを見た。
&lt;span class="math"&gt;\(m\)&lt;/span&gt; をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;ガチャガチャ弄ってるってるけど示唆があんまりない。&lt;/li&gt;
&lt;li&gt;低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; は &lt;span class="math"&gt;\(m\)&lt;/span&gt; を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。&lt;/li&gt;
&lt;li&gt;同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一のsin波を等価させたときはわかりやすい。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="440.0Hzのsin波に対する各タップの平均勾配変化グラフ" src="./images/sin_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;440.0Hzのsin波に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="ボイス対する各タップの平均勾配変化グラフ" src="./images/voice_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ボイス対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="ピアノ演奏に対する各タップの平均勾配変化グラフ" src="./images/ruriko_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ピアノ演奏に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="signed-lms2"&gt;
&lt;h2&gt;Signed-LMSの目的関数の2階微分&lt;/h2&gt;
&lt;p&gt;勇気を出してやってみる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2 …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="mathrm-e-varepsilon-n-x-n-m"&gt;
&lt;h2&gt;残差勾配 &lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; の挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が大きいときは無視できるのでは？ なお、長時間平均値は0に収束していることを見た。
&lt;span class="math"&gt;\(m\)&lt;/span&gt; をずらした時の平均値の様子を見る。どこかで影響が小さくなって打ち切れるはず。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;ガチャガチャ弄ってるってるけど示唆があんまりない。&lt;/li&gt;
&lt;li&gt;低次（〜10）の係数は大きく変動する傾向。しかし、次に述べるピッチなどに影響しているのか、全てに当てはまる傾向ではない。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\varepsilon(n) x(n - m)]\)&lt;/span&gt; は &lt;span class="math"&gt;\(m\)&lt;/span&gt; を大きくすれば単調減少するわけではない。音源依存で傾向が異なる。ピッチ？か何かに反応して大きくなる場合がある。&lt;/li&gt;
&lt;li&gt;同一発音区間では、フィルタ係数の符号は同一になる傾向が見られる。単一のsin波を等価させたときはわかりやすい。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="440.0Hzのsin波に対する各タップの平均勾配変化グラフ" src="./images/sin_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;440.0Hzのsin波に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="ボイス対する各タップの平均勾配変化グラフ" src="./images/voice_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ボイス対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="ピアノ演奏に対する各タップの平均勾配変化グラフ" src="./images/ruriko_mean_gradient.png" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;ピアノ演奏に対する各タップの平均勾配変化&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="signed-lms2"&gt;
&lt;h2&gt;Signed-LMSの目的関数の2階微分&lt;/h2&gt;
&lt;p&gt;勇気を出してやってみる。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;符号関数を &lt;span class="math"&gt;\(\tanh(Tx)\)&lt;/span&gt; で近似して微分してみる（&lt;span class="math"&gt;\(T\)&lt;/span&gt; は温度パラメータで、&lt;span class="math"&gt;\(\tanh(Tx)\)&lt;/span&gt; を &lt;span class="math"&gt;\(T \to \infty\)&lt;/span&gt; ならしめれば符号関数に近づく）と、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{d}{dx} \tanh(Tx) = T (\tanh(Tx))^{\prime} = T(1 - \tanh^{2}(Tx))
\end{equation*}
&lt;/div&gt;
&lt;p&gt;さて、 &lt;span class="math"&gt;\(1 - \tanh^{2}(Tx)\)&lt;/span&gt; に注目すると、&lt;span class="math"&gt;\(T\)&lt;/span&gt; の極限では &lt;span class="math"&gt;\(x = 0\)&lt;/span&gt; を除き0を取るが、&lt;span class="math"&gt;\(x = 0\)&lt;/span&gt; において1を取る。よってこれはインパルス関数になる（極限と微分操作を交換したけどやかましいことは暗黙で...）。&lt;/p&gt;
&lt;p&gt;符号関数を微分するとインパルス関数が出てくることについては &lt;a class="reference external" href="https://teenaka.at.webry.info/201301/article_10.html"&gt;超関数的微分_δ関数関連（２）&lt;/a&gt; を見るのが早いかも。以下では、その話に従って、&lt;span class="math"&gt;\(\frac{d}{dx} \mathrm{sign}(x) = 2\delta(x)\)&lt;/span&gt; とする。&lt;/p&gt;
&lt;p&gt;さて、今一度評価関数 &lt;span class="math"&gt;\(\mathrm{E}[|\varepsilon(n)|]\)&lt;/span&gt; の偏微分と2階の偏導関数を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{}{h(m)} \mathrm{E}[|\varepsilon(n)|] &amp;amp;= \mathrm{E}\left[ \parfrac{}{h(m)} |\varepsilon(n)| \right] \\
&amp;amp;= \mathrm{E}\left[ \left\{ \parfrac{}{h(m)} \varepsilon(n) \right\} \mathrm{sign}[\varepsilon(n)] \right] \\
&amp;amp;= -\mathrm{E}\left[ \mathrm{sign}[\varepsilon(n)]  x(n - m) \right] \\
\frac{\partial^{2}}{\partial h(m) \partial h(k)} \mathrm{E}[|\varepsilon(n)|] &amp;amp;= - \parfrac{}{h(k)} \mathrm{E}\left[ \mathrm{sign}[\varepsilon(n)]  x(n - m) \right] \\
&amp;amp;= - \mathrm{E}\left[ \left\{ \parfrac{}{h(k)} \varepsilon(n) \right\} 2\delta(\varepsilon(n)) x(n - m) \right] \\
&amp;amp;= 2\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]
\end{align*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right]\)&lt;/span&gt; に注目する。これは &lt;span class="math"&gt;\(\varepsilon(n) = 0\)&lt;/span&gt; のときだけ和を取る演算だ。&lt;span class="math"&gt;\(\sum\)&lt;/span&gt; を用いると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\mathrm{E}\left[ \delta(\varepsilon(n)) x(n - m) x(n - k) \right] = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 1, \varepsilon(n) = 0}^{N} x(n - m) x(n - k)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;という計算に該当する。厳密計算は &lt;span class="math"&gt;\(\varepsilon(n) = 0\)&lt;/span&gt; なる &lt;span class="math"&gt;\(n\)&lt;/span&gt; を見つけたら足していく感じでいいと思うけど、今は &lt;span class="math"&gt;\(\varepsilon(n)\)&lt;/span&gt; はラプラス分布に従うと仮定している。だからラプラス分布に従って &lt;span class="math"&gt;\(P(\varepsilon(n) = 0) = \frac{1}{2\lambda}\)&lt;/span&gt; （分散 &lt;span class="math"&gt;\(2\lambda^{2}\)&lt;/span&gt; ）の重み付けをして計算してしまって良いように見えるのだがどうなんだろう。なんか怪しくて考え続けている。&lt;/p&gt;
&lt;p&gt;もし適応フィルタに組み込むなら、残差が0になったら上の式に従ってヘッセ行列を更新し、ニュートン法を使い続ける。これは試してみたい。問題はヘッセ行列が逆行列を持つかというところ…4-20で半正定値であることは確認したが正定値とは限らない。共役勾配法を検討する必要があるかも。&lt;span class="math"&gt;\(\ve{X}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; は正則になるとは思えない…。（軽く試したけどすぐにだめな例が見つかった。）&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;他の頂いたアイディア&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;周波数領域に一旦飛ばすのはあり？&lt;ul&gt;
&lt;li&gt;ありだけど計算量が高い。圧縮率が上がるのであれば大アリ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;確率的PCAとか使えない？辞書は小さくて済む。&lt;/li&gt;
&lt;li&gt;線形ダイナミクスにより上手く定式化できない？&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;優先度低&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;出す学会については HND 先生に聞くこと。&lt;ul&gt;
&lt;li&gt;相談する機会はどこかで絶対に必要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;著作権処理済み音源データベースについて相談&lt;ul&gt;
&lt;li&gt;→ 自分で情報をまとめて、申し込んでいいかというところまで進めるべし。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://staff.aist.go.jp/m.goto/PAPER/SIGMUS200205goto.pdf"&gt;RWC 研究用音楽データベース: 音楽ジャンルデータベースと楽器音データベース&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://staff.aist.go.jp/m.goto/RWC-MDB/index-j.html"&gt;RWC研究用音楽データベース&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;→ 進めた。動けるようになったら書類をまとめていく。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Donohoさんなどが圧縮センシングの文脈で既にやりきってない？&lt;ul&gt;
&lt;li&gt;ありえる。調査すべし。&lt;/li&gt;
&lt;li&gt;→ ライス大学では成果をすべて公開しているから見るだけ見たほうが良い。&lt;/li&gt;
&lt;li&gt;→ &lt;a class="reference external" href="http://dsp.rice.edu/cs/"&gt;http://dsp.rice.edu/cs/&lt;/a&gt; を見よ。&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://hal.archives-ouvertes.fr/hal-00424165/document"&gt;Compressed sensing block MAP-LMS adaptive filter for sparse channel estimation and a bayesian Cramer-Rao bound&lt;/a&gt; 残差はガウス分布としてるけどクラメル-ラオ下限との絡みを述べている。何か重要そう。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.dbabacan.info/papers/babacan_CS.pdf"&gt;Bayesian Compressive Sensing Using Laplace Priors&lt;/a&gt; これもパラメータの事前分布にラプラス分布を導入してベイズ推定するもの。残差ではないはず。&lt;/li&gt;
&lt;li&gt;「L1」, 「Laplace」, 「residual」, 「lossless」で検索したけどスパース解を求めるものばかり。今のところはセーフ？&lt;/li&gt;
&lt;li&gt;→ 継続して調査はする。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category><category term="IRLS"></category><category term="LMS"></category></entry><entry><title>2020-04-20</title><link href="/2020-04-20.html" rel="alternate"></link><published>2020-04-20T14:10:00+09:00</published><updated>2020-04-21T12:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-20:/2020-04-20.html</id><summary type="html">&lt;div class="section" id="irls"&gt;
&lt;h2&gt;IRLSの更新式について&lt;/h2&gt;
&lt;p&gt;MathJaxの環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 &lt;tt class="docutils literal"&gt;newcommand&lt;/tt&gt; を行えばずっと使えるみたい。便利。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逐次的更新の件について。IRLSでは以下の評価関数 &lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; の最小化を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
J(\ve{\beta}) = \sum^{M}_{i = 1} w_{i} (y_{i …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="irls"&gt;
&lt;h2&gt;IRLSの更新式について&lt;/h2&gt;
&lt;p&gt;MathJaxの環境を確認しつつ使用中。プリアンブルが無いけどページ内で一回 &lt;tt class="docutils literal"&gt;newcommand&lt;/tt&gt; を行えばずっと使えるみたい。便利。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand\ve[1]{\boldsymbol{#1}}
\newcommand\parfrac[2]{\frac{\partial #1}{\partial #2}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;逐次的更新の件について。IRLSでは以下の評価関数 &lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; の最小化を考える。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
J(\ve{\beta}) = \sum^{M}_{i = 1} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})^{2}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここで &lt;span class="math"&gt;\(M\)&lt;/span&gt; は観測数。これは二次式だから評価関数は凸関数になる。早速 &lt;span class="math"&gt;\(\ve{\beta}\)&lt;/span&gt; で偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{}{\ve{\beta}} J(\ve{\beta}) &amp;amp;= \sum^{M}_{i = 1} w_{i} 2 \left(- \frac{\partial}{\partial \ve{\beta}} \innerp{\ve{\beta}}{\ve{x}_{i}} \right) (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \\
 &amp;amp;= -2 \sum^{M}_{i = 1} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \ve{x}_{i}
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\parfrac{}{\ve{\beta}} J(\ve{\beta}) = 0\)&lt;/span&gt; とおくと、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\sum_{i = 1}^{M} w_{i} \innerp{\ve{\beta}}{\ve{x}_{i}} \ve{x}_{i} &amp;amp;= \sum_{i = 1}^{M} w_{i} y_{i} \ve{x}_{i} \\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{c}
   w_{1} \innerp{\ve{\beta}}{\ve{x}_{1}} \\
   \vdots     \\
   w_{M} \innerp{\ve{\beta}}{\ve{x}_{M}}
 \end{array}
\right]
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{c}
   w_{1}y_{1} \\
   \vdots     \\
   w_{M}y_{M}
 \end{array}
\right]
\\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   \innerp{\ve{\beta}}{\ve{x}_{1}} \\
   \vdots     \\
   \innerp{\ve{\beta}}{\ve{x}_{M}}
 \end{array}
\right]
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   y_{1} \\
   \vdots     \\
   y_{M}
 \end{array}
\right]
\\
\iff
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\left[
 \begin{array}{c}
   \ve{x}_{1}^{\mathsf{T}} \\
   \vdots     \\
   \ve{x}_{M}^{\mathsf{T}}
 \end{array}
\right]
\ve{\beta}
&amp;amp;=
\left[ \ve{x}_{1} ... \ve{x}_{M} \right]
\left[
 \begin{array}{ccc}
   w_{1}  &amp;amp; \dots  &amp;amp; 0      \\
   \vdots &amp;amp; \ddots &amp;amp; \vdots \\
   0      &amp;amp; \dots  &amp;amp; w_{M}
 \end{array}
\right]
\ve{y}
\\
\iff
\ve{X} \ve{W} \ve{X}^{\mathsf{T}} \ve{\beta} &amp;amp;= \ve{X} \ve{W} \ve{y}
\end{align*}
&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\ve{X}\ve{W}\ve{X}^{\mathsf{T}}\)&lt;/span&gt; が正則（TODO: &lt;span class="math"&gt;\(\ve{X}\)&lt;/span&gt; が行フルランク、かつ &lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; が正則なら行けそうに見えるけど本当か？）の場合は閉形式で係数が求まる:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{\beta} = (\ve{X} \ve{W} \ve{X}^{\mathsf{T}})^{-1} \ve{X} \ve{W} \ve{y}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ここまでは一般論。さて、更新式に注目する。&lt;span class="math"&gt;\(\beta_{j}\)&lt;/span&gt; だけで偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\parfrac{J(\ve{\beta})}{\beta_{j}} &amp;amp;= \sum_{i = 1}^{M} \parfrac{}{\beta_{j}} w_{i} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})^{2} \\
&amp;amp;= -2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})
\end{align*}
&lt;/div&gt;
&lt;p&gt;残差のL1ノルム最小化を考えるときは &lt;span class="math"&gt;\(w_{i} = \frac{1}{|y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}|}\)&lt;/span&gt; とおくので代入すると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\parfrac{J(\ve{\beta})}{\beta_{j}} = -2 \sum_{i = 1}^{M} (\ve{x}_{i})_{j} \mathrm{sign}(y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;瞬間値（&lt;/strong&gt; &lt;span class="math"&gt;\(M=1\)&lt;/span&gt; &lt;strong&gt;とする）を考えるとSigned-LMSの更新式そのものになっている。&lt;/strong&gt; 和を取ると平均操作に近いから、LMSアルゴリズムと考えていることは同じ。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\parfrac{J(\ve{\beta})}{\beta_{j}}\)&lt;/span&gt; を更に &lt;span class="math"&gt;\(\beta_{k}\)&lt;/span&gt; で偏微分してみると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial^{2} J(\ve{\beta})}{\partial \beta_{j} \partial \beta_{k}} &amp;amp;= -2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} \parfrac{}{\beta_{k}} (y_{i} - \innerp{\ve{\beta}}{\ve{x}_{i}}) \\
&amp;amp;= 2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (\ve{x}_{i})_{k} \\
&amp;amp;= 2 \left[ (\ve{x}_{1})_{j} \dots (\ve{x}_{M})_{j} \right]
 \left[
  \begin{array}{c}
    w_{1} (\ve{x}_{1})_{k} \\
    \vdots     \\
    w_{M} (\ve{x}_{M})_{k}
  \end{array}
 \right]
 = 2 \left[ (\ve{x}_{1})_{j} \dots (\ve{x}_{M})_{j} \right] \ve{W}
 \left[
  \begin{array}{c}
    (\ve{x}_{1})_{k} \\
    \vdots     \\
    (\ve{x}_{M})_{k}
  \end{array}
 \right]
\end{align*}
&lt;/div&gt;
&lt;p&gt;2次式が出てくるのがわかる（&lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; は計量だ）。そして &lt;span class="math"&gt;\((\ve{H})_{jk} = \frac{\partial^{2} J(\ve{\beta})}{\partial \beta_{j} \partial \beta_{k}}\)&lt;/span&gt; なるヘッセ行列 &lt;span class="math"&gt;\(\ve{H}\)&lt;/span&gt; は以下:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\ve{H} = 2 \ve{X} \ve{W} \ve{X}^{\mathsf{T}}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;ヘッセ行列の性質により関数の最小値・最大値の存在がわかる。対称行列なのは間違いない（&lt;span class="math"&gt;\((\ve{X})_{ij} = (\ve{X})_{ji}\)&lt;/span&gt; は自明）。（固有値分解とは見れない。&lt;span class="math"&gt;\(\ve{H}\)&lt;/span&gt; は &lt;span class="math"&gt;\(N \times N\)&lt;/span&gt; の行列であるのに対して、&lt;span class="math"&gt;\(\ve{X}\)&lt;/span&gt; は &lt;span class="math"&gt;\(N \times M\)&lt;/span&gt; の行列。&lt;span class="math"&gt;\(\ve{X} \ve{X}^{\mathsf{T}}\)&lt;/span&gt; は平均化、除算を抜いた分散共分散行列になり半正定値行列。）また、任意のベクトル &lt;span class="math"&gt;\(\ve{v}\)&lt;/span&gt; に対して、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ve{v}^{\mathsf{T}} \ve{X} \ve{W} \ve{X}^{\mathsf{T}} \ve{v} &amp;amp;= \ve{v}^{\mathsf{T}} \ve{X} \ve{W}^{1/2} \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} \\
&amp;amp;= (\ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v})^{\mathsf{T}} \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} \\
&amp;amp;= || \ve{W}^{1/2} \ve{X}^{\mathsf{T}} \ve{v} ||_{2}^{2} \geq 0
\end{align*}
&lt;/div&gt;
&lt;p&gt;だから、&lt;span class="math"&gt;\(\ve{W}\)&lt;/span&gt; が半正定値（&lt;span class="math"&gt;\(\iff\)&lt;/span&gt; すべての重みが非負）ならばヘッセ行列は半正定値行列で、極小値が最小値になる。また、&lt;span class="math"&gt;\(J(\ve{\beta})\)&lt;/span&gt; は凸関数（半正定値だから狭義の凸関数ではない）。
もう少しヘッセ行列を見る。ヘッセ行列を上手く使えたらニュートン法で解けそうな気がして。&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
(\ve{H})_{jk} = 2 \sum_{i = 1}^{M} w_{i} (\ve{x}_{i})_{j} (\ve{x}_{i})_{k}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;より、スペクトル分解的に見ると、&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{1}{2} \ve{H} &amp;amp;=
w_{1} \left[
  \begin{array}{ccc}
    (\ve{x}_{1})_{1}^{2}  &amp;amp; \dots &amp;amp; (\ve{x}_{1})_{1} (\ve{x}_{1})_{N} \\
    \vdots &amp;amp; \ddots &amp;amp; \vdots \\
    (\ve{x}_{1})_{N} (\ve{x}_{1})_{1} &amp;amp; \dots &amp;amp; (\ve{x}_{1})_{N}^{2} \\
  \end{array}
 \right]
 + \dots +
 w_{M} \left[
  \begin{array}{ccc}
    (\ve{x}_{M})_{1}^{2}  &amp;amp; \dots &amp;amp; (\ve{x}_{M})_{1} (\ve{x}_{M})_{N} \\
    \vdots &amp;amp; \ddots &amp;amp; \vdots \\
    (\ve{x}_{M})_{N} (\ve{x}_{M})_{1} &amp;amp; \dots &amp;amp; (\ve{x}_{M})_{N}^{2} \\
  \end{array}
 \right] \\
 &amp;amp;= w_{1} \ve{x}_{1} \ve{x}_{1}^{\mathsf{T}} + \dots + w_{M} \ve{x}_{M} \ve{x}_{M}^{\mathsf{T}} \\
 &amp;amp;= \sum_{i = 1}^{M} w_{i} \ve{x}_{i} \ve{x}_{i}^{\mathsf{T}}
\end{align*}
&lt;/div&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;信号処理的には &lt;span class="math"&gt;\(\ve{x}_{1}, \ve{x}_{2}, \dots \ve{x}_{M}\)&lt;/span&gt; は系列で現れる。&lt;/li&gt;
&lt;li&gt;LMSフィルタでは &lt;span class="math"&gt;\(i = 1\)&lt;/span&gt; の時だけを考えていたと考えられれる。 &lt;span class="math"&gt;\(i = 2,\dots,M\)&lt;/span&gt; のときの影響は少ないのではないかと思う。&lt;/li&gt;
&lt;li&gt;FIRフィルタを考えるのならば、各 &lt;span class="math"&gt;\(\ve{x}_{1}\)&lt;/span&gt; は入ってきた1次元信号データを時系列順に並べたものだから、直前のベクトル &lt;span class="math"&gt;\(\ve{x}_{2}\)&lt;/span&gt; を使えそうな構造に見える。&lt;/li&gt;
&lt;li&gt;上の仮定を使ってヘッセ行列の逆行列 &lt;span class="math"&gt;\(\ve{H}^{-1}\)&lt;/span&gt; を逐次近似計算できない？&lt;/li&gt;
&lt;li&gt;分散共分散行列がほぼヘッセ行列になってるけどこれは何？&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.iim.cs.tut.ac.jp/~kanatani/papers/jcov.pdf"&gt;金谷さんの解説&lt;/a&gt; にそれとなく解説がある。フィッシャー情報行列との関連もある。。。クラメル・ラオの下限についてわかりやすい説明あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://web.econ.keio.ac.jp/staff/bessho/lecture/09/091014ML.pdf"&gt;最尤法&lt;/a&gt; にもそれとなく解説あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://oku.edu.mie-u.ac.jp/~okumura/stat/141115.html"&gt;奥村さん&lt;/a&gt; もあり。観測からヘッセ行列を構成できる？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;そして自然勾配のアイディアが出てくる。自然勾配を使ったLMSアルゴリズムは…あった…&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://scholarsmine.mst.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&amp;amp;httpsredir=1&amp;amp;article=2780&amp;amp;context=ele_comeng_facwork"&gt;Normalized Natural Gradient Adaptive Filtering for Sparse and Nonsparse Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.7538&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;甘利先生による解説&lt;/a&gt; で、LMSアルゴリズム含めて大まかなところはだいたい言ってる。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/profile/Ligang_Liu3/publication/44098179_On_Improvement_of_Proportionate_Adaptive_Algorithms_for_Sparse_Impulse_Response/links/00b495315266ab9cfd000000.pdf"&gt;高知工科大学の博論&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ワンチャンスL1残差最小化はやってないかも。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;前のMTGで言われたことの整理&lt;/li&gt;
&lt;li&gt;分散行列、ヘッセ行列、フィッシャー情報行列、自然勾配の整理&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/"&gt;Fisher Information Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OMPが気になる。試してみたい。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category><category term="IRLS"></category></entry><entry><title>2020-04-19</title><link href="/2020-04-19.html" rel="alternate"></link><published>2020-04-19T19:30:00+09:00</published><updated>2020-04-20T14:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-19:/2020-04-19.html</id><summary type="html">&lt;div class="section" id="irls-iteratively-reweighted-least-squares-2"&gt;
&lt;h2&gt;IRLS(Iteratively Reweighted Least Squares) その2&lt;/h2&gt;
&lt;p&gt;理論ばっかり追っていて悶々してきたので、IRLSでL1残差最小化が解けないか実験してみる。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/kibo35/sparse-modeling/blob/master/ch05.ipynb"&gt;第5章 厳密解から近似解へ&lt;/a&gt; に『スパースモデリング』5章のPython実装あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/kibo35/items/66ec4479b0899ea4987d#irlsの概要"&gt;スパースモデリング：第3章 追跡アルゴリズム&lt;/a&gt; は『スパースモデリング』3章のPython実装。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IRLSの実装は &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; を参考に。Pythonで簡単にできた。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;

&lt;span class="c1"&gt;# IRLS法によりPhi @ x = yのスパース解を求める&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;EPSILON&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# 重みの計算&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# 小さくなりすぎた重みは打ち切る&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;EPSILON …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="irls-iteratively-reweighted-least-squares-2"&gt;
&lt;h2&gt;IRLS(Iteratively Reweighted Least Squares) その2&lt;/h2&gt;
&lt;p&gt;理論ばっかり追っていて悶々してきたので、IRLSでL1残差最小化が解けないか実験してみる。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/kibo35/sparse-modeling/blob/master/ch05.ipynb"&gt;第5章 厳密解から近似解へ&lt;/a&gt; に『スパースモデリング』5章のPython実装あり。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/kibo35/items/66ec4479b0899ea4987d#irlsの概要"&gt;スパースモデリング：第3章 追跡アルゴリズム&lt;/a&gt; は『スパースモデリング』3章のPython実装。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IRLSの実装は &lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; を参考に。Pythonで簡単にできた。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;

&lt;span class="c1"&gt;# IRLS法によりPhi @ x = yのスパース解を求める&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;EPSILON&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# 重みの計算&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# 小さくなりすぎた重みは打ち切る&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;EPSILON&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EPSILON&lt;/span&gt;
    &lt;span class="c1"&gt;# 対角行列に展開&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;order&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# 更新後の係数: Phi.T @ W @ Phi @ x = Phi.T @ W @ y の解&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;DIMENSION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

    &lt;span class="c1"&gt;# 解ベクトル&lt;/span&gt;
    &lt;span class="n"&gt;X_ANSWER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;xhistory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# 観測を生成&lt;/span&gt;
    &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DIMENSION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;X_ANSWER&lt;/span&gt;
    &lt;span class="c1"&gt;# 加法的雑音を重畳&lt;/span&gt;
    &lt;span class="c1"&gt;# yrand = y + numpy.random.normal(0, 0.3, (NUM_SAMPLES, 1))&lt;/span&gt;
    &lt;span class="n"&gt;yrand&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;laplace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;emp_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# IRLSを繰り返し適用&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_ITERATION&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;irls_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yrand&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;xhistory&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;
        &lt;span class="n"&gt;emp_error&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yrand&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;NUM_SAMPLES&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;実装は楽だったけど、誤差解析が沼。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;誤差を重畳してみると、真の誤差と経験誤差が当然一致しない。&lt;/li&gt;
&lt;li&gt;経験誤差的には局所解に入っている印象。&lt;/li&gt;
&lt;li&gt;サンプル数が少ないと大域最小解に入らないケースあり（経験誤差曲面の最小値が真の誤差の曲面の最小値に不一致）&lt;/li&gt;
&lt;li&gt;経験誤差の曲面は二次曲線に見える。（2次式の最小化を考えているから当然のはず。）&lt;/li&gt;
&lt;li&gt;最小二乗解よりも誤差が悪い時がある。最小二乗解はorder=2とすれば良くて、その時重み行列Wは単位行列になり、普通の最小二乗法と一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;思いつき:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;IRLSは評価関数の最小化を考える時閉形式で求まるので何も考えない。パラメータに関してもう一度微分できるのでニュートン法使えそう。&lt;/li&gt;
&lt;li&gt;フィルタのときのように逐次的に求められない？&lt;ul&gt;
&lt;li&gt;パラメータ全てではなく1こずつ。サンプルについても1こずつ。更新していく。評価関数の最小化は平均値の最小化に見受けられるので、逐次的に更新しても良いように見える。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今日は遅いのでもう寝る&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category><category term="IRLS"></category></entry><entry><title>2020-04-18</title><link href="/2020-04-18.html" rel="alternate"></link><published>2020-04-18T17:30:00+09:00</published><updated>2020-04-19T00:19:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-18:/2020-04-18.html</id><summary type="html">&lt;div class="section" id="irls-iteratively-reweighted-least-squares"&gt;
&lt;h2&gt;IRLS(Iteratively Reweighted Least Squares)&lt;/h2&gt;
&lt;p&gt;LAD(Least Absolute Deviation)を近似的・逐次的に解く方法としてのIRLSについて調査。そういえば基本的な原理を抑えていなかった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://retrofocus28.blogspot.com/2015/09/iteratively-reweighted-least-squares.html"&gt;Iteratively Reweighted Least Squares　についてサクッと。&lt;/a&gt; 文字通りサクッとしたまとめ。OMPを使って解いているというのがとても気になる&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cnx.org/contents/krkDdys0&amp;#64;12/Iterative-Reweighted-Least-Squares"&gt;Iterative Reweighted Least Squares&lt;/a&gt; 導入から解法まで。しかしなぜ解が求まるのかは不明。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.3-IRLS.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; バッファロー大の講義資料？これも何故解けるのかはちゃんと書いてない。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.maths.lth.se/matematiklth/personal/fredrik/Session3.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; これが一番いいかも。なぜ解けるかもざっくり証明がある。&lt;ul&gt;
&lt;li&gt;そこで出てきたsupergradient（優勾配？劣勾配に対応している？）がよくわからん。資料のすぐ下に解説があったけど。 &lt;a class="reference external" href="http://www.its.caltech.edu/~kcborder/Notes/Supergrad.pdf"&gt;Supergradients&lt;/a&gt; に定義はあったけど幾何学的イメージが欲しい。&lt;/li&gt;
&lt;li&gt;Weiszfeld Algorithmsという幾何中央値を求めるアルゴリズムは &lt;a class="reference external" href="http://users.cecs.anu.edu.au/~trumpf/pubs/aftab_hartley_trumpf_PAMI2014.pdf"&gt;Generalized Weiszfeld Algorithms for …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="irls-iteratively-reweighted-least-squares"&gt;
&lt;h2&gt;IRLS(Iteratively Reweighted Least Squares)&lt;/h2&gt;
&lt;p&gt;LAD(Least Absolute Deviation)を近似的・逐次的に解く方法としてのIRLSについて調査。そういえば基本的な原理を抑えていなかった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://retrofocus28.blogspot.com/2015/09/iteratively-reweighted-least-squares.html"&gt;Iteratively Reweighted Least Squares　についてサクッと。&lt;/a&gt; 文字通りサクッとしたまとめ。OMPを使って解いているというのがとても気になる&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cnx.org/contents/krkDdys0&amp;#64;12/Iterative-Reweighted-Least-Squares"&gt;Iterative Reweighted Least Squares&lt;/a&gt; 導入から解法まで。しかしなぜ解が求まるのかは不明。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.3-IRLS.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; バッファロー大の講義資料？これも何故解けるのかはちゃんと書いてない。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.maths.lth.se/matematiklth/personal/fredrik/Session3.pdf"&gt;Iterative Reweighted Least Squares&lt;/a&gt; これが一番いいかも。なぜ解けるかもざっくり証明がある。&lt;ul&gt;
&lt;li&gt;そこで出てきたsupergradient（優勾配？劣勾配に対応している？）がよくわからん。資料のすぐ下に解説があったけど。 &lt;a class="reference external" href="http://www.its.caltech.edu/~kcborder/Notes/Supergrad.pdf"&gt;Supergradients&lt;/a&gt; に定義はあったけど幾何学的イメージが欲しい。&lt;/li&gt;
&lt;li&gt;Weiszfeld Algorithmsという幾何中央値を求めるアルゴリズムは &lt;a class="reference external" href="http://users.cecs.anu.edu.au/~trumpf/pubs/aftab_hartley_trumpf_PAMI2014.pdf"&gt;Generalized Weiszfeld Algorithms for Lq Optimization&lt;/a&gt; に解説あり。しかしこの論文いいこと言ってる。「Generalized Weiszfeld Algorithms」は圧縮センシングとは異なりスパース表現を求めるわけではない。スパース性は担保されなくても、よりL1ノルムの意味で小さい解を求める。&lt;/li&gt;
&lt;li&gt;なぜ、IRLSとLMSアルゴリズムを結びつける研究がないのか。IRLSの逐次適用によってもフィルタ係数を更新していけそうだけど。試してみるし、類似研究が無いか引き続き調べる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;『スパースモデリング』の5章にも記述はある。しかし残差のL1最小化ではない。&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category><category term="IRLS"></category></entry><entry><title>2020-04-17</title><link href="/2020-04-17.html" rel="alternate"></link><published>2020-04-17T23:00:00+09:00</published><updated>2020-04-17T23:00:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-17:/2020-04-17.html</id><summary type="html">&lt;div class="section" id="lad-least-absolute-deviation"&gt;
&lt;h2&gt;LAD(Least Absolute Deviation)&lt;/h2&gt;
&lt;p&gt;LAD(Least Absolute Deviation)を見ている。これは、残差をL1ノルムにした回帰問題一般のこと。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://core.ac.uk/download/pdf/81785239.pdf"&gt;最尤推定による近似的手法&lt;/a&gt; は軽く読んだ。各傾きと切片を固定して逐次更新していく。更新時は中央値を拾ってくる。うーん中央値だと高速推定が厳しい。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ラプラス分布の最尤推定しようとしてもがく。対数尤度とって見てみても、単純な絶対値和が出て止まるし、反復スケーリング法を参考に、パラメータの増分を加えた時の対数尤度の下限を求めようとしたが上手く行かず。4時間飛ばす。&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="最尤推定の計算のあがき" src="./images/IMG_3828.jpg" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;最尤推定の計算のあがき&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;あがいて「A maximum likelihood approach to least absolute deviation regression」を引用している文献を漁ったら辞書学習をL1にしているやつが、やっぱりいた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://winsty.net/papers/onndl.pdf"&gt;Online Robust Non-negative Dictionary Learning for Visual Tracking&lt;/a&gt; パーティクルフィルターを使っておる。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上の文献で使ってるHuber Loss結構すごくね？この誤差に基づくLMSアルゴリズムねえの？→「Robust …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="lad-least-absolute-deviation"&gt;
&lt;h2&gt;LAD(Least Absolute Deviation)&lt;/h2&gt;
&lt;p&gt;LAD(Least Absolute Deviation)を見ている。これは、残差をL1ノルムにした回帰問題一般のこと。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://core.ac.uk/download/pdf/81785239.pdf"&gt;最尤推定による近似的手法&lt;/a&gt; は軽く読んだ。各傾きと切片を固定して逐次更新していく。更新時は中央値を拾ってくる。うーん中央値だと高速推定が厳しい。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ラプラス分布の最尤推定しようとしてもがく。対数尤度とって見てみても、単純な絶対値和が出て止まるし、反復スケーリング法を参考に、パラメータの増分を加えた時の対数尤度の下限を求めようとしたが上手く行かず。4時間飛ばす。&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="最尤推定の計算のあがき" src="./images/IMG_3828.jpg" style="width: 40%;" /&gt;
&lt;p class="caption"&gt;最尤推定の計算のあがき&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;あがいて「A maximum likelihood approach to least absolute deviation regression」を引用している文献を漁ったら辞書学習をL1にしているやつが、やっぱりいた。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://winsty.net/papers/onndl.pdf"&gt;Online Robust Non-negative Dictionary Learning for Visual Tracking&lt;/a&gt; パーティクルフィルターを使っておる。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上の文献で使ってるHuber Loss結構すごくね？この誤差に基づくLMSアルゴリズムねえの？→「Robust Huber adaptive filter」だけど中身を読めず…&lt;/p&gt;
&lt;p&gt;また、 &lt;a class="reference external" href="https://www.ml.uni-saarland.de/Lectures/CVX-SS10/ConvexOptimization-07-07-10.pdf"&gt;Convex Optimization and Modeling&lt;/a&gt; を読んでたらHuber損失はL1とL2の中間的な性質を示すようで、0に集中しなくなりそうな印象を受けた。&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category><category term="L1ノルム"></category><category term="LAD"></category></entry><entry><title>2020-04-16</title><link href="/2020-04-16.html" rel="alternate"></link><published>2020-04-16T23:20:00+09:00</published><updated>2020-04-16T23:20:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-16:/2020-04-16.html</id><summary type="html">&lt;div class="section" id="lms"&gt;
&lt;h2&gt;LMSフィルターの挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt; の挙動を追いたい。色々な信号に対して、&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が十分大きいとき、0に近づくかどうか&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を知りたい。もし0に近づくならば有効な過程として解法に使える。
しかしその前に、LMSフィルター自体の挙動を追いたい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;残差はどの様に減る？残差の時系列は？&lt;ul&gt;
&lt;li&gt;ステップサイズにより収束の度合い（残差の分布）が違う...&lt;/li&gt;
&lt;li&gt;当然、フィルタ次数でも収束の度合い（残差の分布）が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;残差分布はどうなってる？Signed-LMSでラプラス分布に近づいてる？&lt;ul&gt;
&lt;li&gt;これは本当のようで、Signed-LMSの方が裾が細い残差分布が得られている。&lt;/li&gt;
&lt;li&gt;単純な正弦波に対してはLMSのほうが残差が小さくなるが、ボイスやピアノ音源に対しては圧倒的にSignLMSの方が性能が良い（残差のヒストグラムを見ると、裾が狭い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathrm{E}[e(n)x(n-m)]\)&lt;/span&gt; は両方とも0 …&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="lms"&gt;
&lt;h2&gt;LMSフィルターの挙動観察&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt; の挙動を追いたい。色々な信号に対して、&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; が十分大きいとき、0に近づくかどうか&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を知りたい。もし0に近づくならば有効な過程として解法に使える。
しかしその前に、LMSフィルター自体の挙動を追いたい。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;残差はどの様に減る？残差の時系列は？&lt;ul&gt;
&lt;li&gt;ステップサイズにより収束の度合い（残差の分布）が違う...&lt;/li&gt;
&lt;li&gt;当然、フィルタ次数でも収束の度合い（残差の分布）が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;残差分布はどうなってる？Signed-LMSでラプラス分布に近づいてる？&lt;ul&gt;
&lt;li&gt;これは本当のようで、Signed-LMSの方が裾が細い残差分布が得られている。&lt;/li&gt;
&lt;li&gt;単純な正弦波に対してはLMSのほうが残差が小さくなるが、ボイスやピアノ音源に対しては圧倒的にSignLMSの方が性能が良い（残差のヒストグラムを見ると、裾が狭い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{E}[\mathrm{sign}[e(n)]x(n-m)]\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathrm{E}[e(n)x(n-m)]\)&lt;/span&gt; は両方とも0。&lt;ul&gt;
&lt;li&gt;逐次計算していったら、音源非依存で0に近づいていく&lt;/li&gt;
&lt;li&gt;当然だよな…そもそもの過程として入力と雑音は無相関と仮定しているのだから。&lt;ul&gt;
&lt;li&gt;仮定しているのだからは正しくなくて、無相関にするようにフィルタ係数を更新しているが正しい。&lt;/li&gt;
&lt;li&gt;無相関になったときに勾配が0で最急勾配法が止まる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;なんか絶対値誤差最小化ってどっかで見たよな…と思っていたら、&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Least_absolute_deviations"&gt;https://en.wikipedia.org/wiki/Least_absolute_deviations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;修士のときに一回戦っていた。&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=2ahUKEwi4wZXEhe3oAhUZMd4KHZrzDqQQFjAAegQIARAB&amp;amp;url=https%3A%2F%2Fis.cuni.cz%2Fwebapps%2Fzzp%2Fdownload%2F130215341&amp;amp;usg=AOvVaw3Cxgr7_WLuDQqhL1aKQl9f"&gt;カレル大学卒論&lt;/a&gt; が結構まとまっている。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(L_{1}\)&lt;/span&gt; ノルム最小化を近接オペレータの繰り返し適用で解けんじゃね？と思っている&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="https://yamagensakam.hatenablog.com/entry/2018/02/14/075106"&gt;近接勾配法とproximal operator&lt;/a&gt; を読んだが、パラメータ正則化だけだな&lt;/li&gt;
&lt;li&gt;パラメータ正則化はあるけど、残差をスパースにするのがない。なんで？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category></entry><entry><title>2020-04-10</title><link href="/2020-04-10.html" rel="alternate"></link><published>2020-04-10T23:18:00+09:00</published><updated>2020-04-10T23:18:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-10:/2020-04-10.html</id><content type="html">&lt;div class="section" id="id2"&gt;
&lt;h2&gt;続・古いロスレス音声コーデックの調査&lt;/h2&gt;
&lt;p&gt;古いロスレス音声コーデックと理論の概要を取りまとめた雑誌の特集があった:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eie.polyu.edu.hk/~enyhchan/ce_ac_p1.pdf"&gt;Lossless Compression of Digital Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理論としてもその通りだし、雑誌発行時点(1998)からさしたるブレークスルーが無いように見える。&lt;/p&gt;
&lt;p&gt;AudioPak, OggSquish, Philips, Sonarc, WAという謎のコーデック現る…。いったい何個あるんだ。&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category></entry><entry><title>2020-04-08</title><link href="/2020-04-08.html" rel="alternate"></link><published>2020-04-08T16:45:00+09:00</published><updated>2020-04-08T23:45:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-08:/2020-04-08.html</id><summary type="html">&lt;div class="section" id="id2"&gt;
&lt;h2&gt;古いロスレス音声コーデックの調査&lt;/h2&gt;
&lt;p&gt;ロスレス音声の歴史を探るために古いロスレス音声コーデックの情報を探っている。以下のサイトが &lt;a class="reference external" href="https://wiki.hydrogenaud.io/index.php?title=Lossless_comparison"&gt;Hydrogenaudioでの比較&lt;/a&gt; よりも古い内容を扱っている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html"&gt;Lossless Compression of Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;見つけたロスレス音声コーデックを一覧する。というかほぼ &lt;a class="reference external" href="https://www.rarewares.org/rrw/about.php"&gt;Really Rare Wares&lt;/a&gt; 様へのリンク。&lt;/p&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;古めのロスレス音声コーデック&lt;/h3&gt;
&lt;div class="section" id="rkau-rk-audio"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/rkau.php"&gt;RKAU(RK Audio)&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;古い比較において優秀な圧縮率を誇っていた。当時のMonkey's Audioよりも上。サイトを覗いたら exe と dll のみの配布だった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020124045327/http://rksoft.virtualave.net/rkau.html"&gt;RKAUのホームページ（魚拓）&lt;/a&gt; を見ても特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="audiozip"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/audiozip.php"&gt;AudioZip&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;これも圧縮率が比較的優秀。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020207080740/http://www.csp.ntu.edu.sg:8000/MMS/MMCProjects.htm"&gt;AudioZipのホームページ（魚拓）&lt;/a&gt; を見てもこちらも特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="wavarc"&gt;
&lt;h4&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/wavarc/0readme.html"&gt;WavArc&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;こちらも最大圧縮率(-c5)を選択するとそれなりに優秀な結果を出していた。このページにexeとドキュメントをまとめたzipもあり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavezip"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/wavezip.php"&gt;WaveZip&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;圧縮率よりは速度重視のコーデックのようだ。MUSICompress というアルゴリズムの実装。 &lt;a class="reference external" href="https://www.rarewares.org/rrw/files/lossless/musi_txt.txt"&gt;WaveZipのデータシート&lt;/a&gt; によると符号化にはLZ(Lampel-Ziv)を使用しているようだ。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html#wavezip"&gt;WaveZipの概要&lt;/a&gt; が比較サイトに掲載されていた …&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="id2"&gt;
&lt;h2&gt;古いロスレス音声コーデックの調査&lt;/h2&gt;
&lt;p&gt;ロスレス音声の歴史を探るために古いロスレス音声コーデックの情報を探っている。以下のサイトが &lt;a class="reference external" href="https://wiki.hydrogenaud.io/index.php?title=Lossless_comparison"&gt;Hydrogenaudioでの比較&lt;/a&gt; よりも古い内容を扱っている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html"&gt;Lossless Compression of Audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;見つけたロスレス音声コーデックを一覧する。というかほぼ &lt;a class="reference external" href="https://www.rarewares.org/rrw/about.php"&gt;Really Rare Wares&lt;/a&gt; 様へのリンク。&lt;/p&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;古めのロスレス音声コーデック&lt;/h3&gt;
&lt;div class="section" id="rkau-rk-audio"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/rkau.php"&gt;RKAU(RK Audio)&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;古い比較において優秀な圧縮率を誇っていた。当時のMonkey's Audioよりも上。サイトを覗いたら exe と dll のみの配布だった。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020124045327/http://rksoft.virtualave.net/rkau.html"&gt;RKAUのホームページ（魚拓）&lt;/a&gt; を見ても特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="audiozip"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/audiozip.php"&gt;AudioZip&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;これも圧縮率が比較的優秀。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20020207080740/http://www.csp.ntu.edu.sg:8000/MMS/MMCProjects.htm"&gt;AudioZipのホームページ（魚拓）&lt;/a&gt; を見てもこちらも特に情報なし。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="wavarc"&gt;
&lt;h4&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/wavarc/0readme.html"&gt;WavArc&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;こちらも最大圧縮率(-c5)を選択するとそれなりに優秀な結果を出していた。このページにexeとドキュメントをまとめたzipもあり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavezip"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/wavezip.php"&gt;WaveZip&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;圧縮率よりは速度重視のコーデックのようだ。MUSICompress というアルゴリズムの実装。 &lt;a class="reference external" href="https://www.rarewares.org/rrw/files/lossless/musi_txt.txt"&gt;WaveZipのデータシート&lt;/a&gt; によると符号化にはLZ(Lampel-Ziv)を使用しているようだ。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html#wavezip"&gt;WaveZipの概要&lt;/a&gt; が比較サイトに掲載されていた。どうやら、入力波形を近似波形と誤差波形に分けて符号化するようだ。WaveZipではHu&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lpac-ltac"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://www.rarewares.org/rrw/lpac.php"&gt;LPAC/LTAC&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;LPACはMPEG4-ALSの前身。LPACの前身がLTAC。LPACの平均的な圧縮率は優秀なようだ。 &lt;a class="reference external" href="https://web.archive.org/web/20060213124711/http://www.nue.tu-berlin.de/wer/liebchen/lpac.html"&gt;LPAC（魚拓）&lt;/a&gt; に以前公開していたサイトあり。&lt;/p&gt;
&lt;p&gt;LTAC(Lossless Transform Audio Compression)は名前の通り変換符号化に基づくロスレス音声圧縮コーデック、LPAC(Lossless Predictive Audio Compression)は予測に基づくロスレス音声圧縮コーデック。&lt;/p&gt;
&lt;p&gt;LPACに ベルリン工科大学、Real Networks、NTT の改良が加わってMPEG4-ALSが出来上がり、それ以降LPACの開発は停止されている。この経緯については &lt;a class="reference external" href="https://web.archive.org/web/20060212123059/http://www.nue.tu-berlin.de/forschung/projekte/lossless/mp4als.html"&gt;MPEG4-ALS（魚拓）&lt;/a&gt; に記述あり。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="shorten"&gt;
&lt;h4&gt;&lt;a class="reference external" href="https://archive.is/Z8k97"&gt;Shorten（魚拓）&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;おそらくロスレス音声の最古参にして基礎。なんと執筆時点（2020-04-08）でも &lt;tt class="docutils literal"&gt;brew&lt;/tt&gt; でインストールできた（ &lt;a class="reference external" href="https://linux.die.net/man/1/shorten"&gt;Shortenのmanページ&lt;/a&gt; もあるから各Linuxディストリビューションで使えるものと想像する）。エンコード速度はピカイチ。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9797AA37C32F12179AF0803D8C2B22D2?doi=10.1.1.53.7337&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Shortenの論文&lt;/a&gt; （テクニカルレポート）もある。この論文で、今のロスレス音声につながる重要な事実に幾つか触れている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;音声信号は準定常（短い区間では定常とみなせる）だからブロックに分けてエンコード/デコードすべき。&lt;/li&gt;
&lt;li&gt;音声のモデル化には線形予測(LPC, Linear Predictive Coding)が使える。&lt;/li&gt;
&lt;li&gt;残差信号はガウス分布よりもラプラス分布に従っていると見える。その符号化にはライス符号を使うのが良い。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この時点で既にラプラス分布を仮定したパラメータ設定を行っているからかなりの慧眼。他のロスレス音声コーデックはShortenを発展させたものに過ぎないと見える。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt;所感&lt;/h3&gt;
&lt;p&gt;どうも2000年代前半までは各自でロスレス音声コーデックを作り、各自で最強を謳っていたらしい。&lt;/p&gt;
&lt;p&gt;歴史を雑にまとめると、1994年にShortenの論文が出てから、それよりも圧縮率の良いもの、圧縮速度（展開速度）が早いものが開発されて混沌に突入し上記のコーデックが現れた。その後、Monkey's Audio, WavPack, FLAC, LPAC（MPEG4-ALS）が生き残り、2000年以降はLa（更新停止）, TAK, TTA, ALAC（更新停止）, WMAL(Windows Media Audio Lossless), 2010年以降はOptimFROGが出現しているようだ。&lt;/p&gt;
&lt;p&gt;気になるのは比較サイトの &lt;a class="reference external" href="http://www.firstpr.com.au/audiocomp/lossless/index.html#rice"&gt;Rice Coding, AKA Rice Packing, Elias Gamma codes and other approaches&lt;/a&gt; である。Rice符号よりも効率の良いとされるPod符号の紹介がある。要観察。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id15"&gt;
&lt;h2&gt;スパース適応フィルタ&lt;/h2&gt;
&lt;p&gt;LPCの定式化をスパースにする試みは多くなされている。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.cs.tut.fi/~tabus/2013GhidoTabus.pdf"&gt;Sparse Modeling for Lossless Audio Compression&lt;/a&gt; : Ghidoさん（OptimFROGの人）の試み&lt;ul&gt;
&lt;li&gt;貪欲法によりスパース解を求めている。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.jstage.jst.go.jp/article/jasj/71/11/71_KJ00010109335/_pdf/-char/ja"&gt;スパース表現に基づく音声音響符号化&lt;/a&gt; : NTTの試み&lt;ul&gt;
&lt;li&gt;最小二乗解を求めるのではなくL1最小化に置き換えた定式化を行う。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;でも、TTAがやっているような適応フィルタをスパース解に近づける手法はまだロスレス音声に対してやっていないように見える。
スパースな解を目指してフィルタ係数を更新する適応フィルタはスパース適応フィルタ(Sparse Adaptive Filters)というようで、2000年代以降に研究が進んでいるようだ。&lt;/p&gt;
&lt;p&gt;最も基本的な適応フィルタであるLMS(Least Mean Square)フィルタは名前の通り二乗誤差最小化に立脚している。
スパース適応フィルタの主な用途はエコーキャンセル、ブラインド話者分離、複数話者特定ではあるが、やはり変換後の分布がスパースになるというのは大きい。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://signal.ee.bilkent.edu.tr/defevent/papers/cr1256.pdf"&gt;スパース適応フィルタの最近のサーベイ論文&lt;/a&gt; を流し読みした。スパース適応フィルタは、変数更新のときに1部の変数だけ更新する方法と、スパース最適化に従って更新するやり方の2つがあった。PNLMS(Proportionate NLMS), IPNLMS(Improved PNLMS)が後者の定式化で興味あり。引き続き見ていく。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/pdf/1012.5066.pdf"&gt;Regularized Least-Mean-Square Algorithms&lt;/a&gt; には正則化を入れたLMSアルゴリズムの解説あり。LASSOにモチベーションを受けた最適化アルゴリズムが &lt;a class="reference external" href="https://wiki.eecs.umich.edu/global/data/hero/images/7/7b/Yilun-icassp2-09.pdf"&gt;ZA-LMS&lt;/a&gt; や &lt;a class="reference external" href="http://azadproject.ir/wp-content/uploads/2017/01/2018-Online-Sparse-System-Identification-and-Signal-Reconstruction-Using-Projections-.pdf"&gt;APWL1&lt;/a&gt; として提案されている。&lt;/p&gt;
&lt;/div&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="Lossless Audio"></category><category term="ロスレス音声"></category><category term="スパース符号化"></category></entry><entry><title>2020-04-02</title><link href="/2020-04-02.html" rel="alternate"></link><published>2020-04-02T18:00:00+09:00</published><updated>2020-04-02T21:34:00+09:00</updated><author><name>aiki</name></author><id>tag:None,2020-04-02:/2020-04-02.html</id><content type="html">&lt;p&gt;GitHub io + Pelican を使ってみた。しばらくこちらで日報を書きたい。
GitHub io + Pelicanは以下の記事を参考にしている。まだあんまり分かってない。&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/yusukew62/items/7b01d2370cdbe170b28d"&gt;Python製静的HTMLジェネレータのPelicanでGitHub Pagesを公開する方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/ririli/items/0e06b21cb709beae4514"&gt;GitHub Pagesで静的サイトを簡単に作る&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://qiita.com/saira/items/71faa202efb4320cb41d"&gt;Python製 Pelican を使ってサクッとブログを公開する&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pelicanthemes.com"&gt;Pelicanのテーマ集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/getpelican/pelican-themes/issues/460#issuecomment-346652986"&gt;テーマ導入時にハマったので参考にしたissue comment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今日は（というか3月末）からSLAの高速化作業とまとめをしていた。&lt;/p&gt;
&lt;p&gt;格子型フィルタ演算はどうしても1乗算型にできず。次数演算を4次数にしてSSE演算するのがやっと。
SSE化するときに、スカラー演算とベクトル演算が混じったときに処理負荷が大きく上がってハマった。
&lt;a class="reference external" href="https://stackoverflow.com/questions/10313397/where-does-the-sse-instructions-outperform-normal-instructions"&gt;StackOverFlowの記事&lt;/a&gt; では &lt;cite&gt;_mm_set_epi32&lt;/cite&gt; のコストが高い旨記述あり。 &lt;cite&gt;_mm_loadu_si128&lt;/cite&gt; の使用に置き換えた。
&lt;a class="reference external" href="https://stackoverflow.com/questions/24446516/performance-worsens-when-using-sse-simple-addition-of-integer-arrays"&gt;他の記事&lt;/a&gt; で言及があってようやく分かった。全てをベクトル演算化したところ、処理負荷は4/5倍になった。あんまり早くなっていない。遺憾。&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://herumi.in.coocan.jp/prog/gcc-and-vc.html"&gt;gccとVC&lt;/a&gt; にはgccとVisual Studioの挙動の差異について色々と書いてあった。&lt;/p&gt;
</content><category term="雑記"></category><category term="SLA"></category><category term="SSE"></category><category term="test"></category><category term="pelican"></category><category term="githubio"></category></entry></feed>